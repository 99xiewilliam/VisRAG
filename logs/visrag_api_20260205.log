2026-02-05 00:21:38 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 00:21:39 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 00:21:39 [INFO] fastapi_app.service.query_service: By how much do they outperform standard BERT in the paper which named <Enriching BERT with Knowledge Graph Embeddings for Document Classification>?
2026-02-05 00:21:39 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 00:21:40 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 00:21:40 [INFO] fastapi_app.service.query_service: Query text collection: pdf_enriching_bert_with_knowledge_graph_embeddings_for_document_classification_pdf_7, top_k=3
2026-02-05 00:21:40 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 00:21:55 [INFO] fastapi_app.service.ocr_service: Running OCR2 vLLM generation
2026-02-05 00:21:59 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 00:21:59 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 20119.6ms
2026-02-05 00:22:36 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 00:22:36 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 00:22:36 [INFO] fastapi_app.service.query_service: By how much do they outperform standard BERT in the paper which named <Enriching BERT with Knowledge Graph Embeddings for Document Classification>?
2026-02-05 00:22:36 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 00:22:37 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 00:22:37 [INFO] fastapi_app.service.query_service: Query text collection: pdf_enriching_bert_with_knowledge_graph_embeddings_for_document_classification_pdf_7, top_k=3
2026-02-05 00:22:38 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 00:22:53 [INFO] fastapi_app.service.ocr_service: Running OCR2 vLLM generation
2026-02-05 00:22:56 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 00:22:56 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 20531.4ms
2026-02-05 00:57:15 [INFO] fastapi_app.app: GET / 404 1.6ms
2026-02-05 00:57:16 [INFO] fastapi_app.app: GET / 404 0.6ms
2026-02-05 00:57:16 [INFO] fastapi_app.app: GET /favicon.ico 404 0.5ms
2026-02-05 00:57:17 [INFO] fastapi_app.app: GET /robots.txt 404 0.5ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 1.9ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.6ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.7ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.4ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.4ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.5ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.3ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.5ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.5ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.3ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.4ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.4ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.4ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.5ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.3ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.3ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.3ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 1.4ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.5ms
2026-02-05 01:09:10 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 404 0.3ms
2026-02-05 01:09:24 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 01:09:24 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 01:09:24 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): a_simple_discriminative_training_method_for_machine_translation_with_large-scale_features -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/43a21d7f19ef4d98b9024dc9a473b4a5_A simple discriminative training method for machine translation with large-scale features.pdf
2026-02-05 01:09:24 [INFO] fastapi_app.service.index_service: PDF pages: 7 (text-only global)
2026-02-05 01:09:26 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 2044.4ms
2026-02-05 01:09:26 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): active_learning_for_chinese_word_segmentation_in_medical_text -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/dee78027a2814d44b57af332560333ef_Active Learning for Chinese Word Segmentation in Medical Text.pdf
2026-02-05 01:09:26 [INFO] fastapi_app.service.index_service: PDF pages: 10 (text-only global)
2026-02-05 01:09:28 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 1934.9ms
2026-02-05 01:09:28 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): bertram-_improved_word_embeddings_have_big_impact_on_contextualized_model_performance -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/02332ecb05454907a9a8f9cfb845ed8a_BERTRAM- Improved Word Embeddings Have Big Impact on Contextualized Model Performance.pdf
2026-02-05 01:09:28 [INFO] fastapi_app.service.index_service: PDF pages: 12 (text-only global)
2026-02-05 01:09:31 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 3174.6ms
2026-02-05 01:09:31 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): classification_betters_regression_in_query-based_multi-document_summarisation_techniques_for_question_answering-_macquarie_university_at_bioasq7b -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/cf7bb0815135447ea9d2cda9bc795faa_Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering- Macquarie University at BioASQ7b.pdf
2026-02-05 01:09:31 [INFO] fastapi_app.service.index_service: PDF pages: 12 (text-only global)
2026-02-05 01:09:32 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 1703.2ms
2026-02-05 01:09:32 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): crosswoz-_a_large-scale_chinese_cross-domain_task-oriented_dialogue_dataset -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/d523578016c04a25b5850cc9cb47e0e4_CrossWOZ- A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset.pdf
2026-02-05 01:09:33 [INFO] fastapi_app.service.index_service: PDF pages: 14 (text-only global)
2026-02-05 01:09:36 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 3338.4ms
2026-02-05 01:09:36 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): diachronic_topics_in_new_high_german_poetry -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/5aac9183ac414568b03354149cf932f9_Diachronic Topics in New High German Poetry.pdf
2026-02-05 01:09:36 [INFO] fastapi_app.service.index_service: PDF pages: 7 (text-only global)
2026-02-05 01:09:37 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 982.3ms
2026-02-05 01:09:37 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): dissim-_a_discourse-aware_syntactic_text_simplification_frameworkfor_english_and_german -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/c76a5bf5ecef49f2a0b218d1f8e16ef8_DisSim- A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German.pdf
2026-02-05 01:09:37 [INFO] fastapi_app.service.index_service: PDF pages: 4 (text-only global)
2026-02-05 01:09:38 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 993.7ms
2026-02-05 01:09:38 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): diversity_density_and_homogeneity-_quantitative_characteristic_metrics_for_text_collections -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/32edb8a951444b188b336655280cb96c_Diversity, Density, and Homogeneity- Quantitative Characteristic Metrics for Text Collections.pdf
2026-02-05 01:09:38 [INFO] fastapi_app.service.index_service: PDF pages: 8 (text-only global)
2026-02-05 01:09:40 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 2057.8ms
2026-02-05 01:09:40 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): enriching_bert_with_knowledge_graph_embeddings_for_document_classification -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/9312286b39d94512b8417c5c4bcb466c_Enriching BERT with Knowledge Graph Embeddings for Document Classification.pdf
2026-02-05 01:09:40 [INFO] fastapi_app.service.index_service: PDF pages: 8 (text-only global)
2026-02-05 01:09:42 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 2102.8ms
2026-02-05 01:09:42 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): important_attribute_identification_in_knowledge_graph -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/1887d5a7b77842b6917330edb1f27236_Important Attribute Identification in Knowledge Graph.pdf
2026-02-05 01:09:42 [INFO] fastapi_app.service.index_service: PDF pages: 13 (text-only global)
2026-02-05 01:09:44 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 1966.6ms
2026-02-05 01:09:44 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): improving_spoken_language_understanding_by_exploiting_asr_n-best_hypotheses -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/efa93e8e4ab1447ea552e5c751826be4_Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses.pdf
2026-02-05 01:09:44 [INFO] fastapi_app.service.index_service: PDF pages: 5 (text-only global)
2026-02-05 01:09:45 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 1474.0ms
2026-02-05 01:09:45 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): inscript-_narrative_texts_annotated_with_script_information -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/087b41332d6048329978dc83727711fd_InScript- Narrative texts annotated with script information.pdf
2026-02-05 01:09:46 [INFO] fastapi_app.service.index_service: PDF pages: 9 (text-only global)
2026-02-05 01:09:48 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 2689.9ms
2026-02-05 01:09:48 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): investigating_robustness_and_interpretability_of_link_prediction_via_adversarial_modifications -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/74b6de9b68654f978ceceea1defb8d8b_Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications.pdf
2026-02-05 01:09:48 [INFO] fastapi_app.service.index_service: PDF pages: 12 (text-only global)
2026-02-05 01:09:51 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 3180.7ms
2026-02-05 01:09:51 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): joint_entity_linking_with_deep_reinforcement_learning -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/72445c295e724aa3bacb2ad6776be196_Joint Entity Linking with Deep Reinforcement Learning.pdf
2026-02-05 01:09:51 [INFO] fastapi_app.service.index_service: PDF pages: 10 (text-only global)
2026-02-05 01:09:55 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 3251.7ms
2026-02-05 01:09:55 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): learning_supervised_topic_models_for_classification_and_regression_from_crowds -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/d991a363e1e84b4a94d8031311c51788_Learning Supervised Topic Models for Classification and Regression from Crowds.pdf
2026-02-05 01:09:55 [INFO] fastapi_app.service.index_service: PDF pages: 15 (text-only global)
2026-02-05 01:09:59 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 4432.5ms
2026-02-05 01:09:59 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): learning_word_embeddings_from_the_portuguese_twitter_stream-_a_study_of_some_practical_aspects -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/85e673823d1b4702a0e755926d1afe2f_Learning Word Embeddings from the Portuguese Twitter Stream- A Study of some Practical Aspects.pdf
2026-02-05 01:09:59 [INFO] fastapi_app.service.index_service: PDF pages: 12 (text-only global)
2026-02-05 01:10:01 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 2079.6ms
2026-02-05 01:10:01 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): marrying_universal_dependencies_and_universal_morphology -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/d39ff6d65ad643a0819f23d1b2b3bb28_Marrying Universal Dependencies and Universal Morphology.pdf
2026-02-05 01:10:01 [INFO] fastapi_app.service.index_service: PDF pages: 11 (text-only global)
2026-02-05 01:10:04 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 2852.5ms
2026-02-05 01:10:04 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): procedural_reasoning_networks_for_understanding_multimodal_procedures -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/7444bd1fc62b4572919ed5eacb81ceef_Procedural Reasoning Networks for Understanding Multimodal Procedures.pdf
2026-02-05 01:10:04 [INFO] fastapi_app.service.index_service: PDF pages: 11 (text-only global)
2026-02-05 01:10:07 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 3315.6ms
2026-02-05 01:10:07 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): qnamaker-_data_to_bot_in_2_minutes -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/78beaf9d265f4b8fa17d0cfeba19a2a2_QnAMaker- Data to Bot in 2 Minutes.pdf
2026-02-05 01:10:07 [INFO] fastapi_app.service.index_service: PDF pages: 4 (text-only global)
2026-02-05 01:10:09 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 1269.7ms
2026-02-05 01:10:09 [INFO] fastapi_app.service.index_service: Indexing PDF text-only (global): what_drives_the_international_development_agenda_an_nlp_analysis_of_the_united_nations_general_debate_1970-2016 -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/209f1dff00b144b8a6053c94b9c70e1d_What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016.pdf
2026-02-05 01:10:09 [INFO] fastapi_app.service.index_service: PDF pages: 6 (text-only global)
2026-02-05 01:10:11 [INFO] fastapi_app.app: POST /api/v1/index/pdf_text_global 200 2203.8ms
2026-02-05 01:16:22 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 01:16:22 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 01:16:22 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 01:16:23 [INFO] src.generator: 加载 decoder-only: /home/xwh/models/DeepSeek-OCR-2
2026-02-05 01:17:15 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 53693.5ms
2026-02-05 01:18:09 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 01:18:10 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 01:18:24 [INFO] fastapi_app.service.ocr_service: Running OCR2 vLLM generation
2026-02-05 01:18:26 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 596, in query_vision_only
    ocr_text = self.ocr.run(
               ^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 130, in run
    outputs = self._llm.generate([cache_item], sampling_params=self._sampling)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1196, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 473, in generate
    outputs = self._run_engine(use_tqdm=use_tqdm)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1423, in _run_engine
    step_outputs = self.llm_engine.step()
                   ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 1412, in step
    outputs = self.model_executor.execute_model(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 140, in execute_model
    output = self.collective_rpc("execute_model",
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 420, in execute_model
    output = self.model_runner.execute_model(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1764, in execute_model
    hidden_or_intermediate_states = model_executable(
                                    ^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 541, in forward
    vision_embeddings = self.get_multimodal_embeddings(**kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 501, in get_multimodal_embeddings
    vision_embeddings = self._process_image_input(image_input)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 487, in _process_image_input
    vision_features = self._pixel_values_to_embedding(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 398, in _pixel_values_to_embedding
    local_features_1 = self.sam_model(patches)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/sam_vary_sdpa.py", line 176, in forward
    x = blk(x)
        ^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/sam_vary_sdpa.py", line 241, in forward
    x = self.attn(x)
        ^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/sam_vary_sdpa.py", line 294, in forward
    qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
          ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 43.50 MiB is free. Process 2650729 has 3.91 GiB memory in use. Including non-PyTorch memory, this process has 19.60 GiB memory in use. Of the allocated memory 19.13 GiB is allocated by PyTorch, with 2.00 MiB allocated in private pools (e.g., CUDA Graphs), and 146.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 01:18:26 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 16721.2ms
2026-02-05 02:19:55 [INFO] fastapi_app.app: GET / 404 1.4ms
2026-02-05 02:20:05 [INFO] fastapi_app.app: GET / 404 0.8ms
2026-02-05 02:20:11 [INFO] fastapi_app.app: GET /sitemap.xml 404 0.6ms
2026-02-05 03:03:51 [INFO] fastapi_app.app: GET / 404 75.4ms
2026-02-05 03:04:06 [INFO] fastapi_app.app: GET /.well-known/security.txt 404 0.8ms
2026-02-05 10:20:14 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 10:20:14 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 10:20:14 [INFO] fastapi_app.service.index_service: Indexing PDF: a_corpus_of_adpositional_supersenses_for_mandarin_chinese -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/a134255e80cf44d9a719753f3619d2cc_A Corpus of Adpositional Supersenses for Mandarin Chinese.pdf
2026-02-05 10:20:15 [INFO] fastapi_app.service.index_service: PDF pages: 9; images: 9
2026-02-05 10:20:32 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 18636.8ms
2026-02-05 10:20:32 [INFO] fastapi_app.service.index_service: Indexing PDF: a_study_on_neural_network_language_modeling -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/050e08600bd84d719a2b902d7bf5688b_A Study on Neural Network Language Modeling.pdf
2026-02-05 10:20:34 [INFO] fastapi_app.service.index_service: PDF pages: 20; images: 20
2026-02-05 10:20:56 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 24071.1ms
2026-02-05 10:20:56 [INFO] fastapi_app.service.index_service: Indexing PDF: cl-bench-_a_benchmark_for_context_learnin -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/d0573daa0bc34227ad1043c9444d8378_CL-BENCH- A BENCHMARK FOR CONTEXT LEARNIN.pdf
2026-02-05 10:21:03 [INFO] fastapi_app.service.index_service: PDF pages: 78; images: 78
2026-02-05 10:22:39 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 102944.6ms
2026-02-05 10:22:39 [INFO] fastapi_app.service.index_service: Indexing PDF: dynamic_fusion_networks_for_machine_reading_comprehension -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/2964988ddb3741a9810e21e4dd1eefce_Dynamic Fusion Networks for Machine Reading Comprehension.pdf
2026-02-05 10:22:40 [INFO] fastapi_app.service.index_service: PDF pages: 13; images: 13
2026-02-05 10:22:58 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 18618.6ms
2026-02-05 10:22:58 [INFO] fastapi_app.service.index_service: Indexing PDF: ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/75fc6b8bc2674a6ebcb3bc3ab97122fb_LadaBERT- Lightweight Adaptation of BERT through Hybrid Model Compression.pdf
2026-02-05 10:22:59 [INFO] fastapi_app.service.index_service: PDF pages: 10; images: 10
2026-02-05 10:23:12 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 14296.0ms
2026-02-05 10:23:12 [INFO] fastapi_app.service.index_service: Indexing PDF: political_speech_generation -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/993d8c1e537243719bac23c34d8b9bf7_Political Speech Generation.pdf
2026-02-05 10:23:13 [INFO] fastapi_app.service.index_service: PDF pages: 15; images: 15
2026-02-05 10:23:34 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 21333.9ms
2026-02-05 10:23:34 [INFO] fastapi_app.service.index_service: Indexing PDF: recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/6b68cf03de0a4722ad0f688c12656a3a_Recommendation Chart of Domains for Cross-Domain Sentiment Analysis-Findings of A 20 Domain Study.pdf
2026-02-05 10:23:34 [INFO] fastapi_app.service.index_service: PDF pages: 9; images: 9
2026-02-05 10:23:48 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 14358.7ms
2026-02-05 10:28:46 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 10:28:46 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:28:52 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:28:52 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 10:28:52 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:29:02 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:29:02 [INFO] fastapi_app.app: POST /api/v1/query/text 200 15921.4ms
2026-02-05 10:29:02 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:29:03 [INFO] src.generator: 加载 decoder-only: /home/xwh/models/DeepSeek-OCR-2
2026-02-05 10:29:55 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 52898.5ms
2026-02-05 10:29:55 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 10:29:55 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:29:56 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:29:56 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 10:29:56 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:30:06 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:30:06 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 11437.6ms
2026-02-05 10:30:14 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 10:30:14 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:30:16 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:30:16 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 10:30:16 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:30:25 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:30:25 [INFO] fastapi_app.app: POST /api/v1/query/text 200 11690.0ms
2026-02-05 10:30:25 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:30:27 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1474.0ms
2026-02-05 10:30:27 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 10:30:27 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:30:28 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:30:28 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 10:30:28 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:30:36 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:30:36 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9153.1ms
2026-02-05 10:30:36 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 10:30:36 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:30:38 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:30:38 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6, top_k=3
2026-02-05 10:30:38 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:30:44 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:30:44 [INFO] fastapi_app.app: POST /api/v1/query/text 200 7871.0ms
2026-02-05 10:30:44 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:30:45 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1178.1ms
2026-02-05 10:30:45 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 10:30:45 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:30:46 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:30:46 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6, top_k=3
2026-02-05 10:30:46 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:30:52 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:30:52 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6625.6ms
2026-02-05 10:30:52 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 10:30:52 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:30:54 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:30:54 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 10:30:54 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:31:03 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:31:03 [INFO] fastapi_app.app: POST /api/v1/query/text 200 10859.4ms
2026-02-05 10:31:03 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:31:03 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 308.7ms
2026-02-05 10:31:03 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 10:31:03 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:31:04 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:31:04 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 10:31:04 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:31:13 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:31:13 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 10591.4ms
2026-02-05 10:31:13 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 10:31:13 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:31:15 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:31:15 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 10:31:15 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:31:24 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:31:24 [INFO] fastapi_app.app: POST /api/v1/query/text 200 10447.1ms
2026-02-05 10:31:24 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:31:47 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 23087.2ms
2026-02-05 10:31:47 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 10:31:47 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:31:48 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:31:48 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 10:31:48 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:31:55 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:31:55 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8491.5ms
2026-02-05 10:31:56 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 10:31:56 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:31:58 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:31:58 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 10:31:58 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:32:58 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 95, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error timed out>
2026-02-05 10:32:58 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:32:58 [INFO] fastapi_app.app: POST /api/v1/query/text 200 62454.0ms
2026-02-05 10:32:58 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:32:59 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1250.6ms
2026-02-05 10:32:59 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 10:32:59 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:33:00 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:33:00 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 10:33:00 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:33:07 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:33:07 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8131.1ms
2026-02-05 10:38:45 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:38:46 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1287.1ms
2026-02-05 10:38:46 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 10:38:47 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:38:48 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:38:48 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 10:38:48 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:39:48 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 95, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error timed out>
2026-02-05 10:39:48 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:39:48 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 61465.8ms
2026-02-05 10:39:48 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 10:39:49 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 10:39:58 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9831.4ms
2026-02-05 10:40:02 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:40:04 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1357.5ms
2026-02-05 10:40:04 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 10:40:04 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:40:05 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:40:05 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 10:40:05 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:40:14 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:40:14 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9832.7ms
2026-02-05 10:40:14 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 10:40:15 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 10:40:23 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9850.4ms
2026-02-05 10:40:24 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:40:25 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1176.2ms
2026-02-05 10:40:25 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 10:40:25 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:40:26 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:40:26 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6, top_k=3
2026-02-05 10:40:26 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:40:31 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:40:31 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6551.7ms
2026-02-05 10:40:31 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 10:40:32 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 10:40:41 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 10118.7ms
2026-02-05 10:40:41 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:40:42 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 280.7ms
2026-02-05 10:40:42 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 10:40:42 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:40:43 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:40:43 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 10:40:43 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:41:43 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 95, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error timed out>
2026-02-05 10:41:43 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:41:43 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 61395.9ms
2026-02-05 10:41:43 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 10:41:44 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 10:41:50 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7097.3ms
2026-02-05 10:41:50 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:42:13 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 22784.4ms
2026-02-05 10:42:13 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 10:42:13 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:42:14 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:42:14 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 10:42:14 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:42:30 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:42:30 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 16787.9ms
2026-02-05 10:42:30 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 10:42:31 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 10:43:31 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 95, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error timed out>
2026-02-05 10:43:31 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 61255.1ms
2026-02-05 10:43:31 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 10:43:32 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1196.0ms
2026-02-05 10:43:32 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 10:43:32 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 10:43:33 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 10:43:33 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 10:43:33 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 10:43:38 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 10:43:38 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 5286.4ms
2026-02-05 10:43:38 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 10:43:38 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 10:43:45 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7006.9ms
2026-02-05 11:45:47 [INFO] fastapi_app.app: GET / 404 0.6ms
2026-02-05 11:45:47 [INFO] fastapi_app.app: GET /login 404 0.7ms
2026-02-05 12:32:56 [INFO] fastapi_app.app: GET / 404 0.4ms
2026-02-05 12:33:32 [INFO] fastapi_app.app: GET / 404 0.6ms
2026-02-05 12:33:33 [INFO] fastapi_app.app: GET / 404 0.5ms
2026-02-05 12:33:33 [INFO] fastapi_app.app: GET /favicon.ico 404 0.5ms
2026-02-05 12:33:33 [INFO] fastapi_app.app: GET /robots.txt 404 0.4ms
2026-02-05 12:43:06 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 12:43:07 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1318.6ms
2026-02-05 12:43:07 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 12:43:07 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 12:43:09 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 12:43:09 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 12:43:09 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 12:43:18 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 12:43:18 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 10909.8ms
2026-02-05 12:43:18 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 12:43:19 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 12:43:28 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9785.7ms
2026-02-05 12:43:32 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 12:43:34 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1359.7ms
2026-02-05 12:43:34 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 12:43:34 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 12:43:35 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 12:43:35 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 12:43:35 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 12:43:43 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 12:43:43 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9411.8ms
2026-02-05 12:43:43 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 12:43:44 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 12:43:55 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 11864.8ms
2026-02-05 12:43:55 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 12:43:56 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1206.9ms
2026-02-05 12:43:56 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 12:43:56 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 12:43:57 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 12:43:57 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6, top_k=3
2026-02-05 12:43:57 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 12:44:03 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 12:44:03 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6865.9ms
2026-02-05 12:44:03 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 12:44:04 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 12:44:12 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9028.1ms
2026-02-05 12:44:12 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 12:44:12 [INFO] fastapi_app.app: GET / 404 1.2ms
2026-02-05 12:44:12 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 269.7ms
2026-02-05 12:44:12 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 12:44:13 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 12:44:14 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 12:44:14 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 12:44:14 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 12:44:23 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 12:44:23 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 10598.7ms
2026-02-05 12:44:23 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 12:44:24 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 12:44:33 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9728.0ms
2026-02-05 12:44:33 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 12:44:56 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 22982.7ms
2026-02-05 12:44:56 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 12:44:56 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 12:44:57 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 12:44:57 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 12:44:57 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 12:45:07 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 12:45:07 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 11207.9ms
2026-02-05 12:45:07 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 12:45:08 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 12:46:08 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 95, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error timed out>
2026-02-05 12:46:08 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 61210.3ms
2026-02-05 12:46:08 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 12:46:09 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1208.5ms
2026-02-05 12:46:09 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 12:46:10 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 12:46:11 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 12:46:11 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 12:46:11 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 12:47:11 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 95, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error timed out>
2026-02-05 12:47:11 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 12:47:11 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 61329.9ms
2026-02-05 12:47:11 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 12:47:12 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 12:47:17 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 5809.0ms
2026-02-05 13:00:23 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 13:00:24 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 13:00:24 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:00:25 [INFO] src.generator: 加载 decoder-only: /home/xwh/models/DeepSeek-OCR-2
2026-02-05 13:01:14 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 50250.8ms
2026-02-05 13:01:14 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 13:01:14 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:01:15 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:01:15 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 13:01:15 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:03:26 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 110] Connection timed out>
2026-02-05 13:03:26 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:03:26 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 132039.2ms
2026-02-05 13:03:26 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:03:27 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:03:35 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9077.0ms
2026-02-05 13:03:39 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:03:41 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1426.1ms
2026-02-05 13:03:41 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 13:03:41 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:03:42 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:03:42 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 13:03:42 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:03:51 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:03:51 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 10368.6ms
2026-02-05 13:03:51 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:03:52 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:04:02 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 10733.2ms
2026-02-05 13:04:02 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:04:03 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 620.5ms
2026-02-05 13:04:03 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 13:04:03 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:04:04 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:04:04 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6, top_k=3
2026-02-05 13:04:04 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:04:11 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:04:11 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8033.1ms
2026-02-05 13:04:11 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:04:12 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:06:22 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 110] Connection timed out>
2026-02-05 13:06:22 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 131229.1ms
2026-02-05 13:06:22 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:06:22 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 335.8ms
2026-02-05 13:06:22 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 13:06:22 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:06:24 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:06:24 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 13:06:24 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:06:31 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:06:31 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8397.0ms
2026-02-05 13:06:31 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:06:32 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:06:40 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9608.3ms
2026-02-05 13:06:40 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:07:04 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 23656.5ms
2026-02-05 13:07:04 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 13:07:04 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:07:05 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:07:05 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 13:07:05 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:07:13 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:07:13 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9313.6ms
2026-02-05 13:07:13 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:07:14 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:07:22 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9242.5ms
2026-02-05 13:07:22 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:07:24 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1613.5ms
2026-02-05 13:07:24 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 13:07:24 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:07:25 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:07:25 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 13:07:25 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:07:34 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:07:34 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9598.3ms
2026-02-05 13:07:34 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:07:35 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:07:44 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9883.6ms
2026-02-05 13:11:07 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 13:11:07 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 13:11:07 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:11:08 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 13:11:08 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 13:11:09 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-05 13:11:09 [ERROR] fastapi_app.service.query_service: text-only answering failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 405, in query_text_only
    answer = _answer_with_text_only(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 82, in _answer_with_text_only
    raw = (gen.generate(system, user) or "").strip()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/src/generator.py", line 589, in generate
    response = self.client.chat.completions.create(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_value', 'message': 'max_tokens is too large: 32768. This model supports at most 16384 completion tokens, whereas you provided 32768. (request id: 20260205131108737697397F81hdLc1) (request id: 20260205131108732817762SLGTS3iO) (request id: 20260205131108731806735OqdYx4py) (request id: 20260205131108698167299nz2irlel)', 'param': 'max_tokens', 'type': 'invalid_request_error'}}
2026-02-05 13:11:09 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2148.6ms
2026-02-05 13:11:09 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 13:11:09 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:11:10 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:11:10 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 13:11:10 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:11:58 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 13:11:58 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 13:11:59 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:11:59 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 13:11:59 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 13:12:59 [INFO] openai._base_client: Retrying request to /chat/completions in 0.450423 seconds
2026-02-05 13:13:01 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-05 13:13:01 [ERROR] fastapi_app.service.query_service: text-only answering failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 405, in query_text_only
    answer = _answer_with_text_only(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 82, in _answer_with_text_only
    raw = (gen.generate(system, user) or "").strip()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/src/generator.py", line 589, in generate
    response = self.client.chat.completions.create(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_value', 'message': 'max_tokens is too large: 32768. This model supports at most 16384 completion tokens, whereas you provided 32768. (request id: 20260205131301388511673l6dJfjy5) (request id: 20260205131301375670563vUn63U5V) (request id: 20260205131301374699461lPVfITVQ) (request id: 20260205131301340235705gzzlAP1D)', 'param': 'max_tokens', 'type': 'invalid_request_error'}}
2026-02-05 13:13:01 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 63274.4ms
2026-02-05 13:13:01 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 13:13:02 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:13:03 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:13:03 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 13:13:03 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:13:08 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:13:08 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:13:08 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6608.5ms
2026-02-05 13:13:08 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:13:09 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:13:14 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:13:14 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 5502.5ms
2026-02-05 13:13:15 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:13:17 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-05 13:13:17 [ERROR] fastapi_app.service.query_service: text-only answering failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 405, in query_text_only
    answer = _answer_with_text_only(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 82, in _answer_with_text_only
    raw = (gen.generate(system, user) or "").strip()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/src/generator.py", line 589, in generate
    response = self.client.chat.completions.create(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_value', 'message': 'max_tokens is too large: 32768. This model supports at most 16384 completion tokens, whereas you provided 32768. (request id: 2026020513131648567494443uiaWJ7) (request id: 202602051313164813795729Q2umD3i) (request id: 20260205131316480773166TJM9EaOT) (request id: 20260205131316446418076stksC4L5)', 'param': 'max_tokens', 'type': 'invalid_request_error'}}
2026-02-05 13:13:17 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1482.5ms
2026-02-05 13:13:17 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 13:13:17 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:13:18 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:13:18 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 13:13:18 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:13:23 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:13:23 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:13:23 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6258.5ms
2026-02-05 13:13:23 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:13:24 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:13:30 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:13:30 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6967.0ms
2026-02-05 13:13:30 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:14:08 [INFO] fastapi_app.app: GET /api/v2/heartbeat 404 0.7ms
2026-02-05 13:14:08 [INFO] fastapi_app.app: GET / 404 0.5ms
2026-02-05 13:14:08 [INFO] fastapi_app.app: GET / 404 0.5ms
2026-02-05 13:14:08 [INFO] fastapi_app.app: GET /favicon.ico 404 0.4ms
2026-02-05 13:14:30 [INFO] openai._base_client: Retrying request to /chat/completions in 0.435333 seconds
2026-02-05 13:14:32 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-05 13:14:32 [ERROR] fastapi_app.service.query_service: text-only answering failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 405, in query_text_only
    answer = _answer_with_text_only(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 82, in _answer_with_text_only
    raw = (gen.generate(system, user) or "").strip()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/src/generator.py", line 589, in generate
    response = self.client.chat.completions.create(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_value', 'message': 'max_tokens is too large: 32768. This model supports at most 16384 completion tokens, whereas you provided 32768. (request id: 20260205131431565709770Z6Yn4Zar) (request id: 20260205131431561131090rEHgh6nw) (request id: 20260205131431560467439RE8pSSTX) (request id: 20260205131431527266272PeBFKv5z)', 'param': 'max_tokens', 'type': 'invalid_request_error'}}
2026-02-05 13:14:32 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 62293.2ms
2026-02-05 13:14:32 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 13:14:32 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:14:33 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:14:33 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6, top_k=3
2026-02-05 13:14:33 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:14:39 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:14:39 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:14:39 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6811.6ms
2026-02-05 13:14:39 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:14:40 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:14:44 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:14:44 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 5181.2ms
2026-02-05 13:14:44 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:15:44 [INFO] openai._base_client: Retrying request to /chat/completions in 0.422452 seconds
2026-02-05 13:15:46 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-05 13:15:46 [ERROR] fastapi_app.service.query_service: text-only answering failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 405, in query_text_only
    answer = _answer_with_text_only(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 82, in _answer_with_text_only
    raw = (gen.generate(system, user) or "").strip()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/src/generator.py", line 589, in generate
    response = self.client.chat.completions.create(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_value', 'message': 'max_tokens is too large: 32768. This model supports at most 16384 completion tokens, whereas you provided 32768. (request id: 20260205131545909526922ifBPwqzJ) (request id: 20260205131545904945236K7c1J409) (request id: 20260205131545903898934p9K79kDp) (request id: 20260205131545870652966vitWqnEw)', 'param': 'max_tokens', 'type': 'invalid_request_error'}}
2026-02-05 13:15:46 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 61845.0ms
2026-02-05 13:15:46 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 13:15:46 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:15:47 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:15:47 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 13:15:48 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:15:59 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:15:59 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:15:59 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 12780.4ms
2026-02-05 13:15:59 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:16:00 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:16:08 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:16:08 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9264.9ms
2026-02-05 13:16:08 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:16:09 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-05 13:16:09 [ERROR] fastapi_app.service.query_service: text-only answering failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 405, in query_text_only
    answer = _answer_with_text_only(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 82, in _answer_with_text_only
    raw = (gen.generate(system, user) or "").strip()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/src/generator.py", line 589, in generate
    response = self.client.chat.completions.create(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_value', 'message': 'max_tokens is too large: 32768. This model supports at most 16384 completion tokens, whereas you provided 32768. (request id: 202602051316093026223622VaDKo31) (request id: 20260205131609290116276Uk1KEnLu) (request id: 20260205131609289286116WDqvHmfw) (request id: 20260205131609255962853juhGj5BM)', 'param': 'max_tokens', 'type': 'invalid_request_error'}}
2026-02-05 13:16:09 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1345.1ms
2026-02-05 13:16:09 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 13:16:09 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:16:11 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:16:11 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 13:16:11 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:16:19 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:16:19 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:16:19 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9464.3ms
2026-02-05 13:16:19 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:16:20 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:16:26 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:16:26 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7541.7ms
2026-02-05 13:16:26 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:17:27 [INFO] openai._base_client: Retrying request to /chat/completions in 0.407172 seconds
2026-02-05 13:17:28 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-05 13:17:28 [ERROR] fastapi_app.service.query_service: text-only answering failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 405, in query_text_only
    answer = _answer_with_text_only(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 82, in _answer_with_text_only
    raw = (gen.generate(system, user) or "").strip()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/src/generator.py", line 589, in generate
    response = self.client.chat.completions.create(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_value', 'message': 'max_tokens is too large: 32768. This model supports at most 16384 completion tokens, whereas you provided 32768. (request id: 20260205131728146881708VasdrKCx) (request id: 20260205131728142965774XAaQJlRn) (request id: 20260205131728141601853kjt9GIoe) (request id: 20260205131728107059390UTIhYQAo)', 'param': 'max_tokens', 'type': 'invalid_request_error'}}
2026-02-05 13:17:28 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 61748.3ms
2026-02-05 13:17:28 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 13:17:28 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:17:30 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:17:30 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 13:17:30 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:17:38 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:17:38 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:17:38 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9380.7ms
2026-02-05 13:17:38 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:17:39 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:17:45 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:17:45 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7704.9ms
2026-02-05 13:18:41 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 13:18:41 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 13:18:41 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:18:42 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 13:18:42 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 13:18:43 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-05 13:18:43 [ERROR] fastapi_app.service.query_service: text-only answering failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 405, in query_text_only
    answer = _answer_with_text_only(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 82, in _answer_with_text_only
    raw = (gen.generate(system, user) or "").strip()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/src/generator.py", line 589, in generate
    response = self.client.chat.completions.create(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_value', 'message': 'max_tokens is too large: 32768. This model supports at most 16384 completion tokens, whereas you provided 32768. (request id: 20260205131843373607954RQPvcdFp) (request id: 20260205131843369248275jfafVRFu) (request id: 20260205131843367465637j41NZly9) (request id: 20260205131843332927022QP2bm11O)', 'param': 'max_tokens', 'type': 'invalid_request_error'}}
2026-02-05 13:18:43 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2827.9ms
2026-02-05 13:18:43 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 13:18:44 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:18:45 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:18:45 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 13:18:45 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:18:52 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:18:52 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:18:52 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8455.7ms
2026-02-05 13:18:52 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:18:53 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:19:00 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 13:19:00 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7876.4ms
2026-02-05 13:19:01 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:19:03 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-05 13:19:03 [ERROR] fastapi_app.service.query_service: text-only answering failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 405, in query_text_only
    answer = _answer_with_text_only(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 82, in _answer_with_text_only
    raw = (gen.generate(system, user) or "").strip()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/src/generator.py", line 589, in generate
    response = self.client.chat.completions.create(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_value', 'message': 'max_tokens is too large: 32768. This model supports at most 16384 completion tokens, whereas you provided 32768. (request id: 20260205131902745070020Ev3NlfLr) (request id: 20260205131902740876374U3CJvNDa) (request id: 20260205131902740303932hlpeREl9) (request id: 202602051319027057618189QNYDbpm)', 'param': 'max_tokens', 'type': 'invalid_request_error'}}
2026-02-05 13:19:03 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1477.3ms
2026-02-05 13:19:03 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 13:19:03 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:20:07 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 13:20:07 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 13:20:07 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:20:08 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 13:20:08 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 13:20:11 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:20:11 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 4014.9ms
2026-02-05 13:20:11 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 13:20:11 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:20:12 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:20:12 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 13:20:12 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:20:22 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:20:22 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 11512.2ms
2026-02-05 13:20:22 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:20:23 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:20:34 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 12338.8ms
2026-02-05 13:20:39 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:20:41 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:20:41 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2279.4ms
2026-02-05 13:20:41 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 13:20:41 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:20:43 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:20:43 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 13:20:43 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:20:56 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:20:56 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 14229.7ms
2026-02-05 13:20:56 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:20:57 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:21:08 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 12584.9ms
2026-02-05 13:21:08 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:21:11 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:21:11 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2553.4ms
2026-02-05 13:21:11 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 13:21:11 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:21:12 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:21:12 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6, top_k=3
2026-02-05 13:21:12 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:23:22 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 110] Connection timed out>
2026-02-05 13:23:22 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:23:22 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 130847.2ms
2026-02-05 13:23:22 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:23:23 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:25:33 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 99, in _openai_compat
    with urllib.request.urlopen(req, timeout=int(self.cfg.openai_timeout)) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 110] Connection timed out>
2026-02-05 13:25:33 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 131070.6ms
2026-02-05 13:25:33 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:26:33 [INFO] openai._base_client: Retrying request to /chat/completions in 0.424089 seconds
2026-02-05 13:27:38 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 13:27:39 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 13:27:39 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:27:39 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 13:27:39 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 13:27:42 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:27:43 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 4753.2ms
2026-02-05 13:27:43 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 13:27:43 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:27:44 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:27:44 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 13:27:44 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:27:52 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:27:52 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9130.1ms
2026-02-05 13:27:52 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:27:53 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:28:01 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8229.7ms
2026-02-05 13:28:05 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:28:08 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:28:08 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2468.4ms
2026-02-05 13:28:08 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 13:28:08 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:28:09 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:28:09 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 13:28:09 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:28:19 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:28:19 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 10781.2ms
2026-02-05 13:28:19 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:28:20 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:28:26 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6985.0ms
2026-02-05 13:28:26 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:28:28 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:28:28 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2111.4ms
2026-02-05 13:28:28 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 13:28:28 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:28:29 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:28:29 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6, top_k=3
2026-02-05 13:28:29 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:28:36 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:28:36 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8410.5ms
2026-02-05 13:28:36 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:28:37 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:28:46 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 10113.5ms
2026-02-05 13:28:46 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:28:48 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:28:48 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2068.6ms
2026-02-05 13:28:48 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 13:28:48 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:28:50 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:28:50 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 13:28:50 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:28:57 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:28:57 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8894.5ms
2026-02-05 13:28:57 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:28:58 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:29:06 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9105.2ms
2026-02-05 13:29:06 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:30:07 [INFO] openai._base_client: Retrying request to /chat/completions in 0.442228 seconds
2026-02-05 13:30:09 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:30:09 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 62827.1ms
2026-02-05 13:30:09 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 13:30:09 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:30:11 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:30:11 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 13:30:11 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:30:19 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:30:19 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9639.2ms
2026-02-05 13:30:19 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:30:20 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:30:28 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9095.9ms
2026-02-05 13:30:28 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:30:30 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:30:30 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2359.0ms
2026-02-05 13:30:30 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 13:30:30 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:30:32 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:30:32 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 13:30:32 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:30:39 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:30:39 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8548.4ms
2026-02-05 13:30:39 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:30:40 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:30:48 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9102.5ms
2026-02-05 13:43:55 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:43:58 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:43:58 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2639.0ms
2026-02-05 13:43:58 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 13:43:58 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:43:59 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:43:59 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 13:43:59 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:44:07 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:44:07 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8872.3ms
2026-02-05 13:44:07 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:44:08 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:44:17 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 10399.7ms
2026-02-05 13:44:22 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:44:25 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:44:25 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2827.6ms
2026-02-05 13:44:25 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 13:44:25 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:44:26 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:44:26 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 13:44:26 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:44:38 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:44:38 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 13341.2ms
2026-02-05 13:44:38 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:44:39 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:44:49 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 11170.1ms
2026-02-05 13:44:49 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:44:52 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:44:52 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2916.8ms
2026-02-05 13:44:52 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 13:44:52 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:44:53 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:44:53 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6, top_k=3
2026-02-05 13:44:53 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:45:00 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:45:00 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8040.1ms
2026-02-05 13:45:00 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:45:01 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:45:08 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7706.5ms
2026-02-05 13:45:08 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:46:08 [INFO] openai._base_client: Retrying request to /chat/completions in 0.398301 seconds
2026-02-05 13:46:10 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:46:10 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 62518.1ms
2026-02-05 13:46:10 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 13:46:10 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:46:12 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:46:12 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 13:46:12 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:46:21 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:46:21 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 10178.6ms
2026-02-05 13:46:21 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:46:22 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:46:30 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9155.6ms
2026-02-05 13:46:30 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:46:32 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:46:32 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2256.8ms
2026-02-05 13:46:32 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 13:46:32 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:46:34 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:46:34 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 13:46:34 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:46:41 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:46:41 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9264.8ms
2026-02-05 13:46:41 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:46:42 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:46:52 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 10298.5ms
2026-02-05 13:46:52 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:46:54 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:46:54 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2123.2ms
2026-02-05 13:46:54 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 13:46:54 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:46:55 [INFO] fastapi_app.service.query_service: Rerank applied to image candidates
2026-02-05 13:46:55 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 13:46:55 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:47:02 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:47:02 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8184.9ms
2026-02-05 13:47:02 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:47:03 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:47:11 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8611.8ms
2026-02-05 13:49:07 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 13:49:07 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 13:49:07 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:49:08 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 13:49:08 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 13:49:10 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:49:10 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2934.7ms
2026-02-05 13:49:10 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 13:49:10 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:49:10 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 13:49:10 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:49:18 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:49:18 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8076.9ms
2026-02-05 13:49:18 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:49:18 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:49:26 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8771.9ms
2026-02-05 13:49:31 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:49:34 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:49:34 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2999.7ms
2026-02-05 13:49:34 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 13:49:34 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:49:34 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 13:49:34 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:49:44 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:49:44 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9639.6ms
2026-02-05 13:49:44 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:49:44 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:49:53 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9603.0ms
2026-02-05 13:49:53 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:49:55 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:49:55 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1980.5ms
2026-02-05 13:49:55 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 13:49:55 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:49:55 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 13:49:55 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:50:02 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:50:02 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6271.3ms
2026-02-05 13:50:02 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:50:02 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:50:10 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8551.0ms
2026-02-05 13:50:10 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:50:12 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:50:12 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1815.1ms
2026-02-05 13:50:12 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 13:50:12 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:50:12 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 13:50:12 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:50:20 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:50:20 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8266.1ms
2026-02-05 13:50:20 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:50:20 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:50:29 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8704.1ms
2026-02-05 13:50:29 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:50:31 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:50:31 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2225.1ms
2026-02-05 13:50:31 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 13:50:31 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:50:31 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 13:50:31 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:50:40 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:50:40 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8673.2ms
2026-02-05 13:50:40 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:50:40 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:50:47 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7432.8ms
2026-02-05 13:50:47 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:50:49 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:50:49 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1729.6ms
2026-02-05 13:50:49 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 13:50:49 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:50:49 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 13:50:49 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:50:56 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:50:56 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 7281.1ms
2026-02-05 13:50:56 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:50:56 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:51:03 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6398.4ms
2026-02-05 13:54:01 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 13:54:01 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 13:55:01 [ERROR] fastapi_app.service.query_service: query_enhance failed; using original query
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 188, in _maybe_enhance_query
    with urllib.request.urlopen(req, timeout=int(getattr(oai, "timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error timed out>
2026-02-05 13:55:01 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:55:02 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 13:55:02 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 13:55:04 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:55:04 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 63407.0ms
2026-02-05 13:56:04 [ERROR] fastapi_app.service.query_service: query_enhance failed; using original query
Traceback (most recent call last):
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1344, in do_open
    h.request(req.get_method(), req.selector, req.data, headers,
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1338, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1384, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1472, in connect
    super().connect()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/http/client.py", line 1003, in connect
    self.sock = self._create_connection(
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 865, in create_connection
    raise exceptions[0]
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/socket.py", line 850, in create_connection
    sock.connect(sa)
TimeoutError: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 188, in _maybe_enhance_query
    with urllib.request.urlopen(req, timeout=int(getattr(oai, "timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 532, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1392, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 1347, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error timed out>
2026-02-05 13:56:04 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 13:56:04 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:56:04 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 13:56:04 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:56:57 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 13:56:57 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 13:57:00 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:57:01 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 13:57:01 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 13:57:03 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:57:03 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 5705.7ms
2026-02-05 13:57:06 [INFO] fastapi_app.service.query_service: Datasets where LadaBERT achieves state-of-the-art performance, including benchmarks, task-specific datasets, and comparisons on metrics like accuracy, F1 score, or improvement over baselines in NLP tasks.
2026-02-05 13:57:06 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:57:06 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 13:57:06 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:57:14 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:57:14 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 11500.8ms
2026-02-05 13:57:18 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:57:18 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:57:25 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 10884.4ms
2026-02-05 13:57:33 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:57:35 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:57:35 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 5551.8ms
2026-02-05 13:57:38 [INFO] fastapi_app.service.query_service: Inter-annotator agreement metrics, Fleiss' kappa, Cohen's kappa, Krippendorff’s alpha, percentage agreement, reliability scores, annotation consistency, agreement evaluation results, reported in methodology or results sections.
2026-02-05 13:57:38 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:57:38 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_3, top_k=3
2026-02-05 13:57:38 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:57:45 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:57:45 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 10156.0ms
2026-02-05 13:57:48 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:57:48 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:57:55 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9957.3ms
2026-02-05 13:57:59 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:58:01 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:58:01 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 5407.1ms
2026-02-05 13:58:04 [INFO] fastapi_app.service.query_service: Dataset size, number of samples, total instances, training set size, validation set size, test set size, data distribution, and corpus statistics in the context of the study or experiment.
2026-02-05 13:58:04 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:58:04 [INFO] fastapi_app.service.query_service: Query text collection: pdf_political_speech_generation_4, top_k=3
2026-02-05 13:58:04 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:58:10 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:58:10 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9254.2ms
2026-02-05 13:58:13 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:58:14 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:58:23 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 12957.0ms
2026-02-05 13:58:26 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:58:29 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:58:29 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 5679.7ms
2026-02-05 13:58:32 [INFO] fastapi_app.service.query_service: Available source domains for selection across 20 categories, domain list, domain types or groups in source selection process.
2026-02-05 13:58:32 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:58:32 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 13:58:32 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:58:40 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:58:40 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 11426.6ms
2026-02-05 13:58:43 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:58:43 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:58:50 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 10094.6ms
2026-02-05 13:58:53 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:58:55 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:58:55 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 5315.2ms
2026-02-05 13:58:59 [INFO] fastapi_app.service.query_service: Improvement on RACE dataset by introduced approach, performance gain, accuracy increase, percentage improvement, comparison to baseline models, evaluation metrics, results on reading comprehension task.
2026-02-05 13:59:00 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 13:59:00 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 13:59:00 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 13:59:08 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 13:59:08 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 12017.6ms
2026-02-05 13:59:45 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 13:59:45 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 13:59:54 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 46937.3ms
2026-02-05 13:59:57 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 13:59:59 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 13:59:59 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 5017.1ms
2026-02-05 14:00:03 [INFO] fastapi_app.service.query_service: Suggested directions, future work, or research opportunities for improving language models, enhancing performance, addressing limitations, advancing architectures, training strategies, datasets, benchmarks, or evaluation metrics in NLP.
2026-02-05 14:00:03 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:00:03 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_14, top_k=3
2026-02-05 14:00:03 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:00:13 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:00:13 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 13429.6ms
2026-02-05 14:00:16 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:00:16 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:00:22 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8842.6ms
2026-02-05 14:04:49 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 14:04:49 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 14:04:49 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:04:50 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 14:04:50 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 14:04:53 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:04:53 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 4371.2ms
2026-02-05 14:04:53 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 14:04:53 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:04:53 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 14:04:53 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:05:00 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:05:00 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 7205.4ms
2026-02-05 14:05:00 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:05:00 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:05:08 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7397.1ms
2026-02-05 14:05:12 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:06:12 [INFO] openai._base_client: Retrying request to /chat/completions in 0.433410 seconds
2026-02-05 14:07:13 [INFO] openai._base_client: Retrying request to /chat/completions in 0.794662 seconds
2026-02-05 14:07:16 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:07:16 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 124244.9ms
2026-02-05 14:07:16 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 14:07:17 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:07:17 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 14:07:17 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:07:30 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:07:30 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 13796.6ms
2026-02-05 14:07:30 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:07:30 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:07:37 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6308.3ms
2026-02-05 14:07:37 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:07:39 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:07:39 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1921.7ms
2026-02-05 14:07:39 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 14:07:39 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:07:39 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 14:07:39 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:07:45 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:07:45 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6919.2ms
2026-02-05 14:07:46 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:07:46 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:07:53 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7513.4ms
2026-02-05 14:07:53 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:08:53 [INFO] openai._base_client: Retrying request to /chat/completions in 0.422915 seconds
2026-02-05 14:08:55 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:08:55 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 62465.9ms
2026-02-05 14:08:55 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 14:08:56 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:08:56 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 14:08:56 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:09:02 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:09:02 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6620.9ms
2026-02-05 14:09:02 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:09:02 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:09:12 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9615.0ms
2026-02-05 14:09:12 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:09:14 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:09:14 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1979.8ms
2026-02-05 14:09:14 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 14:09:14 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:09:14 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 14:09:14 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:09:22 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:09:22 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 7807.1ms
2026-02-05 14:09:22 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:09:22 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:09:32 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 10393.8ms
2026-02-05 14:09:32 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:09:34 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:09:34 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2218.0ms
2026-02-05 14:09:34 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 14:09:34 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:09:34 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 14:09:34 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:09:41 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:09:41 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6780.4ms
2026-02-05 14:09:41 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:09:41 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:09:49 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8099.1ms
2026-02-05 14:13:28 [INFO] torch._subclasses.fake_tensor: FakeTensor cache stats:
2026-02-05 14:13:28 [INFO] torch._subclasses.fake_tensor:   cache_hits: 0
2026-02-05 14:13:28 [INFO] torch._subclasses.fake_tensor:   cache_misses: 0
2026-02-05 14:33:15 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 14:33:16 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 14:33:16 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:33:16 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 14:33:16 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 14:33:19 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:33:19 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 3919.5ms
2026-02-05 14:33:19 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 14:33:19 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:33:19 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 14:33:19 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:33:19 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:25 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 350, in query
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:25 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:33:25 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 5525.0ms
2026-02-05 14:33:25 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:33:25 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:33:25 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:26 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 498, in query_vision_only
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:26 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 1179.3ms
2026-02-05 14:33:31 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:33:35 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:33:35 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 4208.9ms
2026-02-05 14:33:35 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 14:33:35 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:33:35 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 14:33:35 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:33:35 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:36 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 350, in query
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:36 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:33:36 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 1224.3ms
2026-02-05 14:33:36 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:33:36 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:33:36 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:37 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 498, in query_vision_only
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:37 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 1197.7ms
2026-02-05 14:33:37 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:33:39 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:33:39 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1273.0ms
2026-02-05 14:33:39 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 14:33:39 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:33:39 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 14:33:39 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:33:39 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:40 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 350, in query
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:40 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:33:40 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 1279.7ms
2026-02-05 14:33:40 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:33:40 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:33:40 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:41 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 498, in query_vision_only
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:41 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 1186.3ms
2026-02-05 14:33:41 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:33:43 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:33:43 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1602.6ms
2026-02-05 14:33:43 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 14:33:43 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:33:43 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 14:33:43 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:33:43 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:44 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 350, in query
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:44 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:33:44 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 1269.3ms
2026-02-05 14:33:44 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:33:44 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:33:44 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:45 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 498, in query_vision_only
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:45 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 1179.0ms
2026-02-05 14:33:45 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:33:47 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:33:47 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2363.5ms
2026-02-05 14:33:47 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 14:33:48 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:33:48 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 14:33:48 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:33:48 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:49 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 350, in query
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:49 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:33:49 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 1351.4ms
2026-02-05 14:33:49 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:33:49 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:33:49 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:50 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 498, in query_vision_only
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:50 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 1195.1ms
2026-02-05 14:33:50 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:33:52 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:33:52 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1772.3ms
2026-02-05 14:33:52 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 14:33:52 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:33:52 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 14:33:52 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:33:52 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:53 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 350, in query
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:53 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:33:53 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 1387.2ms
2026-02-05 14:33:53 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:33:53 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:33:53 [WARNING] fastapi_app.service.answer_service: local_vl backend is not implemented yet; falling back to ocr2
2026-02-05 14:33:55 [ERROR] fastapi_app.service.query_service: OCR2 failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/query_service.py", line 498, in query_vision_only
    ocr_text = self.answer_service.generate_from_image(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 40, in generate_from_image
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 95, in run
    self._lazy_init()
  File "/home/xwh/VisRAG/fastapi_app/service/ocr_service.py", line 37, in _lazy_init
    self._llm = LLM(
                ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 510, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 486, in from_vllm_config
    return cls(
           ^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 275, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/utils.py", line 2456, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/worker.py", line 203, in load_model
    self.model_runner.load_model()
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1111, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 452, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 133, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepseek_ocr2.py", line 292, in __init__
    self.qwen2_model = build_qwen2_decoder_as_encoder()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 297, in build_qwen2_decoder_as_encoder
    decoder_as_encoder = Qwen2Decoder2Encoder(
                         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 235, in __init__
    self.model = CustomQwen2Decoder(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 62, in __init__
    self.model = self._create_custom_model(Qwen2Model, config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm/deepencoderv2/qwen2_d2e.py", line 174, in _create_custom_model
    return CustomQwen2ModelInner(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 789, in __init__
    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 167, in __init__
    torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 256.19 MiB is free. Process 2650729 has 3.91 GiB memory in use. Process 3904810 has 18.90 GiB memory in use. Including non-PyTorch memory, this process has 498.00 MiB memory in use. Of the allocated memory 188.66 MiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-05 14:33:55 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 1380.5ms
2026-02-05 14:37:13 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 14:37:13 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 14:37:13 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:37:14 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 14:37:14 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 14:37:16 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:37:16 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 3489.6ms
2026-02-05 14:37:16 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 14:37:16 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:37:16 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 14:37:16 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:37:16 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:16 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:37:16 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 139.5ms
2026-02-05 14:37:16 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:37:16 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:37:16 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:16 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 65.6ms
2026-02-05 14:37:21 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:37:23 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:37:23 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1615.1ms
2026-02-05 14:37:23 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 14:37:23 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:37:23 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 14:37:23 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:37:23 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:23 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:37:23 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 93.4ms
2026-02-05 14:37:23 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:37:23 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:37:23 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:23 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 63.7ms
2026-02-05 14:37:23 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:37:24 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:37:24 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1255.3ms
2026-02-05 14:37:24 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 14:37:24 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:37:24 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 14:37:24 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:37:24 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:24 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:37:24 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 74.8ms
2026-02-05 14:37:24 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:37:24 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:37:24 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:24 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 53.5ms
2026-02-05 14:37:24 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:37:26 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:37:26 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1462.2ms
2026-02-05 14:37:26 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 14:37:26 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:37:26 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 14:37:26 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:37:26 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:26 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:37:26 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 115.8ms
2026-02-05 14:37:26 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:37:26 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:37:26 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:26 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 62.7ms
2026-02-05 14:37:26 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:37:28 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:37:28 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1839.5ms
2026-02-05 14:37:28 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 14:37:28 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:37:28 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 14:37:28 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:37:28 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:28 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:37:28 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 93.1ms
2026-02-05 14:37:28 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:37:28 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:37:28 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:28 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 64.7ms
2026-02-05 14:37:28 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:37:30 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:37:30 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1680.8ms
2026-02-05 14:37:30 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 14:37:30 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:37:30 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 14:37:30 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (direct)
2026-02-05 14:37:30 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:30 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:37:30 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 89.2ms
2026-02-05 14:37:30 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:37:30 [INFO] fastapi_app.service.query_service: Running OCR2 on retrieved page (vision-only)
2026-02-05 14:37:30 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:37:30 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 63.9ms
2026-02-05 14:38:34 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 14:38:34 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 14:38:34 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:38:35 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 14:38:35 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 14:38:38 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:38:38 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 4097.1ms
2026-02-05 14:38:38 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 14:38:38 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:38:38 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 14:38:38 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:38 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:38 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:38:38 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 121.6ms
2026-02-05 14:38:38 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:38:38 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:38 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:38 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 67.6ms
2026-02-05 14:38:43 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:38:45 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:38:45 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1628.2ms
2026-02-05 14:38:45 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 14:38:45 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:38:45 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 14:38:45 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:45 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:45 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:38:45 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 90.6ms
2026-02-05 14:38:45 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:38:45 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:45 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:45 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 68.0ms
2026-02-05 14:38:45 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:38:46 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:38:46 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1429.0ms
2026-02-05 14:38:46 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 14:38:46 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:38:46 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 14:38:46 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:46 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:46 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:38:46 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 88.5ms
2026-02-05 14:38:46 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:38:46 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:46 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:46 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 62.5ms
2026-02-05 14:38:46 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:38:48 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:38:48 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1282.6ms
2026-02-05 14:38:48 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 14:38:48 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:38:48 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 14:38:48 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:48 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:48 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:38:48 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 98.4ms
2026-02-05 14:38:48 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:38:48 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:48 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:48 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 66.8ms
2026-02-05 14:38:48 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:38:50 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:38:50 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1918.6ms
2026-02-05 14:38:50 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 14:38:50 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:38:50 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 14:38:50 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:50 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:50 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:38:50 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 77.3ms
2026-02-05 14:38:50 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:38:50 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:50 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:50 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 65.4ms
2026-02-05 14:38:50 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:38:52 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:38:52 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2141.8ms
2026-02-05 14:38:52 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 14:38:52 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:38:52 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 14:38:52 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:52 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:52 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:38:52 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 86.5ms
2026-02-05 14:38:52 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:38:52 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:38:52 [ERROR] fastapi_app.service.answer_service: local_vl answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 103, in _local_vl
    with urllib.request.urlopen(req, timeout=int(getattr(self.cfg, "local_vl_timeout", 60))) as resp:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/home/xwh/.conda/envs/deepseek-ocr/lib/python3.12/urllib/request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 400: Bad Request
2026-02-05 14:38:52 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 58.0ms
2026-02-05 14:40:18 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 14:40:19 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 14:40:19 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:40:19 [INFO] src.generator: 使用 OpenAI 模型: gpt-4o
2026-02-05 14:40:19 [INFO] src.generator: API Base: https://api.zhizengzeng.com/v1
2026-02-05 14:40:22 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:40:22 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 3862.0ms
2026-02-05 14:40:22 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 14:40:22 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:40:22 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 14:40:22 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:27 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:40:27 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 4806.0ms
2026-02-05 14:40:27 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:40:27 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:27 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 328.6ms
2026-02-05 14:40:32 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:40:34 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:40:34 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2667.9ms
2026-02-05 14:40:34 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 14:40:34 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:40:34 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 14:40:35 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:35 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:40:35 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 918.0ms
2026-02-05 14:40:35 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:40:35 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:36 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 334.8ms
2026-02-05 14:40:36 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:40:37 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:40:37 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 990.9ms
2026-02-05 14:40:37 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 14:40:37 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:40:37 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 14:40:37 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:38 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:40:38 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 791.5ms
2026-02-05 14:40:38 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:40:38 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:38 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 291.5ms
2026-02-05 14:40:38 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:40:39 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:40:39 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1353.5ms
2026-02-05 14:40:39 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 14:40:39 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:40:39 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 14:40:39 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:40 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:40:40 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 910.8ms
2026-02-05 14:40:40 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:40:40 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:40 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 338.8ms
2026-02-05 14:40:40 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:40:42 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:40:42 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2009.6ms
2026-02-05 14:40:43 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 14:40:43 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:40:43 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 14:40:43 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:43 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:40:43 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 841.7ms
2026-02-05 14:40:43 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:40:43 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:44 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 331.6ms
2026-02-05 14:40:44 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:40:45 [INFO] httpx: HTTP Request: POST https://api.zhizengzeng.com/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:40:45 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1550.8ms
2026-02-05 14:40:45 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 14:40:45 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:40:45 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 14:40:45 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:46 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:40:46 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 774.3ms
2026-02-05 14:40:46 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:40:46 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:40:46 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 317.9ms
2026-02-05 14:47:33 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 14:47:33 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 14:47:33 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:47:34 [INFO] src.generator: 使用 OpenAI 模型: /home/xwh/models/Qwen3-VL-2B-Instruct
2026-02-05 14:47:34 [INFO] src.generator: API Base: http://127.0.0.1:8001/v1
2026-02-05 14:47:35 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:47:35 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 2142.8ms
2026-02-05 14:47:35 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 14:47:35 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:47:35 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 14:47:35 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:36 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:47:36 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 476.5ms
2026-02-05 14:47:36 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:47:36 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:36 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 175.6ms
2026-02-05 14:47:40 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:47:40 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:47:40 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 105.9ms
2026-02-05 14:47:40 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 14:47:41 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:47:41 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 14:47:41 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:41 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:47:41 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 258.4ms
2026-02-05 14:47:41 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:47:41 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:41 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 179.8ms
2026-02-05 14:47:41 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:47:41 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:47:41 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 512.8ms
2026-02-05 14:47:41 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 14:47:41 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:47:41 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:47:42 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 217.6ms
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:42 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 211.4ms
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:47:42 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:47:42 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 107.3ms
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:47:42 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 216.3ms
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:42 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 191.1ms
2026-02-05 14:47:42 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:47:43 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:47:43 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 190.8ms
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:47:43 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 253.5ms
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:43 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 228.7ms
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:47:43 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:47:43 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 168.1ms
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 14:47:43 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:44 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:47:44 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 240.2ms
2026-02-05 14:47:44 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:47:44 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=local_vl)
2026-02-05 14:47:44 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 168.5ms
2026-02-05 14:51:07 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 14:51:07 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 14:51:08 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:51:08 [INFO] src.generator: 使用 OpenAI 模型: /home/xwh/models/Qwen3-VL-2B-Instruct
2026-02-05 14:51:08 [INFO] src.generator: API Base: http://127.0.0.1:8001/v1
2026-02-05 14:51:09 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:51:09 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1422.5ms
2026-02-05 14:51:09 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 14:51:09 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:51:09 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 14:51:09 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:51:25 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:51:25 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 16565.8ms
2026-02-05 14:51:25 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:51:25 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:51:42 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 16420.4ms
2026-02-05 14:51:47 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:51:47 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:51:47 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 150.3ms
2026-02-05 14:51:47 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 14:51:47 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:51:47 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 14:51:47 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:51:59 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:51:59 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 11680.7ms
2026-02-05 14:51:59 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:51:59 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:52:23 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 24710.4ms
2026-02-05 14:52:23 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:52:24 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:52:24 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 539.3ms
2026-02-05 14:52:24 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 14:52:24 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:52:24 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 14:52:24 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:52:30 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:52:30 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6019.9ms
2026-02-05 14:52:30 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:52:30 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:52:38 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8339.5ms
2026-02-05 14:52:38 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:52:38 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:52:38 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 77.8ms
2026-02-05 14:52:38 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 14:52:38 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:52:38 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 14:52:38 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:52:51 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:52:51 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 12333.2ms
2026-02-05 14:52:51 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:52:51 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:52:58 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7765.6ms
2026-02-05 14:52:59 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:52:59 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:52:59 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 190.4ms
2026-02-05 14:52:59 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 14:52:59 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:52:59 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 14:52:59 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:53:08 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:53:08 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9501.6ms
2026-02-05 14:53:08 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:53:08 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:53:41 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 32593.1ms
2026-02-05 14:53:41 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:53:41 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:53:41 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 194.9ms
2026-02-05 14:53:41 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 14:53:41 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:53:41 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 14:53:41 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:53:47 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:53:47 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 5813.4ms
2026-02-05 14:53:47 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:53:47 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:53:54 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6925.1ms
2026-02-05 14:56:07 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 14:56:07 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 14:56:07 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:56:08 [INFO] src.generator: 使用 OpenAI 模型: /home/xwh/models/Qwen3-VL-2B-Instruct
2026-02-05 14:56:08 [INFO] src.generator: API Base: http://127.0.0.1:8001/v1
2026-02-05 14:56:08 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:56:08 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1795.3ms
2026-02-05 14:56:09 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 14:56:09 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:56:09 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 14:56:09 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:56:16 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:56:16 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 7262.2ms
2026-02-05 14:56:16 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:56:16 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:56:22 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6687.7ms
2026-02-05 14:56:27 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:56:27 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:56:27 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 105.7ms
2026-02-05 14:56:27 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 14:56:27 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:56:27 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 14:56:27 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:56:37 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:56:37 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9692.6ms
2026-02-05 14:56:37 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:56:37 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:56:45 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8117.8ms
2026-02-05 14:56:45 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:56:46 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:56:46 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 411.6ms
2026-02-05 14:56:46 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 14:56:46 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:56:46 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 14:56:46 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:56:51 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:56:51 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 5407.1ms
2026-02-05 14:56:51 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:56:51 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:56:58 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6814.2ms
2026-02-05 14:56:58 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:56:58 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:56:58 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 83.4ms
2026-02-05 14:56:58 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 14:56:58 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:56:58 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 14:56:58 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:57:08 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:57:08 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9728.3ms
2026-02-05 14:57:08 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:57:08 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:57:17 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9047.1ms
2026-02-05 14:57:17 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:57:17 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:57:17 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 112.9ms
2026-02-05 14:57:17 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 14:57:17 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:57:17 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 14:57:17 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:57:26 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:57:26 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9277.1ms
2026-02-05 14:57:26 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:57:26 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:57:33 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7422.4ms
2026-02-05 14:57:34 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 14:57:34 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 14:57:34 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 162.7ms
2026-02-05 14:57:34 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 14:57:34 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 14:57:34 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 14:57:34 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:57:40 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 14:57:40 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 5915.1ms
2026-02-05 14:57:40 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 14:57:40 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 14:57:51 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 11513.3ms
2026-02-05 15:04:09 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:04:09 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:04:09 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 359.9ms
2026-02-05 15:04:09 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 15:04:09 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:04:09 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 15:04:09 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:04:17 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:04:17 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 7929.3ms
2026-02-05 15:04:17 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:04:17 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:04:24 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6957.4ms
2026-02-05 15:04:28 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:04:28 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:04:28 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 118.1ms
2026-02-05 15:04:28 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 15:04:29 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:04:29 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 15:04:29 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:04:36 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:04:36 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8006.5ms
2026-02-05 15:04:37 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:04:37 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:04:44 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7920.7ms
2026-02-05 15:04:44 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:04:45 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:04:45 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 956.9ms
2026-02-05 15:04:45 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 15:04:45 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:04:45 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 15:04:45 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:04:58 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:04:58 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 13012.2ms
2026-02-05 15:04:58 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:04:58 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:05:06 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7471.2ms
2026-02-05 15:05:06 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:05:06 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:05:06 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 99.6ms
2026-02-05 15:05:06 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 15:05:06 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:05:06 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 15:05:06 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:05:15 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:05:15 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8771.9ms
2026-02-05 15:05:15 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:05:15 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:05:25 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 10678.2ms
2026-02-05 15:05:26 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:05:26 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:05:26 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 204.2ms
2026-02-05 15:05:26 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 15:05:26 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:05:26 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 15:05:26 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:05:32 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:05:32 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6626.4ms
2026-02-05 15:05:32 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:05:32 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:05:40 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7252.5ms
2026-02-05 15:05:40 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:05:40 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:05:40 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 142.0ms
2026-02-05 15:05:40 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 15:05:40 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:05:40 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 15:05:40 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:05:50 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:05:50 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9762.6ms
2026-02-05 15:05:50 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:05:50 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:05:55 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 5883.8ms
2026-02-05 15:08:27 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:08:27 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:08:27 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 521.5ms
2026-02-05 15:08:27 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 15:08:27 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:08:27 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 15:08:27 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:08:35 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:08:35 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 7619.9ms
2026-02-05 15:08:35 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:08:35 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:08:42 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6995.1ms
2026-02-05 15:08:47 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:08:47 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:08:47 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 106.1ms
2026-02-05 15:08:47 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 15:08:47 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:08:47 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 15:08:47 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:08:54 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:08:54 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6842.3ms
2026-02-05 15:08:54 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:08:54 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:09:03 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9062.8ms
2026-02-05 15:09:03 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:09:03 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:09:03 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 417.5ms
2026-02-05 15:09:03 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 15:09:03 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:09:03 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 15:09:03 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:09:10 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:09:10 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6956.0ms
2026-02-05 15:09:10 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:09:10 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:09:15 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 5365.8ms
2026-02-05 15:09:15 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:09:16 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:09:16 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 91.3ms
2026-02-05 15:09:16 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 15:09:16 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:09:16 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 15:09:16 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:09:22 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:09:22 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6975.0ms
2026-02-05 15:09:23 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:09:23 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:09:30 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7400.6ms
2026-02-05 15:09:30 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:09:30 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:09:30 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 90.0ms
2026-02-05 15:09:30 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 15:09:30 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:09:30 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 15:09:30 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:09:37 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:09:37 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6819.1ms
2026-02-05 15:09:37 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:09:37 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:09:44 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6760.6ms
2026-02-05 15:09:44 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:09:44 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:09:44 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 121.6ms
2026-02-05 15:09:44 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 15:09:44 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:09:44 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 15:09:44 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:10:19 [ERROR] fastapi_app.service.answer_service: openai_compat answer generation failed
Traceback (most recent call last):
  File "/home/xwh/VisRAG/fastapi_app/service/answer_service.py", line 177, in _openai_compat
    content = (((data.get("choices") or [])[0] or {}).get("message") or {}).get("content")
                ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range
2026-02-05 15:10:19 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:10:19 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 35165.1ms
2026-02-05 15:10:19 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:10:19 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:10:28 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 9478.8ms
2026-02-05 15:50:32 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:50:32 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:50:32 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 528.3ms
2026-02-05 15:50:32 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 15:50:32 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:50:32 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 15:50:32 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:50:43 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:50:43 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 10374.5ms
2026-02-05 15:50:43 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:50:43 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:50:49 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6138.3ms
2026-02-05 15:50:54 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:50:54 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:50:54 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 113.1ms
2026-02-05 15:50:54 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 15:50:54 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:50:54 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 15:50:54 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:51:02 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:51:02 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 8875.8ms
2026-02-05 15:51:03 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:51:03 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:51:11 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8090.3ms
2026-02-05 15:51:11 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:51:11 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:51:11 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 521.6ms
2026-02-05 15:51:11 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 15:51:11 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:51:11 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 15:51:11 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:51:16 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:51:16 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 4775.5ms
2026-02-05 15:51:16 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:51:16 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:51:23 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6765.0ms
2026-02-05 15:51:23 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:51:23 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:51:23 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 79.4ms
2026-02-05 15:51:23 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 15:51:23 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:51:23 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 15:51:23 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:51:32 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:51:32 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 9132.4ms
2026-02-05 15:51:32 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:51:32 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:51:41 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8928.3ms
2026-02-05 15:51:41 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:51:41 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:51:41 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 188.4ms
2026-02-05 15:51:41 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 15:51:41 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:51:41 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 15:51:41 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:51:48 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:51:48 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 7162.9ms
2026-02-05 15:51:48 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:51:48 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:51:55 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6323.9ms
2026-02-05 15:51:55 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 15:51:55 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 15:51:55 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 131.8ms
2026-02-05 15:51:55 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 15:51:55 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 15:51:55 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 15:51:55 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:52:00 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 15:52:00 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 5662.4ms
2026-02-05 15:52:00 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 15:52:00 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 15:52:09 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8235.2ms
2026-02-05 16:00:17 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 16:00:18 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 16:00:18 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 1278.8ms
2026-02-05 16:00:18 [INFO] fastapi_app.service.query_service: On which datasets does LadaBERT achieve state-of-the-art?
2026-02-05 16:00:18 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 16:00:18 [INFO] fastapi_app.service.query_service: Query text collection: pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1, top_k=3
2026-02-05 16:00:18 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:00:25 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 16:00:25 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 7249.0ms
2026-02-05 16:00:26 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 16:00:26 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:00:33 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7408.9ms
2026-02-05 16:00:37 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 16:00:38 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 16:00:38 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 109.4ms
2026-02-05 16:00:38 [INFO] fastapi_app.service.query_service: What inter-annotator agreement did they obtain?
2026-02-05 16:00:38 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 16:00:38 [INFO] fastapi_app.service.query_service: Query text collection: pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4, top_k=3
2026-02-05 16:00:38 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:00:48 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 16:00:48 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 10401.7ms
2026-02-05 16:00:48 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 16:00:48 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:00:56 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 8459.0ms
2026-02-05 16:00:56 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 16:00:57 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 16:00:57 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 496.5ms
2026-02-05 16:00:57 [INFO] fastapi_app.service.query_service: what is the size of the dataset?
2026-02-05 16:00:57 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 16:00:57 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_16, top_k=3
2026-02-05 16:00:57 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:01:02 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 16:01:02 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 4910.5ms
2026-02-05 16:01:02 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 16:01:02 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:01:09 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 6767.0ms
2026-02-05 16:01:09 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 16:01:09 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 16:01:09 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 84.6ms
2026-02-05 16:01:09 [INFO] fastapi_app.service.query_service: What 20 domains are available for selection of source domain?
2026-02-05 16:01:09 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 16:01:09 [INFO] fastapi_app.service.query_service: Query text collection: pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1, top_k=3
2026-02-05 16:01:09 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:01:17 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 16:01:17 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 7965.6ms
2026-02-05 16:01:17 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 16:01:17 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:01:24 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7504.2ms
2026-02-05 16:01:24 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 16:01:24 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 16:01:24 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 160.9ms
2026-02-05 16:01:24 [INFO] fastapi_app.service.query_service: How much improvement is given on RACE by their introduced approach?
2026-02-05 16:01:24 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 16:01:24 [INFO] fastapi_app.service.query_service: Query text collection: pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9, top_k=3
2026-02-05 16:01:24 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:01:31 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 16:01:31 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 6869.6ms
2026-02-05 16:01:31 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 16:01:31 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:01:37 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 5586.3ms
2026-02-05 16:01:37 [INFO] fastapi_app.service.query_service: Query text-only collection: global_text_chunks, top_k=3
2026-02-05 16:01:37 [INFO] httpx: HTTP Request: POST http://127.0.0.1:8001/v1/chat/completions "HTTP/1.1 200 OK"
2026-02-05 16:01:37 [INFO] fastapi_app.app: POST /api/v1/query/text_only 200 128.6ms
2026-02-05 16:01:37 [INFO] fastapi_app.service.query_service: What directions are suggested to improve language models?
2026-02-05 16:01:37 [INFO] fastapi_app.service.query_service: Query image collection: vision_pages, top_k=3
2026-02-05 16:01:37 [INFO] fastapi_app.service.query_service: Query text collection: pdf_cl-bench-_a_benchmark_for_context_learnin_18, top_k=3
2026-02-05 16:01:37 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:01:42 [INFO] fastapi_app.service.query_service: Query completed
2026-02-05 16:01:42 [INFO] fastapi_app.app: POST /api/v1/query/vision 200 5088.0ms
2026-02-05 16:01:42 [INFO] fastapi_app.service.query_service: Query image collection (vision-only): vision_pages, top_k=3
2026-02-05 16:01:42 [INFO] fastapi_app.service.query_service: Running answer_generator on retrieved page (backend=openai_compat)
2026-02-05 16:01:50 [INFO] fastapi_app.app: POST /api/v1/query/vision_only 200 7522.9ms
2026-02-05 16:18:14 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2026-02-05 16:18:15 [INFO] fastapi_app.dao.chroma_dao: ChromaDAO initialized: /home/xwh/VisRAG/output/chroma_db
2026-02-05 16:18:15 [INFO] fastapi_app.service.index_service: Indexing PDF: a_corpus_of_adpositional_supersenses_for_mandarin_chinese -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/7b6ebf210baf4e999c0d65486f7a9313_A Corpus of Adpositional Supersenses for Mandarin Chinese.pdf
2026-02-05 16:18:16 [INFO] fastapi_app.service.index_service: PDF pages: 9; images: 9
2026-02-05 16:18:38 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 24279.7ms
2026-02-05 16:18:38 [INFO] fastapi_app.service.index_service: Indexing PDF: a_study_on_neural_network_language_modeling -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/b9c2581eea1b436da855edb6c2aedf12_A Study on Neural Network Language Modeling.pdf
2026-02-05 16:18:40 [INFO] fastapi_app.service.index_service: PDF pages: 20; images: 20
2026-02-05 16:19:13 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 34463.8ms
2026-02-05 16:19:13 [INFO] fastapi_app.service.index_service: Indexing PDF: cl-bench-_a_benchmark_for_context_learnin -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/d7af43071b424358bf65339ccd78d7b9_CL-BENCH- A BENCHMARK FOR CONTEXT LEARNIN.pdf
2026-02-05 16:19:19 [INFO] fastapi_app.service.index_service: PDF pages: 78; images: 78
2026-02-05 16:21:43 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 149877.7ms
2026-02-05 16:21:43 [INFO] fastapi_app.service.index_service: Indexing PDF: dynamic_fusion_networks_for_machine_reading_comprehension -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/507ebe4719ed4745a0fec92921506e5a_Dynamic Fusion Networks for Machine Reading Comprehension.pdf
2026-02-05 16:21:44 [INFO] fastapi_app.service.index_service: PDF pages: 13; images: 13
2026-02-05 16:22:09 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 25926.3ms
2026-02-05 16:22:09 [INFO] fastapi_app.service.index_service: Indexing PDF: ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/3c55ef3dd2184f6fbf0649fc50e04e8b_LadaBERT- Lightweight Adaptation of BERT through Hybrid Model Compression.pdf
2026-02-05 16:22:10 [INFO] fastapi_app.service.index_service: PDF pages: 10; images: 10
2026-02-05 16:22:29 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 20102.5ms
2026-02-05 16:22:29 [INFO] fastapi_app.service.index_service: Indexing PDF: political_speech_generation -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/5e806bba866f4ea4ae39970464c09a57_Political Speech Generation.pdf
2026-02-05 16:22:30 [INFO] fastapi_app.service.index_service: PDF pages: 15; images: 15
2026-02-05 16:22:58 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 28824.3ms
2026-02-05 16:22:58 [INFO] fastapi_app.service.index_service: Indexing PDF: recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study -> /home/xwh/VisRAG/output/api_assets/uploads/pdfs/492b36a801844ba8823e8dd73a0c2da3_Recommendation Chart of Domains for Cross-Domain Sentiment Analysis-Findings of A 20 Domain Study.pdf
2026-02-05 16:22:59 [INFO] fastapi_app.service.index_service: PDF pages: 9; images: 9
2026-02-05 16:23:19 [INFO] fastapi_app.app: POST /api/v1/index/pdf 200 21686.9ms
