{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_0", "doc": "DeepSeek-OCR- Contexts Optical Compression.pdf", "page": 1, "question": "What are the two components of DeepSeek-OCR?", "answer": "DeepSeek-OCR consists of two components: DeepEncoder\nand DeepSeek3B-MoE-A570M as the decoder.", "context": "We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long\ncontexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder\nand DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core\nengine, designed to maintain low activations under high-resolution input while achieving high\ncompression ratios to ensure an optimal and manageable number of vision tokens. Experiments\nshow that when the number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10×), the model can achieve decoding (OCR) precision of 97%. Even at a\ncompression ratio of 20×, the OCR accuracy still remains at about 60%. This shows considerable\npromise for research areas such as historical long-context compression and memory forgetting\nmechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value.\nOn OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens,\nand outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than\n800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs\nat a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly\naccessible at http://github.com/deepseek-ai/DeepSeek-OCR.\n600­700\n700­800\n800­900\n900­1000\n1000­1100\n1100­1200\n1200­1300\nText Tokens in Per Page (Ground­truth)\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nPrecision (%)\n96.5%\n93.8%\n83.8%\n85.8%\n79.3%\n76.3%\n59.1%\n98.5%\n97.3%\n96.8%\n96.8%\n91.5%\n89.8%\n87.1%\n64 vis toks(left)\n100 vis toks(left)\n64 vis toks(right)\n100 vis toks(right)\n0x\n5x\n10x\n15x\n20x\nCompression (×)\n10.5\n6.7\n11.8\n7.5\n13.2\n8.5\n15.1\n9.7\n16.5\n10.6\n17.7\n11.3\n19.7\n12.6\n(a) Compression on Fox benchmark\n7000\n6000\n5000\n4000\n3000\n2000\n1500\n0.1\n0.2\n0.3\n0.4\n0.5\nOverall Performance (Edit Distance)\nInternVL2-76B\nQwen2.5-VL-7B\nOLMOCR\nOCRFlux-3B\nInternVL3-78B\nQwen2.5-VL-72B\nMinerU2.0\nDeepSeek-OCR \n(Gundam-M 200dpi)\ndots.ocr\ndots.ocr (200dpi)\nVison Tokens > 1500 \n Average per image (\n More)\nEncoder Series\nDeepEncoder Series\nQwenEncoder Series\nInternVLEncoder Series\nOther Encoders\n1000\n800\n600\n500\n400\n300\n250\n200\n150\n100\nSmolDocli"}
{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_1", "doc": "DeepSeek-OCR- Contexts Optical Compression.pdf", "page": 1, "question": "What OCR precision is achieved when the compression ratio is < 10×?", "answer": "Experiments\nshow that when the number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10×), the model can achieve decoding (OCR) precision of 97%.", "context": "We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long\ncontexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder\nand DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core\nengine, designed to maintain low activations under high-resolution input while achieving high\ncompression ratios to ensure an optimal and manageable number of vision tokens. Experiments\nshow that when the number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10×), the model can achieve decoding (OCR) precision of 97%. Even at a\ncompression ratio of 20×, the OCR accuracy still remains at about 60%. This shows considerable\npromise for research areas such as historical long-context compression and memory forgetting\nmechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value.\nOn OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens,\nand outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than\n800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs\nat a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly\naccessible at http://github.com/deepseek-ai/DeepSeek-OCR.\n600­700\n700­800\n800­900\n900­1000\n1000­1100\n1100­1200\n1200­1300\nText Tokens in Per Page (Ground­truth)\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nPrecision (%)\n96.5%\n93.8%\n83.8%\n85.8%\n79.3%\n76.3%\n59.1%\n98.5%\n97.3%\n96.8%\n96.8%\n91.5%\n89.8%\n87.1%\n64 vis toks(left)\n100 vis toks(left)\n64 vis toks(right)\n100 vis toks(right)\n0x\n5x\n10x\n15x\n20x\nCompression (×)\n10.5\n6.7\n11.8\n7.5\n13.2\n8.5\n15.1\n9.7\n16.5\n10.6\n17.7\n11.3\n19.7\n12.6\n(a) Compression on Fox benchmark\n7000\n6000\n5000\n4000\n3000\n2000\n1500\n0.1\n0.2\n0.3\n0.4\n0.5\nOverall Performance (Edit Distance)\nInternVL2-76B\nQwen2.5-VL-7B\nOLMOCR\nOCRFlux-3B\nInternVL3-78B\nQwen2.5-VL-72B\nMinerU2.0\nDeepSeek-OCR \n(Gundam-M 200dpi)\ndots.ocr\ndots.ocr (200dpi)\nVison Tokens > 1500 \n Average per image (\n More)\nEncoder Series\nDeepEncoder Series\nQwenEncoder Series\nInternVLEncoder Series\nOther Encoders\n1000\n800\n600\n500\n400\n300\n250\n200\n150\n100\nSmolDocli"}
{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_2", "doc": "DeepSeek-OCR- Contexts Optical Compression.pdf", "page": 1, "question": "What OCR accuracy is reported at a compression ratio of 20×?", "answer": "Even at a\ncompression ratio of 20×, the OCR accuracy still remains at about 60%.", "context": "We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long\ncontexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder\nand DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core\nengine, designed to maintain low activations under high-resolution input while achieving high\ncompression ratios to ensure an optimal and manageable number of vision tokens. Experiments\nshow that when the number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10×), the model can achieve decoding (OCR) precision of 97%. Even at a\ncompression ratio of 20×, the OCR accuracy still remains at about 60%. This shows considerable\npromise for research areas such as historical long-context compression and memory forgetting\nmechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value.\nOn OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens,\nand outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than\n800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs\nat a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly\naccessible at http://github.com/deepseek-ai/DeepSeek-OCR.\n600­700\n700­800\n800­900\n900­1000\n1000­1100\n1100­1200\n1200­1300\nText Tokens in Per Page (Ground­truth)\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nPrecision (%)\n96.5%\n93.8%\n83.8%\n85.8%\n79.3%\n76.3%\n59.1%\n98.5%\n97.3%\n96.8%\n96.8%\n91.5%\n89.8%\n87.1%\n64 vis toks(left)\n100 vis toks(left)\n64 vis toks(right)\n100 vis toks(right)\n0x\n5x\n10x\n15x\n20x\nCompression (×)\n10.5\n6.7\n11.8\n7.5\n13.2\n8.5\n15.1\n9.7\n16.5\n10.6\n17.7\n11.3\n19.7\n12.6\n(a) Compression on Fox benchmark\n7000\n6000\n5000\n4000\n3000\n2000\n1500\n0.1\n0.2\n0.3\n0.4\n0.5\nOverall Performance (Edit Distance)\nInternVL2-76B\nQwen2.5-VL-7B\nOLMOCR\nOCRFlux-3B\nInternVL3-78B\nQwen2.5-VL-72B\nMinerU2.0\nDeepSeek-OCR \n(Gundam-M 200dpi)\ndots.ocr\ndots.ocr (200dpi)\nVison Tokens > 1500 \n Average per image (\n More)\nEncoder Series\nDeepEncoder Series\nQwenEncoder Series\nInternVLEncoder Series\nOther Encoders\n1000\n800\n600\n500\n400\n300\n250\n200\n150\n100\nSmolDocli"}
{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_3", "doc": "DeepSeek-OCR- Contexts Optical Compression.pdf", "page": 1, "question": "According to Figure 1, what does it describe?", "answer": "Figure 1 | Figure (a) shows the compression ratio (number of text tokens in ground truth/number of vision tokens model used) testing on Fox [21] benchmark; Figure (b) shows performance comparisons on OmniDocBench [27]. DeepSeek-OCR can achieve state-of-the-art performance among end-to-end models enjoying the fewest vision tokens.", "context": "Figure 1 | Figure (a) shows the compression ratio (number of text tokens in ground truth/number of vision tokens model used) testing on Fox [21] benchmark; Figure (b) shows performance comparisons on OmniDocBench [27]. DeepSeek-OCR can achieve state-of-the-art performance among end-to-end models enjoying the fewest vision tokens."}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_0", "doc": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf", "page": 1, "question": "What is RAGen proposed for in this paper?", "answer": "Here, we\npropose RAGen, a scalable and modular frame-\nwork for generating domain-grounded ques-\ntion–answer–context (QAC) triples tailored\nto diverse RAG adaptation approaches.", "context": "Retrieval-Augmented Generation (RAG) com-\nbines the language understanding and reason-\ning power of large language models (LLMs)\nwith external retrieval to enable domain-\ngrounded responses. Effectively adapting RAG\nsystems to domain-specific settings requires\nspecialized, context-rich training data beyond\ngeneral-purpose question-answering. Here, we\npropose RAGen, a scalable and modular frame-\nwork for generating domain-grounded ques-\ntion–answer–context (QAC) triples tailored\nto diverse RAG adaptation approaches. RA-\nGen produces these QAC triples by identify-\ning key concepts in documents, generating di-\nverse questions guided by Bloom’s Taxonomy-\ninspired principles, and pairing them with\nprecise answers extracted from relevant con-\ntexts. RAGen supports multiple RAG adap-\ntation strategies, including the optimization of\nkey components such as the LLM, retriever, and\nembedding model, etc. Its modular pipeline fea-\ntures semantic chunking, hierarchical concept\nextraction, and multi-chunk retrieval, along\nwith the introduction of curated distractor con-\ntexts to promote robust reasoning. Designed for\nscalability, RAGen efficiently handles large and\nevolving document corpora without redundant\nprocessing, making it especially suitable for\ndynamic evolving domains such as scientific\nresearch and enterprise knowledge bases."}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_1", "doc": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf", "page": 1, "question": "How does RAGen produce domain-grounded QAC triples?", "answer": "Gen produces these QAC triples by identify-\ning key concepts in documents, generating di-\nverse questions guided by Bloom’s Taxonomy-\ninspired principles, and pairing them with\nprecise answers extracted from relevant con-\ntexts.", "context": "Retrieval-Augmented Generation (RAG) com-\nbines the language understanding and reason-\ning power of large language models (LLMs)\nwith external retrieval to enable domain-\ngrounded responses. Effectively adapting RAG\nsystems to domain-specific settings requires\nspecialized, context-rich training data beyond\ngeneral-purpose question-answering. Here, we\npropose RAGen, a scalable and modular frame-\nwork for generating domain-grounded ques-\ntion–answer–context (QAC) triples tailored\nto diverse RAG adaptation approaches. RA-\nGen produces these QAC triples by identify-\ning key concepts in documents, generating di-\nverse questions guided by Bloom’s Taxonomy-\ninspired principles, and pairing them with\nprecise answers extracted from relevant con-\ntexts. RAGen supports multiple RAG adap-\ntation strategies, including the optimization of\nkey components such as the LLM, retriever, and\nembedding model, etc. Its modular pipeline fea-\ntures semantic chunking, hierarchical concept\nextraction, and multi-chunk retrieval, along\nwith the introduction of curated distractor con-\ntexts to promote robust reasoning. Designed for\nscalability, RAGen efficiently handles large and\nevolving document corpora without redundant\nprocessing, making it especially suitable for\ndynamic evolving domains such as scientific\nresearch and enterprise knowledge bases."}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_2", "doc": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf", "page": 1, "question": "Which key components can RAGen optimize for RAG adaptation?", "answer": "including the optimization of\nkey components such as the LLM, retriever, and\nembedding model, etc.", "context": "Retrieval-Augmented Generation (RAG) com-\nbines the language understanding and reason-\ning power of large language models (LLMs)\nwith external retrieval to enable domain-\ngrounded responses. Effectively adapting RAG\nsystems to domain-specific settings requires\nspecialized, context-rich training data beyond\ngeneral-purpose question-answering. Here, we\npropose RAGen, a scalable and modular frame-\nwork for generating domain-grounded ques-\ntion–answer–context (QAC) triples tailored\nto diverse RAG adaptation approaches. RA-\nGen produces these QAC triples by identify-\ning key concepts in documents, generating di-\nverse questions guided by Bloom’s Taxonomy-\ninspired principles, and pairing them with\nprecise answers extracted from relevant con-\ntexts. RAGen supports multiple RAG adap-\ntation strategies, including the optimization of\nkey components such as the LLM, retriever, and\nembedding model, etc. Its modular pipeline fea-\ntures semantic chunking, hierarchical concept\nextraction, and multi-chunk retrieval, along\nwith the introduction of curated distractor con-\ntexts to promote robust reasoning. Designed for\nscalability, RAGen efficiently handles large and\nevolving document corpora without redundant\nprocessing, making it especially suitable for\ndynamic evolving domains such as scientific\nresearch and enterprise knowledge bases."}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p3_3", "doc": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf", "page": 3, "question": "According to Figure 1, what does it describe?", "answer": "Figure 1: Overview of RAGen framework, a three-stage process that first extract document concepts and then construct question stems, and finally create Question-Answer-Context datasets. resentation learning, while generation modules typ- ically leverage pre-trained encoder–decoder mod-", "context": "Figure 1: Overview of RAGen framework, a three-stage process that first extract document concepts and then construct question stems, and finally create Question-Answer-Context datasets. resentation learning, while generation modules typ- ically leverage pre-trained encoder–decoder mod-"}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_0", "doc": "Long-Context Language Modeling with Parallel Context Encoding.pdf", "page": 1, "question": "What is CEPE introduced for?", "answer": "We introduce\nContext Expansion with Parallel Encoding\n(CEPE\n), a framework that can be applied\nto any existing decoder-only LLMs to extend\ntheir context window.", "context": "Extending large language models (LLMs) to\nprocess longer inputs is crucial for a wide range\nof applications. However, the substantial com-\nputational cost of transformers and limited gen-\neralization of positional encoding restrict the\nsize of their context window. We introduce\nContext Expansion with Parallel Encoding\n(CEPE\n), a framework that can be applied\nto any existing decoder-only LLMs to extend\ntheir context window. CEPE employs a small\nencoder to process long inputs chunk by chunk,\nenabling the frozen decoder to utilize addi-\ntional contexts via cross-attention. CEPE is\nefficient, generalizable, and versatile: trained\nwith 8K-token documents, it extends the con-\ntext window of LLAMA-2 to 128K tokens,\noffering 10× the throughput with only 1/6 of\nthe memory. CEPE yields strong performance\non language modeling and in-context learning.\nCEPE also excels in retrieval-augmented appli-\ncations, while existing long-context models de-\ngenerate with retrieved contexts. We further in-\ntroduce a CEPE variant that can extend the con-\ntext window of instruction-tuned models using\nonly unlabeled data, and showcase its effective-\nness on LLAMA-2-CHAT, leading to a strong\ninstruction-following model that can leverage\nvery long contexts on downstream tasks.1"}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_1", "doc": "Long-Context Language Modeling with Parallel Context Encoding.pdf", "page": 1, "question": "To how many tokens does CEPE extend LLAMA-2's context window?", "answer": "it extends the con-\ntext window of LLAMA-2 to 128K tokens,\noffering 10× the throughput with only 1/6 of\nthe memory.", "context": "Extending large language models (LLMs) to\nprocess longer inputs is crucial for a wide range\nof applications. However, the substantial com-\nputational cost of transformers and limited gen-\neralization of positional encoding restrict the\nsize of their context window. We introduce\nContext Expansion with Parallel Encoding\n(CEPE\n), a framework that can be applied\nto any existing decoder-only LLMs to extend\ntheir context window. CEPE employs a small\nencoder to process long inputs chunk by chunk,\nenabling the frozen decoder to utilize addi-\ntional contexts via cross-attention. CEPE is\nefficient, generalizable, and versatile: trained\nwith 8K-token documents, it extends the con-\ntext window of LLAMA-2 to 128K tokens,\noffering 10× the throughput with only 1/6 of\nthe memory. CEPE yields strong performance\non language modeling and in-context learning.\nCEPE also excels in retrieval-augmented appli-\ncations, while existing long-context models de-\ngenerate with retrieved contexts. We further in-\ntroduce a CEPE variant that can extend the con-\ntext window of instruction-tuned models using\nonly unlabeled data, and showcase its effective-\nness on LLAMA-2-CHAT, leading to a strong\ninstruction-following model that can leverage\nvery long contexts on downstream tasks.1"}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_2", "doc": "Long-Context Language Modeling with Parallel Context Encoding.pdf", "page": 1, "question": "What throughput and memory improvements are claimed for CEPE?", "answer": "offering 10× the throughput with only 1/6 of\nthe memory.", "context": "Extending large language models (LLMs) to\nprocess longer inputs is crucial for a wide range\nof applications. However, the substantial com-\nputational cost of transformers and limited gen-\neralization of positional encoding restrict the\nsize of their context window. We introduce\nContext Expansion with Parallel Encoding\n(CEPE\n), a framework that can be applied\nto any existing decoder-only LLMs to extend\ntheir context window. CEPE employs a small\nencoder to process long inputs chunk by chunk,\nenabling the frozen decoder to utilize addi-\ntional contexts via cross-attention. CEPE is\nefficient, generalizable, and versatile: trained\nwith 8K-token documents, it extends the con-\ntext window of LLAMA-2 to 128K tokens,\noffering 10× the throughput with only 1/6 of\nthe memory. CEPE yields strong performance\non language modeling and in-context learning.\nCEPE also excels in retrieval-augmented appli-\ncations, while existing long-context models de-\ngenerate with retrieved contexts. We further in-\ntroduce a CEPE variant that can extend the con-\ntext window of instruction-tuned models using\nonly unlabeled data, and showcase its effective-\nness on LLAMA-2-CHAT, leading to a strong\ninstruction-following model that can leverage\nvery long contexts on downstream tasks.1"}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_3", "doc": "Long-Context Language Modeling with Parallel Context Encoding.pdf", "page": 1, "question": "According to Figure 1, what does it describe?", "answer": "Figure 1: A comparison between CEPE and other tech- niques of extending LLMs’ context window, including YARN (Peng et al., 2024), STREAMINGLLM (Xiao et al., 2024b), and REPLUG (Shi et al., 2024). CEPE", "context": "Figure 1: A comparison between CEPE and other tech- niques of extending LLMs’ context window, including YARN (Peng et al., 2024), STREAMINGLLM (Xiao et al., 2024b), and REPLUG (Shi et al., 2024). CEPE"}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p1_0", "doc": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf", "page": 1, "question": "What does the survey introduce as a unifying framework?", "answer": "This survey introduces neural network reprogramma-\nbility as a unifying framework that bridges mainstream model adaptation techniques–model\nreprogramming, prompt tuning, and prompt instruction–previously fragmented research areas\nyet converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen.", "context": "As large-scale pre-trained foundation models continue to expand in size and capability, efficiently\nadapting them to specific downstream tasks has become increasingly critical. Despite substantial\nresearch progress, existing adaptation approaches have evolved largely in isolation, without a clear\nunderstanding of their interrelationships. This survey introduces neural network reprogramma-\nbility as a unifying framework that bridges mainstream model adaptation techniques–model\nreprogramming, prompt tuning, and prompt instruction–previously fragmented research areas\nyet converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen. These methods exploit\nneural networks’ sensitivity to information manipulation on different interfaces, be it through\nperturbing inputs, inserting tokens into intermediate layers, or providing task-specific examples\nin context, to redirect model behaviors towards desired outcomes. Building on the concept of\nreprogrammability, we present a taxonomy that categorizes such information manipulation-based\nadaptation approaches across four key dimensions: manipulation format (fixed or learnable),\nlocation (interfaces where manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream tasks). Notably,\nthis framework applies consistently across data modalities, independent of specific model architec-\ntures. Moreover, viewing established techniques such as in-context learning and chain-of-thought\nprompting through this lens reveals both their theoretical connections and practical distinctions.\nWe further analyze remaining technical challenges and ethical considerations, positioning neural\nnetwork reprogrammability as a fundamental paradigm for efficient model adaptation. We lastly\nidentify promising research directions emerging from this integrative viewpoint.\n*Equal contributions.\nCorrespondence: fengliu.ml@gmail.com"}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p1_1", "doc": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf", "page": 1, "question": "What shared principle do these methods converge on?", "answer": "converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen.", "context": "As large-scale pre-trained foundation models continue to expand in size and capability, efficiently\nadapting them to specific downstream tasks has become increasingly critical. Despite substantial\nresearch progress, existing adaptation approaches have evolved largely in isolation, without a clear\nunderstanding of their interrelationships. This survey introduces neural network reprogramma-\nbility as a unifying framework that bridges mainstream model adaptation techniques–model\nreprogramming, prompt tuning, and prompt instruction–previously fragmented research areas\nyet converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen. These methods exploit\nneural networks’ sensitivity to information manipulation on different interfaces, be it through\nperturbing inputs, inserting tokens into intermediate layers, or providing task-specific examples\nin context, to redirect model behaviors towards desired outcomes. Building on the concept of\nreprogrammability, we present a taxonomy that categorizes such information manipulation-based\nadaptation approaches across four key dimensions: manipulation format (fixed or learnable),\nlocation (interfaces where manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream tasks). Notably,\nthis framework applies consistently across data modalities, independent of specific model architec-\ntures. Moreover, viewing established techniques such as in-context learning and chain-of-thought\nprompting through this lens reveals both their theoretical connections and practical distinctions.\nWe further analyze remaining technical challenges and ethical considerations, positioning neural\nnetwork reprogrammability as a fundamental paradigm for efficient model adaptation. We lastly\nidentify promising research directions emerging from this integrative viewpoint.\n*Equal contributions.\nCorrespondence: fengliu.ml@gmail.com"}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p1_2", "doc": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf", "page": 1, "question": "What are the four key dimensions in the taxonomy?", "answer": "across four key dimensions: manipulation format (fixed or learnable),\nlocation (interfaces where manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream tasks).", "context": "As large-scale pre-trained foundation models continue to expand in size and capability, efficiently\nadapting them to specific downstream tasks has become increasingly critical. Despite substantial\nresearch progress, existing adaptation approaches have evolved largely in isolation, without a clear\nunderstanding of their interrelationships. This survey introduces neural network reprogramma-\nbility as a unifying framework that bridges mainstream model adaptation techniques–model\nreprogramming, prompt tuning, and prompt instruction–previously fragmented research areas\nyet converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen. These methods exploit\nneural networks’ sensitivity to information manipulation on different interfaces, be it through\nperturbing inputs, inserting tokens into intermediate layers, or providing task-specific examples\nin context, to redirect model behaviors towards desired outcomes. Building on the concept of\nreprogrammability, we present a taxonomy that categorizes such information manipulation-based\nadaptation approaches across four key dimensions: manipulation format (fixed or learnable),\nlocation (interfaces where manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream tasks). Notably,\nthis framework applies consistently across data modalities, independent of specific model architec-\ntures. Moreover, viewing established techniques such as in-context learning and chain-of-thought\nprompting through this lens reveals both their theoretical connections and practical distinctions.\nWe further analyze remaining technical challenges and ethical considerations, positioning neural\nnetwork reprogrammability as a fundamental paradigm for efficient model adaptation. We lastly\nidentify promising research directions emerging from this integrative viewpoint.\n*Equal contributions.\nCorrespondence: fengliu.ml@gmail.com"}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p2_3", "doc": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf", "page": 2, "question": "According to Figure 1, what does it describe?", "answer": "Figure 1 | Paradigm shift from conventional parameter-centric adaptation (i.e., modifying model parameters) to reprogrammability-centric adaptation (i.e., modifying input data and model output). This represents a shift in thought from modifying the model to align with the task to modifying the task to align with the model. fine-tuning, where a substantial portion, if not all, of the model’s parameters are updated using task-specific", "context": "Figure 1 | Paradigm shift from conventional parameter-centric adaptation (i.e., modifying model parameters) to reprogrammability-centric adaptation (i.e., modifying input data and model output). This represents a shift in thought from modifying the model to align with the task to modifying the task to align with the model. fine-tuning, where a substantial portion, if not all, of the model’s parameters are updated using task-specific"}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_0", "doc": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf", "page": 1, "question": "What challenge does RAG face in fragmented data scenarios?", "answer": "it encounters significant challenges in practi-\ncal scenarios where data is inherently discrete\nand fragmented.", "context": "Retrieval-Augmented Generation (RAG) has\nproven effective for knowledge synthesis, yet\nit encounters significant challenges in practi-\ncal scenarios where data is inherently discrete\nand fragmented.\nIn most environments, in-\nformation is distributed across isolated files\nlike reports and logs that lack explicit links.\nStandard search engines process files indepen-\ndently, ignoring the connections between them.\nFurthermore, manually building Knowledge\nGraphs is impractical for such vast data. To\nbridge this gap, we present Orion-RAG. Our\ncore insight is simple yet effective: we do\nnot need heavy algorithms to organize this\ndata. Instead, we use a low-complexity strat-\negy to extract lightweight “paths” that natu-\nrally link related concepts. We demonstrate that\nthis streamlined approach suffices to transform\nfragmented documents into semi-structured\ndata, enabling the system to link information\nacross different files effectively. Extensive ex-\nperiments demonstrate that Orion-RAG con-\nsistently outperforms mainstream frameworks\nacross diverse domains, supporting real-time\nupdates and explicit Human-in-the-Loop verifi-\ncation with high cost-efficiency. Experiments\non FinanceBench demonstrate superior preci-\nsion with a 25.2% relative improvement over\nstrong baselines."}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_1", "doc": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf", "page": 1, "question": "What is the core insight of Orion-RAG?", "answer": "Our\ncore insight is simple yet effective: we do\nnot need heavy algorithms to organize this\ndata.", "context": "Retrieval-Augmented Generation (RAG) has\nproven effective for knowledge synthesis, yet\nit encounters significant challenges in practi-\ncal scenarios where data is inherently discrete\nand fragmented.\nIn most environments, in-\nformation is distributed across isolated files\nlike reports and logs that lack explicit links.\nStandard search engines process files indepen-\ndently, ignoring the connections between them.\nFurthermore, manually building Knowledge\nGraphs is impractical for such vast data. To\nbridge this gap, we present Orion-RAG. Our\ncore insight is simple yet effective: we do\nnot need heavy algorithms to organize this\ndata. Instead, we use a low-complexity strat-\negy to extract lightweight “paths” that natu-\nrally link related concepts. We demonstrate that\nthis streamlined approach suffices to transform\nfragmented documents into semi-structured\ndata, enabling the system to link information\nacross different files effectively. Extensive ex-\nperiments demonstrate that Orion-RAG con-\nsistently outperforms mainstream frameworks\nacross diverse domains, supporting real-time\nupdates and explicit Human-in-the-Loop verifi-\ncation with high cost-efficiency. Experiments\non FinanceBench demonstrate superior preci-\nsion with a 25.2% relative improvement over\nstrong baselines."}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_2", "doc": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf", "page": 1, "question": "What relative improvement is reported on FinanceBench?", "answer": "25.2% relative improvement over\nstrong baselines.", "context": "Retrieval-Augmented Generation (RAG) has\nproven effective for knowledge synthesis, yet\nit encounters significant challenges in practi-\ncal scenarios where data is inherently discrete\nand fragmented.\nIn most environments, in-\nformation is distributed across isolated files\nlike reports and logs that lack explicit links.\nStandard search engines process files indepen-\ndently, ignoring the connections between them.\nFurthermore, manually building Knowledge\nGraphs is impractical for such vast data. To\nbridge this gap, we present Orion-RAG. Our\ncore insight is simple yet effective: we do\nnot need heavy algorithms to organize this\ndata. Instead, we use a low-complexity strat-\negy to extract lightweight “paths” that natu-\nrally link related concepts. We demonstrate that\nthis streamlined approach suffices to transform\nfragmented documents into semi-structured\ndata, enabling the system to link information\nacross different files effectively. Extensive ex-\nperiments demonstrate that Orion-RAG con-\nsistently outperforms mainstream frameworks\nacross diverse domains, supporting real-time\nupdates and explicit Human-in-the-Loop verifi-\ncation with high cost-efficiency. Experiments\non FinanceBench demonstrate superior preci-\nsion with a 25.2% relative improvement over\nstrong baselines."}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p2_3", "doc": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf", "page": 2, "question": "According to Figure 1, what does it describe?", "answer": "Figure 1: System overview of Orion-RAG. (1) The Path-Annotation Data Augmentation subsystem (left) employs dual-layer labeling agents to construct hierarchical navigation paths from fragmented text, enabling real-time incremental indexing. (2) The Multi-Layer Hybrid Retrieval subsystem (right) utilizes these paths as explicit logical signposts, integrating sparse and dense search to guide the generator towards accurate and interpretable answers.", "context": "Figure 1: System overview of Orion-RAG. (1) The Path-Annotation Data Augmentation subsystem (left) employs dual-layer labeling agents to construct hierarchical navigation paths from fragmented text, enabling real-time incremental indexing. (2) The Multi-Layer Hybrid Retrieval subsystem (right) utilizes these paths as explicit logical signposts, integrating sparse and dense search to guide the generator towards accurate and interpretable answers."}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_0", "doc": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf", "page": 1, "question": "What limitation of existing RAG toolkits does UltraRAG address?", "answer": "However,\nmany existing RAG toolkits lack support for\nknowledge adaptation tailored to specific ap-\nplication scenarios.", "context": "Retrieval-Augmented Generation (RAG) sig-\nnificantly enhances the performance of large\nlanguage models (LLMs) in downstream tasks\nby integrating external knowledge. To facilitate\nresearchers in deploying RAG systems, various\nRAG toolkits have been introduced. However,\nmany existing RAG toolkits lack support for\nknowledge adaptation tailored to specific ap-\nplication scenarios. To address this limitation,\nwe propose UltraRAG, a RAG toolkit that auto-\nmates knowledge adaptation throughout the en-\ntire workflow, from data construction and train-\ning to evaluation, while ensuring ease of use.\nUltraRAG features a user-friendly WebUI that\nstreamlines the RAG process, allowing users\nto build and optimize systems without coding\nexpertise. It supports multimodal input and\nprovides comprehensive tools for managing the\nknowledge base. With its highly modular archi-\ntecture, UltraRAG delivers an end-to-end de-\nvelopment solution, enabling seamless knowl-\nedge adaptation across diverse user scenarios.\nThe code, demonstration videos, and installable\npackage for UltraRAG are publicly available at\nhttps://github.com/OpenBMB/UltraRAG."}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_1", "doc": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf", "page": 1, "question": "What does UltraRAG automate throughout the entire workflow?", "answer": "we propose UltraRAG, a RAG toolkit that auto-\nmates knowledge adaptation throughout the en-\ntire workflow, from data construction and train-\ning to evaluation, while ensuring ease of use.", "context": "Retrieval-Augmented Generation (RAG) sig-\nnificantly enhances the performance of large\nlanguage models (LLMs) in downstream tasks\nby integrating external knowledge. To facilitate\nresearchers in deploying RAG systems, various\nRAG toolkits have been introduced. However,\nmany existing RAG toolkits lack support for\nknowledge adaptation tailored to specific ap-\nplication scenarios. To address this limitation,\nwe propose UltraRAG, a RAG toolkit that auto-\nmates knowledge adaptation throughout the en-\ntire workflow, from data construction and train-\ning to evaluation, while ensuring ease of use.\nUltraRAG features a user-friendly WebUI that\nstreamlines the RAG process, allowing users\nto build and optimize systems without coding\nexpertise. It supports multimodal input and\nprovides comprehensive tools for managing the\nknowledge base. With its highly modular archi-\ntecture, UltraRAG delivers an end-to-end de-\nvelopment solution, enabling seamless knowl-\nedge adaptation across diverse user scenarios.\nThe code, demonstration videos, and installable\npackage for UltraRAG are publicly available at\nhttps://github.com/OpenBMB/UltraRAG."}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_2", "doc": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf", "page": 1, "question": "What does UltraRAG's WebUI enable according to the abstract?", "answer": "UltraRAG features a user-friendly WebUI that\nstreamlines the RAG process, allowing users\nto build and optimize systems without coding\nexpertise.", "context": "Retrieval-Augmented Generation (RAG) sig-\nnificantly enhances the performance of large\nlanguage models (LLMs) in downstream tasks\nby integrating external knowledge. To facilitate\nresearchers in deploying RAG systems, various\nRAG toolkits have been introduced. However,\nmany existing RAG toolkits lack support for\nknowledge adaptation tailored to specific ap-\nplication scenarios. To address this limitation,\nwe propose UltraRAG, a RAG toolkit that auto-\nmates knowledge adaptation throughout the en-\ntire workflow, from data construction and train-\ning to evaluation, while ensuring ease of use.\nUltraRAG features a user-friendly WebUI that\nstreamlines the RAG process, allowing users\nto build and optimize systems without coding\nexpertise. It supports multimodal input and\nprovides comprehensive tools for managing the\nknowledge base. With its highly modular archi-\ntecture, UltraRAG delivers an end-to-end de-\nvelopment solution, enabling seamless knowl-\nedge adaptation across diverse user scenarios.\nThe code, demonstration videos, and installable\npackage for UltraRAG are publicly available at\nhttps://github.com/OpenBMB/UltraRAG."}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p2_3", "doc": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf", "page": 2, "question": "According to Table 1, what does it describe?", "answer": "Table 1: Comparison of UltraRAG Features with Other RAG Frameworks. User-Friendly WebUI. UltraRAG provides an intuitive WebUI that allows users to easily deploy RAG systems and efficiently process knowledge", "context": "Table 1: Comparison of UltraRAG Features with Other RAG Frameworks. User-Friendly WebUI. UltraRAG provides an intuitive WebUI that allows users to easily deploy RAG systems and efficiently process knowledge"}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p1_0", "doc": "Vision-centric Token Compression in Large Language Model.pdf", "page": 1, "question": "Why is token compression considered indispensable in this paper?", "answer": "This dual expansion send compute and memory costs skyrocket-\ning, making token compression indispensable.", "context": "Real-world applications are stretching context windows to hundreds of thousand\nof tokens while Large Language Models (LLMs) swell from billions to trillions\nof parameters. This dual expansion send compute and memory costs skyrocket-\ning, making token compression indispensable. We introduce VISION CENTRIC\nTOKEN COMPRESSION (VIST), a slow–fast compression framework that mir-\nrors human reading: the fast path renders distant tokens into images, letting\na frozen, lightweight vision encoder skim the low-salience context; the slow\npath feeds the proximal window into the LLM for fine-grained reasoning. A\nProbability-informed Visual Enhancement (PVE) objective masks high-frequency\ntokens during training, steering the Resampler to concentrate on semantically rich\nregions—just as skilled reader gloss over function words. On eleven in-context\nlearning benchmarks, VIST achieves the same accuracy with 2.3× fewer tokens,\ncutting FLOPs by 16% and memory by 50%. This method delivers remarkable\nresults, outperforming the strongest text encoder-based compression method CEPE\nby 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and\nCLIN, setting a new standard for token efficiency in LLMs. The project is at\nhttps://github.com/CSU-JPG/VIST."}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p1_1", "doc": "Vision-centric Token Compression in Large Language Model.pdf", "page": 1, "question": "What is VIST's slow–fast compression framework?", "answer": "We introduce VISION CENTRIC\nTOKEN COMPRESSION (VIST), a slow–fast compression framework that mir-\nrors human reading: the fast path renders distant tokens into images, letting\na frozen, lightweight vision encoder skim the low-salience context; the slow\npath feeds the proximal window into the LLM for fine-grained reasoning.", "context": "Real-world applications are stretching context windows to hundreds of thousand\nof tokens while Large Language Models (LLMs) swell from billions to trillions\nof parameters. This dual expansion send compute and memory costs skyrocket-\ning, making token compression indispensable. We introduce VISION CENTRIC\nTOKEN COMPRESSION (VIST), a slow–fast compression framework that mir-\nrors human reading: the fast path renders distant tokens into images, letting\na frozen, lightweight vision encoder skim the low-salience context; the slow\npath feeds the proximal window into the LLM for fine-grained reasoning. A\nProbability-informed Visual Enhancement (PVE) objective masks high-frequency\ntokens during training, steering the Resampler to concentrate on semantically rich\nregions—just as skilled reader gloss over function words. On eleven in-context\nlearning benchmarks, VIST achieves the same accuracy with 2.3× fewer tokens,\ncutting FLOPs by 16% and memory by 50%. This method delivers remarkable\nresults, outperforming the strongest text encoder-based compression method CEPE\nby 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and\nCLIN, setting a new standard for token efficiency in LLMs. The project is at\nhttps://github.com/CSU-JPG/VIST."}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p1_2", "doc": "Vision-centric Token Compression in Large Language Model.pdf", "page": 1, "question": "How many in-context learning benchmarks are used to evaluate VIST?", "answer": "On eleven in-context\nlearning benchmarks, VIST achieves the same accuracy with 2.", "context": "Real-world applications are stretching context windows to hundreds of thousand\nof tokens while Large Language Models (LLMs) swell from billions to trillions\nof parameters. This dual expansion send compute and memory costs skyrocket-\ning, making token compression indispensable. We introduce VISION CENTRIC\nTOKEN COMPRESSION (VIST), a slow–fast compression framework that mir-\nrors human reading: the fast path renders distant tokens into images, letting\na frozen, lightweight vision encoder skim the low-salience context; the slow\npath feeds the proximal window into the LLM for fine-grained reasoning. A\nProbability-informed Visual Enhancement (PVE) objective masks high-frequency\ntokens during training, steering the Resampler to concentrate on semantically rich\nregions—just as skilled reader gloss over function words. On eleven in-context\nlearning benchmarks, VIST achieves the same accuracy with 2.3× fewer tokens,\ncutting FLOPs by 16% and memory by 50%. This method delivers remarkable\nresults, outperforming the strongest text encoder-based compression method CEPE\nby 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and\nCLIN, setting a new standard for token efficiency in LLMs. The project is at\nhttps://github.com/CSU-JPG/VIST."}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p2_3", "doc": "Vision-centric Token Compression in Large Language Model.pdf", "page": 2, "question": "According to Figure 1, what does it describe?", "answer": "Figure 1: Our method VIST adopts a lightweight vision encoder to process loosely relevant long contexts, offering a more cost-efficient alternative to full LLM processing. However, the inherent redundancy in long text leads to redundant visual tokens. Motivated by Selective Reading Strategy where low-frequency (content) words receive longer fixations while high-frequency function words are often skipped, we design Probability-informed", "context": "Figure 1: Our method VIST adopts a lightweight vision encoder to process loosely relevant long contexts, offering a more cost-efficient alternative to full LLM processing. However, the inherent redundancy in long text leads to redundant visual tokens. Motivated by Selective Reading Strategy where low-frequency (content) words receive longer fixations while high-frequency function words are often skipped, we design Probability-informed"}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p1_0", "doc": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf", "page": 1, "question": "What is xRAG introduced for?", "answer": "This paper introduces xRAG, a novel context compression method designed specif-\nically for retrieval-augmented generation.", "context": "This paper introduces xRAG, a novel context compression method designed specif-\nically for retrieval-augmented generation. xRAG redefines the use of document\nembeddings in dense retrieval—traditionally limited to retrieval purposes—by\nintegrating them as features from the retrieval modality. Through a modality\nfusion approach, xRAG effectively merges these embeddings into the language\nmodel’s representation space, eliminating the need for their textual counterparts and\nachieving an extreme compression rate. In xRAG, the modality bridge is the only\ntrainable component, while the retriever and language model remain frozen. This\ndesign choice allows for the reuse of offline-constructed document embeddings and\npreserves the plug-and-play nature of retrieval augmentation. Experimental results\ndemonstrate that xRAG achieves an average improvement of over 10% across six\nknowledge-intensive tasks, compatible with various language model backbones,\nranging from a dense 7B model to an 8x7B Mixture of Experts configuration.\nxRAG not only significantly outperforms previous context compression methods\nbut also matches the performance of uncompressed models on several benchmarks,\nwhile reducing overall FLOPs by a factor of 3.53. This work pioneers new avenues\nin retrieval-augmented generation through multimodal fusion, potentially setting a\ngroundwork for future developments in efficient and scalable retrieval systems."}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p1_1", "doc": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf", "page": 1, "question": "In xRAG, what is the only trainable component?", "answer": "In xRAG, the modality bridge is the only\ntrainable component, while the retriever and language model remain frozen.", "context": "This paper introduces xRAG, a novel context compression method designed specif-\nically for retrieval-augmented generation. xRAG redefines the use of document\nembeddings in dense retrieval—traditionally limited to retrieval purposes—by\nintegrating them as features from the retrieval modality. Through a modality\nfusion approach, xRAG effectively merges these embeddings into the language\nmodel’s representation space, eliminating the need for their textual counterparts and\nachieving an extreme compression rate. In xRAG, the modality bridge is the only\ntrainable component, while the retriever and language model remain frozen. This\ndesign choice allows for the reuse of offline-constructed document embeddings and\npreserves the plug-and-play nature of retrieval augmentation. Experimental results\ndemonstrate that xRAG achieves an average improvement of over 10% across six\nknowledge-intensive tasks, compatible with various language model backbones,\nranging from a dense 7B model to an 8x7B Mixture of Experts configuration.\nxRAG not only significantly outperforms previous context compression methods\nbut also matches the performance of uncompressed models on several benchmarks,\nwhile reducing overall FLOPs by a factor of 3.53. This work pioneers new avenues\nin retrieval-augmented generation through multimodal fusion, potentially setting a\ngroundwork for future developments in efficient and scalable retrieval systems."}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p1_2", "doc": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf", "page": 1, "question": "What average improvement is reported and across how many tasks?", "answer": "average improvement of over 10% across six\nknowledge-intensive tasks, compatible with various language model backbones,\nranging from a dense 7B model to an 8x7B Mixture of Experts configuration.", "context": "This paper introduces xRAG, a novel context compression method designed specif-\nically for retrieval-augmented generation. xRAG redefines the use of document\nembeddings in dense retrieval—traditionally limited to retrieval purposes—by\nintegrating them as features from the retrieval modality. Through a modality\nfusion approach, xRAG effectively merges these embeddings into the language\nmodel’s representation space, eliminating the need for their textual counterparts and\nachieving an extreme compression rate. In xRAG, the modality bridge is the only\ntrainable component, while the retriever and language model remain frozen. This\ndesign choice allows for the reuse of offline-constructed document embeddings and\npreserves the plug-and-play nature of retrieval augmentation. Experimental results\ndemonstrate that xRAG achieves an average improvement of over 10% across six\nknowledge-intensive tasks, compatible with various language model backbones,\nranging from a dense 7B model to an 8x7B Mixture of Experts configuration.\nxRAG not only significantly outperforms previous context compression methods\nbut also matches the performance of uncompressed models on several benchmarks,\nwhile reducing overall FLOPs by a factor of 3.53. This work pioneers new avenues\nin retrieval-augmented generation through multimodal fusion, potentially setting a\ngroundwork for future developments in efficient and scalable retrieval systems."}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p2_3", "doc": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf", "page": 2, "question": "According to Figure 1, what does it describe?", "answer": "Figure 1: xRAG enables efficient retrieval augmentation by adding one document token [X]. compression is applied on the surface form. These approaches, however, either require significant memory for storing LLM activations (e.g., 1.05 MB per token as reported by [58]) or suffer from relatively low compression rates. More critically, these methods overlook a crucial characteristic", "context": "Figure 1: xRAG enables efficient retrieval augmentation by adding one document token [X]. compression is applied on the surface form. These approaches, however, either require significant memory for storing LLM activations (e.g., 1.05 MB per token as reported by [58]) or suffer from relatively low compression rates. More critically, these methods overlook a crucial characteristic"}
