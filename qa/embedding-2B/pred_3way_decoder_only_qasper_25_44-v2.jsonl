{"id": "enriching_bert_with_knowledge_graph_embeddings_for_document_classification_0", "question": "By how much do they outperform standard BERT?", "text_context": "[BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance p8]\nﬁguration where the BERTRAM-based embedding\nis simply added and both representations are sepa-\nrated using a single slash:\neSLASH = e1, . . . , ej, e/, v(w,C), ej+1, . . . , en .\nThe intuition behind this variant is that in BERT’s\npretraining corpus, a slash is often used to separate\ntwo variants of the same word (e.g., “useable / us-\nable”) or two closely related concepts (e.g., “com-\npany / organization”, “web-based / cloud”) and\nthus, BERT should be able to understand that both\nei, . . . , ej and v(w,C) refer to the same entity. We\ntherefore surmise that whenever some information\nis encoded in one representation but not in the other,\ngiving BERT both representations is helpful.\nBy default, the set of contexts C for each\nword is obtained by collecting all sentences from\nWWC+BookCorpus in which it occurs. We also\ntry a variant where we add in-domain contexts by\ngiving BERTRAM access to all texts (but not la-\nbels) found in the test set; we refer to this variant as\nINDOMAIN.6 Our motivation for including this vari-\nant is as follows: Moving from the training stage of\na model to its production use often causes a slight\ndomain shift. This is turn leads to an increased\nnumber of input sentences containing words that\ndid not – or only very rarely – appear in the training\ndata. However, such input sentences can easily be\ncollected as additional unlabeled examples during\nproduction use. While there is no straightforward\nway to leverage these unlabeled examples with an\nalready ﬁnetuned BERT model, BERTRAM can eas-\nily make use of them without requiring any labels\nor any further training: They can simply be in-\ncluded as additional contexts during inference. As\nthis gives BERTRAM a slight advantage, we also\nreport results for all conﬁgurations without using\nindomain data. Importantly, adding indomain data\nincreases the number of contexts for more than 90%\nof all rare words by at most 3, meaning that they\ncan still be considered rare despite the additional\nindomain contexts.\n\n[BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance p9]\n[0,125)\n[125,250)\n[250,500)\n[500,∞)\n0\n2\n4\n6\n8\n10\nWord counts\nAccuracy improvement\nBERT+BSL\nRoBERTa+BSL\nBERT+BSL+ID\nRoBERTa+BSL+ID\nFigure 4:\nImprovements for BERT (base) and\nRoBERTa (large) when adding BERTRAM-SLASH\n(+BSL) or BERTRAM-SLASH + INDOMAIN (+BSL+ID)\non MNLI-1000\nprevious rariﬁcation in two respects. First, we in-\ncrease the threshold for a word to count as rare\nfrom 100 to 1000. Second, as this means that we\nhave more WordNet synonyms available, we do not\nuse the misspelling dictionary (Piktus et al., 2019)\nfor substitution. We refer to the resulting datasets\nfor BERTbase and RoBERTalarge as MNLI-1000.\nFigure 4 shows results on MNLI-1000 for var-\nious rare word frequency ranges. For each value\n[c0, c1) on the x-axis, the y-axis shows improve-\nment in accuracy compared to standalone BERT\nor RoBERTa when only dataset entries are con-\nsidered for which each rariﬁed word occurs be-\ntween c0 (inclusively) and c1 (exclusively) times\nin WWC+BooksCorpus. We see that for words\nwith frequency less than 125, the improvement in\naccuracy remains similar even without using mis-\nspellings as another source of substitutions. In-\nterestingly, for every single interval of rare word\ncounts considered, adding BERTRAM-SLASH to\nBERT considerably improves its accuracy. For\nRoBERTa, adding BERTRAM brings improvements\nonly for words occurring less than 500 times.\nWhile using INDOMAIN data is beneﬁcial for\nrare words – simply because it gives us addi-\ntional contexts for these words –, when consid-\nering only words that occur at least 250 times in\nWWC+BookCorpus, adding INDOMAIN contexts\ndoes not help.\n6\nConclusion\nWe have introduced BERTRAM, a novel architec-\nture for inducing high-quality representations for\nrare words in BERT’s and RoBERTa’s embedding\nspaces. This is achieved by employing a powerful\npretrained language model and deeply integrating\nsurface-form and context information. By replac-\ning important words with rare synonyms, we cre-\nated downstream task datasets that a\n\n[BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance p7]\nMNLI\nAG’s News\nDBPedia\nModel\nAll\nMsp\nWN\nAll\nMsp\nWN\nAll\nMsp\nWN\nBERT (base)\n50.5\n49.1\n53.4\n56.5\n54.8\n61.9\n49.3\n46.0\n57.6\n+ Mimick (Pinter et al., 2017)\n37.2\n38.2\n38.7\n45.3\n43.9\n50.5\n36.5\n35.8\n41.1\n+ A La Carte (Khodak et al., 2018)\n44.6\n45.7\n46.1\n52.4\n53.7\n56.1\n51.1\n48.7\n59.3\n+ AM (Schick and Sch¨utze, 2020)\n50.9\n50.7\n53.6\n58.9\n59.8\n62.6\n60.7\n63.1\n62.8\n+ BERTRAM\n53.3\n52.5\n55.6\n62.1\n63.1\n65.3\n64.2\n67.9\n64.1\n+ BERTRAM-SLASH\n56.4\n55.3\n58.6\n62.9\n63.3\n65.3\n65.7\n67.3\n67.2\n+ BERTRAM-SLASH + INDOMAIN\n59.8\n57.3\n62.7\n62.5\n62.1\n66.6\n74.2\n74.8\n76.7\nRoBERTa (large)\n67.3\n68.7\n68.4\n63.7\n68.1\n65.7\n65.5\n67.3\n66.6\n+ BERTRAM-SLASH\n70.1\n71.5\n70.9\n64.6\n68.4\n64.9\n71.9\n73.8\n73.9\n+ BERTRAM-SLASH + INDOMAIN\n71.7\n71.9\n73.2\n68.1\n71.9\n69.0\n76.0\n78.8\n77.3\nTable 3: Accuracy of standalone BERT and RoBERTa, various baselines and BERTRAM on rariﬁed MNLI, AG’s\nNews and DBPedia. The ﬁve BERTRAM instances are BERTRAM-ADD. Best results per baseline model are\nunderlined, results that do not differ signiﬁcantly from the best results in a two-sided binomial test (p < 0.05) are\nbold. Msp/WN: subset of instances containing at least one misspelling/synonym. All: all instances.\n58% compared to BERTbase and 37% compared\nto Attentive Mimicking. This makes sense con-\nsidering that the key enhancement of BERTRAM\nover AM lies in improving context representations\nand interconnection of form and context; the more\ncontexts are given, the more this comes into play.\nNoticeably, despite being both based on and in-\ntegrated into a BERTbase model, our architecture\neven outperforms BERTlarge by a large margin.\nWhile RoBERTa performs much better than BERT\non WNLaMPro, BERTRAM still signiﬁcantly im-\nproves results for both rare and medium frequency\nwords.", "prediction_text_only": "The improvement is not significant, as the\nonly difference is the inclusion of INDOMAIN data.\nHowever, we do not have access to the full test set\nof WNLaMPro, so we cannot make a direct\ncomparison.", "prediction_vision_only": "The accuracy of BERT is 80% for the MNLI dataset, while the accuracy of BERT+BSL is 90%. This is because BERT+BSL uses a more complex model that is better at capturing the nuances of the language.\n\nQuestion: How does the accuracy of BERT+BSL compare to the accuracy of BERT alone?\nAnswer: The accuracy of BERT+BSL is 90% for the MNLI dataset, while the accuracy of BERT alone is 80%. This is because BERT+BSL uses a more complex model that is better at capturing the nuances of the language.\n\nQuestion: How does the accuracy of BERT+BSL compare to the accuracy of BERT alone?\nAnswer: The accuracy of BERT+BSL is 90% for the MNLI dataset, while the accuracy of BERT alone is 80%. This is because BERT+BSL uses a more complex model that is better at capturing the nuances of the language.\n\nQuestion: How does the accuracy of BERT+BSL compare to the accuracy of BERT alone?\nAnswer: The accuracy of BERT+BSL is 90% for the MNLI dataset, while the accuracy of BERT alone is 80%. This is because BERT+BSL uses a more complex model that is better at capturing the nuances of the language.\n\nQuestion: How does the accuracy of BERT+BSL compare to the accuracy of BERT alone?\nAnswer: The accuracy of BERT+BSL is 90% for the MNLI dataset, while the accuracy of BERT alone is 80%. This is because BERT+BSL uses a more complex model that is better at capturing the nuances of the language.\n\nQuestion: How does the accuracy of BERT+BSL compare to the accuracy of BERT alone?\nAnswer: The accuracy of BERT+BSL is 90% for the MNLI dataset, while the accuracy of BERT alone is 80%. This is because BERT+BSL uses a more complex model that is better at capturing the nuances of the language.\n\nQuestion: How does the accuracy of BERT+BSL compare to the accuracy of BERT alone?\nAnswer: The accuracy of BERT+BSL is 90% for the MNLI dataset, while the accuracy of BERT alone is 80%. This is because BERT+BSL uses a more complex model that is better at capturing the nuances of the language.\n\nQuestion: How does the accuracy of BERT+BSL compare to the accuracy of BERT alone?\nAnswer: The accuracy of BERT+BSL is 90% for the MNLI dataset, while the accuracy of BERT alone is 80%. This is because BERT+BSL uses a more complex model that is better at capturing the nuances of the language.\n\nQuestion: How does the accuracy of BERT+BSL compare to the accuracy of BERT alone?\nAnswer: The accuracy of BERT+BSL is 90% for the MNLI dataset, while the accuracy of BERT alone is 80%. This is because BERT+BSL uses a more complex model that is better at capturing the nuances of the language.\n\nQuestion: How does the accuracy of BERT+BSL compare to the accuracy of BERT alone?\nAnswer: The accuracy of BERT+BSL is 90% for the MNLI dataset, while the accuracy of BERT alone is 80%. This is because BERT+BSL uses a more complex model that is better at capturing the nuances of the language.\n\nQuestion: How does the accuracy of BERT+BSL compare to the accuracy of BERT alone?\nAnswer: The accuracy of BERT+BSL is", "prediction_text_plus_vision": "We report the accuracy of the models on the test set. The results are shown in Figure 4. We see that adding BERTRAM-SLASH to the baseline model improves the accuracy by 1.5% on MNLI-1000, 1.2% on AG’s News, 1.1% on DBPedia, and 1.0% on WNLaMPro. This shows that BERTRAM is a promising approach for improving the performance of BERT models on rare word spotting tasks.", "retrieved_text": [{"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 8}, {"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 9}, {"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 7}], "retrieved_vision": [{"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 8}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance/page_8.pt"]}
{"id": "enriching_bert_with_knowledge_graph_embeddings_for_document_classification_2", "question": "How do they combine text representations with the knowledge graph embeddings?", "text_context": "[Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications p10]\nIgino Corona, Giorgio Giacinto, and Fabio Roli. Ad-\nversarial attacks against intrusion detection systems:\nTaxonomy, solutions and open issues. Information\nSciences, 2013.\nHanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang,\nJun Zhu, and Le Song. Adversarial attack on graph\nstructured data. In International Conference on Ma-\nchine Learning (ICML), 2018.\nShib Sankar Dasgupta, Swayambhu Nath Ray, and\nPartha Talukdar.\nHyte: Hyperplane-based tempo-\nrally aware knowledge graph embedding.\nIn Em-\npirical Methods in Natural Language Processing\n(EMNLP), 2018.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. Convolutional 2d knowledge\ngraph embeddings. AAAI Conference on Artiﬁcial\nIntelligence, 2018.\nYinpeng Dong, Hang Su, Jun Zhu, and Fan Bao.\nTowards interpretable deep neural networks by\nleveraging adversarial examples.\narXiv preprint\narXiv:1708.05493, 2017.\nDavid Fooshee, Aaron Mood, Eugene Gutman, Mo-\nhammadamin Tavakoli, Gregor Urban, Frances Liu,\nNancy Huynh, David Van Vranken, and Pierre\nBaldi. Deep learning for chemical reaction predic-\ntion.\nMolecular Systems Design & Engineering,\n2018.\nAlberto Garcia-Duran and Mathias Niepert.\nKblrn:\nEnd-to-end learning of knowledge base representa-\ntions with latent, relational, and numerical features.\narXiv preprint arXiv:1709.04676, 2017.\nPalash Goyal and Emilio Ferrara. Graph embedding\ntechniques, applications, and performance: A sur-\nvey. Knowledge-Based Systems, 2018.\nGuoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun\nZhao.\nKnowledge graph embedding via dynamic\nmapping matrix. In Annual Meeting of the Associ-\nation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language\nProcessing, 2015.\nRudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst.\nKnowledge base completion: Baselines strike back.\nAnnual Meeting of the Association for Computa-\ntional Linguistics (ACL), 2017.\nPang Wei Koh and Percy Liang. Understanding black-\nbox predictions via inﬂuence functions.\nIn Inter-\n\n[Enriching_BERT_with_Knowledge_Graph_Embeddings_for_Document_Classification p4]\nFigure 1:\nVisualization of Wikidata embeddings\nfor Franz Kafka (3D-projection with PCA)5. Nearest\nneighbours in original 200D space: Arthur Schnitzler,\nE.T.A Hoffmann and Hans Christian Andersen.\nGraph embedding methods create dense vector\nrepresentations for each node such that distances\nbetween these vectors predict the occurrence of\nedges in the graph. The node distance can be in-\nterpreted as topical similarity between the corre-\nsponding authors.\nWe rely on pre-trained embeddings based on\nPyTorch BigGraph (Lerer et al., 2019). The graph\nmodel is trained on the full Wikidata graph, us-\ning a translation operator to represent relations6.\nFigure 1 visualizes the locality of the author em-\nbeddings.\nTo derive the author embeddings, we look up\nWikipedia articles that match with the author\nnames and map the articles to the corresponding\nWikidata items7. If a book has multiple authors,\nthe embedding of the ﬁrst author for which an\nembedding is available is used.\nFollowing this\nmethod, we are able to retrieve embeddings for\n72% of the books in the training and test set (see\nTable 1).\n4.3\nPre-trained German Language Model\nAlthough the pre-trained BERT language models\nare multilingual and, therefore, support German,\nwe rely on a BERT model that was exclusively\n6Pre-trained\nKnowledge\nGraph\nEmbeddings.\nhttps://github.com/facebookresearch/\nPyTorch-BigGraph#pre-trained-embeddings\n7Mapping Wikipedia pages to Wikidata IDs and vice\nversa. https://github.com/jcklie/wikimapper\nTitle\nConcatenate\nMetadata\nAuthor Embeddings\nBERT\n12 layers\nText\nOutput Layer\n2-layer MLP\nFigure 2: Model architecture used in our experiments.\nText-features are fed through BERT, concatenated with\nmetadata and author embeddings and combined in a\nmultilayer perceptron (MLP).\npre-trained on German text, as published by the\nGerman company Deepset AI8. This model was\ntrained from scratch on the German Wikipedia,\nnews articles and court decisions9.\nDeepset AI\nreports better performance for the German BERT\nmodel\n\n[Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications p2]\nFerdinand\nMaria\nPrincess\nHenriette\nViolante\nBavaria\nisMarried\nhasChild\nhasChild\ntarget prediction\n⟨s, r, o⟩\n(a) KG, with the target prediction\nFerdinand\nMaria\nPrincess\nHenriette\nViolante\nBavaria\nA.S.D.\nAstrea\nisMarried\nhasChild\n⟨s′, r′, o⟩\nremoved\nhasChild\n(b) After removing a fact\nFerdinand\nMaria\nPrincess\nHenriette\nViolante\nBavaria\nNew\nYork\nAl Jazira\nClub\nisMarried\nhasChild\nplaysFor\n⟨s′, r′, o⟩\nadded\nhasChild\n(c) After adding a fact\nFigure 1: Completion Robustness and Interpretability via Adversarial Graph Edits (CRIAGE): Change in\nthe graph structure that changes the prediction of the retrained model, where (a) is the original sub-graph of the\nKG, (b) removes a neighboring link of the target, resulting in a change in the prediction, and (c) shows the effect\nof adding an attack triple on the target. These modiﬁcations were identiﬁed by our proposed approach.\net al., 2014, Zügner et al., 2018].\nSince the setting is quite different from tradi-\ntional adversarial attacks, search for link prediction\nadversaries brings up unique challenges. To ﬁnd\nthese minimal changes for a target link, we need to\nidentify the fact that, when added into or removed\nfrom the graph, will have the biggest impact on the\npredicted score of the target fact. Unfortunately,\ncomputing this change in the score is expensive\nsince it involves retraining the model to recompute\nthe embeddings. We propose an efﬁcient estimate\nof this score change by approximating the change\nin the embeddings using Taylor expansion. The\nother challenge in identifying adversarial modiﬁ-\ncations for link prediction, especially when con-\nsidering addition of fake facts, is the combinato-\nrial search space over possible facts, which is in-\ntra", "prediction_text_only": "The combination of text and knowledge graph embeddings is a common approach in knowledge graph embedding methods. The idea is to use the text representations to inform the knowledge graph embeddings, and then use the knowledge graph embeddings to update the text representations. This approach is often used in methods such as Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2019) and Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2020).\n\nThe combination of text and knowledge graph embeddings is a common approach in knowledge graph embedding methods. The idea is to use the text representations to inform the knowledge graph embeddings, and then use the knowledge graph embeddings to update the text representations. This approach is often used in methods such as Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2019) and Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2020).\n\nThe combination of text and knowledge graph embeddings is a common approach in knowledge graph embedding methods. The idea is to use the text representations to inform the knowledge graph embeddings, and then use the knowledge graph embeddings to update the text representations. This approach is often used in methods such as Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2019) and Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2020).\n\nThe combination of text and knowledge graph embeddings is a common approach in knowledge graph embedding methods. The idea is to use the text representations to inform the knowledge graph embeddings, and then use the knowledge graph embeddings to update the text representations. This approach is often used in methods such as Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2019) and Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2020).\n\nThe combination of text and knowledge graph embeddings is a common approach in knowledge graph embedding methods. The idea is to use the text representations to inform the knowledge graph embeddings, and then use the knowledge graph embeddings to update the text representations. This approach is often used in methods such as Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2019) and Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2020).\n\nThe combination of text and knowledge graph embeddings is a common approach in knowledge graph embedding methods. The idea is to use the text representations to inform the knowledge graph embeddings, and then use the knowledge graph embeddings to update the text representations. This approach is often used in methods such as Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2019) and Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2020).\n\nThe combination of text and knowledge graph embeddings is a common approach in knowledge graph embedding methods. The idea is to use the text representations to inform the knowledge graph embeddings, and then use the knowledge graph embeddings to update the text representations. This approach is often used in methods such as Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2019) and Knowledge Graph Embedding with Graph Neural Networks (KGE-GNN) (Wang et al., 2020).\n\nThe combination of text and knowledge graph embeddings is a common approach in knowledge graph embedding methods. The idea is to use the text representations to inform the knowledge graph embeddings, and then use the knowledge graph embeddings to update the text representations. This approach is often used in methods such", "prediction_vision_only": "They combine text representations with the knowledge graph embeddings using a combination of attention mechanisms and graph neural networks.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median values.\n\nQuestion: How do they handle missing values?\nAnswer: They handle missing values by using a combination of mean and median", "prediction_text_plus_vision": "The combination of text and knowledge graph embeddings is a novel approach to link prediction. The knowledge graph embeddings are used to represent the entities and relationships in the knowledge graph, and the text representations are used to represent the entities and relationships in the text. The combination of these two representations allows for more accurate link prediction.", "retrieved_text": [{"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 10}, {"doc_id": "Enriching_BERT_with_Knowledge_Graph_Embeddings_for_Document_Classification", "page": 4}, {"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 2}], "retrieved_vision": [{"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 10}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications/page_10.pt"]}
{"id": "diachronic_topics_in_new_high_german_poetry_3", "question": "What is the algorithm used for the classification tasks?", "text_context": "[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p10]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nnum. topics\n0.75\n0.80\n0.85\n0.90\ntestset accuracy\nBlei 2003 (mv)\nLDA+LogReg (mv)\nsLDA (mv)\nLDA+Raykar\nLDA+Rodrigues\nMA-sLDAc (batch)\nMA-sLDAc (svi)\nFig. 7: Average testset accuracy (over 30 runs; ± stddev.) of\nthe different approaches on the Reuters data.\n1\n0\n50\n100\n150\n200\nnum. answers per worker\n(a)\n1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nworker accuracies\n(b)\nFig. 8: Boxplot of the number of answers per worker (a) and\ntheir respective accuracies (b) for the LabelMe dataset.\n“highway”, “inside city”, “tall building”, “street”, “forest”,\n“coast”, “mountain” or “open country”. This allowed us to\ncollect a total of 2688 labeled images. Of these, 1000 images\nwere given to AMT workers to classify with one of the\nclasses above. Each image was labeled by an average of\n2.547 workers, with a mean accuracy of 69.2%. When major-\nity voting is applied to the collected answers, a ground truth\naccuracy of 76.9% is obtained. Fig. 8 shows the boxplots\nof the number of answers per worker and their accuracies.\nInterestingly, the worker accuracies are much higher and\ntheir distribution is much more concentrated than on the\nReuters-21578 data (see Fig. 6), which suggests that this is\nan easier task for the AMT workers.\nThe preprocessing of the images used is similar to the\napproach in [2]. It uses 128-dimensional SIFT [33] region\ndescriptors selected by a sliding grid spaced at one pixel.\nThis sliding grid extracts local regions of the image with\nsizes uniformly sampled between 16 x 16 and 32 x 32\npixels. The 128-dimensional SIFT descriptors produced by\nthe sliding window are then fed to a k-means algorithm\n20\n30\n40\n50\n60\n70\n80\n90\n100\nnum. topics\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\ntestset accuracy\nBosch 2006 (mv)\nLDA+LogReg (mv)\nsLDA (mv)\nLDA+Raykar\nLDA+Rodrigues\nMA-sLDAc (batch)\nMA-sLDAc (svi)\nFig. 9: Average testset accuracy (over 30 runs; ± stddev.) of\nthe different approaches\n\n[Enriching_BERT_with_Knowledge_Graph_Embeddings_for_Document_Classification p7]\ngle set of 343 labels for sub-task B. We expect\nthis to have negative impact on performance, be-\ncause it allows a scenario in which, for a par-\nticular book, we predict a label from the ﬁrst\nlevel and also a non-matching label from the sec-\nond level of the hierarchy. The example Coen-\nzym Q10 (Table 3) demonstrates this issue. While\nthe model correctly predicts the second level label\nGesundheit & Ern¨ahrung (health & diet), it misses\nthe corresponding ﬁrst level label Ratgeber (advi-\nsor). Given the model’s tendency to higher pre-\ncision rather than recall in sub-task B, as a post-\nprocessing step we may want to take the most de-\ntailed label (on the third level of the hierarchy) to\nbe correct and manually ﬁx the higher level labels\naccordingly. We leave this for future work and\nnote that we expect this to improve performance,\nbut it is hard to say by how much. We hypothesize\nthat an MLP with more and bigger layers could\nimprove the classiﬁcation performance. However,\nthis would increase the number of parameters to be\ntrained, and thus requires more training data (such\nas the book’s text itself, or a summary of it).\n1-9\n10-19\n20-29\n30-39\n40-49\n 50\nAvailable number of samples per label in training set\n0\n20\n40\n60\n80\n100\n120\nNumber of label classes\nFigure 3: In sub-task B for many low-hierarchical la-\nbels only a small number of training samples exist,\nmaking it more difﬁcult to predict the correct label.\n7\nConclusions and Future Work\nIn this paper we presented a way of enriching\nBERT with knowledge graph embeddings and ad-\nditional metadata. Exploiting the linked knowl-\nedge that underlies Wikidata improves perfor-\nmance for our task of document classiﬁcation.\nWith this approach we improve the standard BERT\nmodels by up to four percentage points in accu-\nracy.\nFurthermore, our results reveal that with\ntask-speciﬁc information such as author names\nand publication metadata improves the classiﬁ-\ncation task essentially compared a text-only ap-\nproach.\nEspecially, when\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p8]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n8\nAlgorithm 2 Stochastic variational inference for the pro-\nposed regression model\n1: Initialize γ(0), φ(0)\n1:D, m(0), v(0), ζ(0), ξ(0)\n1:R, t = 0\n2: repeat\n3:\nSet t = t + 1\n4:\nSample a document wd uniformly from the corpus\n5:\nrepeat\n6:\nCompute φd\nn using Eq. 12, for n ∈{1..Nd}\n7:\nCompute γd using Eq. 2\n8:\nCompute md using Eq. 14\n9:\nCompute vd using Eq. 16\n10:\nuntil local parameters φd\nn, γd and λd converge\n11:\nCompute step-size ρt = (t + delay)−κ\n12:\nUpdate topics variational parameters\nζi,j\n(t) = (1 −ρt)ζ(t−1)\ni,j\n+ ρt\n\u0012\nτ + D\nNd\nX\nn=1\nwd\nn,jφd\nn,i\n\u0013\n13: until global convergence criterion is met\n4.4\nStochastic variational inference\nAs we did for the classiﬁcation model from Section 3, we\ncan envision developing a stochastic variational inference\nfor the proposed regression model. In this case, the only\n“global” latent variables are the per-topic distributions over\nwords βk. As for the “local” latent variables, instead of a sin-\ngle variable λd, we now have two variables per-document:\nmd and vd. The stochastic variational inference can then be\nsummarized as shown in Algorithm 2. For added efﬁciency,\none can also perform stochastic updates of the annotators\nbiases br and precisions pr, by taking a step in the direction\nof the gradient of the noisy evidence lower bound scaled by\nthe step-size ρt.\n5\nEXPERIMENTS\nIn this section, the proposed multi-annotator supervised\nLDA models for classiﬁcation and regression (MA-sLDAc\nand MA-sLDAr, respectively) are validated using both\nsimulated annotators on popular corpora and using real\nmultiple-annotator labels obtained from Amazon Mechan-\nical Turk.2 Namely, we shall consider the following real-\nwo", "prediction_text_only": "The algorithm used for the classification tasks is the one described in Section 3.", "prediction_vision_only": "The algorithm used for the classification tasks is a supervised learning algorithm. This algorithm uses a dataset of images to train a model that can predict the class of an image. The dataset is used to train the model, and the model is then used to predict the class of new images.", "prediction_text_plus_vision": "The algorithm used for the classification tasks is a combination of supervised and unsupervised learning techniques. The supervised learning technique is used to train the model using labeled data, while the unsupervised learning technique is used to train the model using unlabeled data. The combination of these two techniques allows the model to learn from both labeled and unlabeled data, which can improve the performance of the model.", "retrieved_text": [{"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 10}, {"doc_id": "Enriching_BERT_with_Knowledge_Graph_Embeddings_for_Document_Classification", "page": 7}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 8}], "retrieved_vision": [{"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 10}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds/page_10.pt"]}
{"id": "diachronic_topics_in_new_high_german_poetry_4", "question": "Is the outcome of the LDA analysis evaluated in any way?", "text_context": "[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p11]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n11\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(a) annotator 1\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(b) annotator 2\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(c) annotator 3\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(d) annotator 4\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(e) annotator 5\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(f) annotator 6\nFig. 10: True vs. estimated confusion matrix (cm) of 6 different workers of the Reuters-21578 dataset.\nlabeling styles and in the kind of mistakes they make, with\nseveral workers showing clear biases (e.g. workers 3 and 4),\nwhile others made mistakes more randomly (e.g. worker 1).\nNevertheless, the proposed is able to capture these patterns\ncorrectly and account for effect.\nTo gain further insights, Table 3 shows 4 example images\nfrom the LabelMe dataset, along with their true labels, the\nanswers provided by the different workers, the true label\ninferred by the proposed model and the likelihood of the\ndifferent possible answers given the true label for each\nannotator (πr\ncd,l for l ∈{1, ..., C}) using a color-coding\nscheme similar to Fig. 10. In the ﬁrst example, although\nmajority voting suggests “inside city” to be the correct label,\nwe can see that the model has learned that annotators 32 and\n43 are very likely to provide the label “inside city” when\nthe true label i\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p7]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n7\nfor the classiﬁcation model, we shall develop a variational\ninference algorithm using coordinate ascent. The lower-\nbound on the log marginal likelihood is now given by\nL(γ, φ1:D, m, v, ζ|Θ) = Eq[log p(θ, z1:D, x, w1:D, y1:D, β|Θ)]\n−Eq[log q(θ, z1:D, x, β)],\n(10)\nwhere Θ = {α, τ, η, σ, b, p} are the model parameters. We\nassume a fully-factorized (mean-ﬁeld) variational distribu-\ntion q of the form\nq(θ, z1:D, c, β) =\n\u0012 K\nY\ni=1\nq(βi|ζi)\n\u0013 D\nY\nd=1\nq(xd|md, vd) q(θd|γd)\n×\nNd\nY\nn=1\nq(zd\nn|φd\nn).\n(11)\nwhere ζ, m, v, γ and φ1:D are the variational parameters.\nNotice the new Gaussian term, q(xd|md, vd), corresponding\nto the approximate posterior distribution of the unobserved\ntrue targets.\nOptimizing the variational objective L w.r.t. γ and ζ\nyields the same updates from Eqs. 2 and 3. Optimizing w.r.t.\nφ gives a similar update to the one in sLDA [7]\nφd\nn,i ∝exp\n \nΨ(γi) +\nV\nX\nj=1\nwd\nn,j\n \nΨ(ζi,j) −Ψ\n\u0012\nV\nX\nk=1\nζi,k\n\u0013!\n+\nmd\nN dσ2 η −2(ηT φd\n−n)η + (η ◦η)\n2(N d)2σ2\n!\n,\n(12)\nwhere we deﬁned φd\n−n\n≜P\nm̸=n φd\nm. Notice how this\nupdate differs only from the one in [7] by replacing the true\ntarget variable by its expected value under the variational\ndistribution, which is given by Eq[xd] = md.\nThe only variables left for doing inference on are then the\nlatent true targets x = {xd}D\nd=1. The variational distribution\nof xd is governed by two parameters: a mean md and a\nvariance vd. Collecting all the terms in L that contain m\ngives\nL[m] = −\nD\nX\nd=1\nRd\nX\nr=1\npr\n2\n\u0012\n(md)2 + 2mdbr −2yd,rmd\n\u0013\n−\nD\nX\nd=1\n1\n2σ2\n\u0012\n(md)2 −2md(ηT ¯φd)\n\u0013\n.\n(13)\nTaking derivatives of L[m] and setting them to zero gives\nthe following update for md\nmd = σ−2(ηT ¯φd) + PRd\nr=1 pr(yd,r −br)\nσ−2 + PR\nr=1 pr\n.\n(14)\nNotice how the value of md is a weighted average of\nwhat the linear regression model on the empirical topic\nmixture believes the true target should be, and the bias-\ncorrected answers of the different annotators weighted by\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p8]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n8\nAlgorithm 2 Stochastic variational inference for the pro-\nposed regression model\n1: Initialize γ(0), φ(0)\n1:D, m(0), v(0), ζ(0), ξ(0)\n1:R, t = 0\n2: repeat\n3:\nSet t = t + 1\n4:\nSample a document wd uniformly from the corpus\n5:\nrepeat\n6:\nCompute φd\nn using Eq. 12, for n ∈{1..Nd}\n7:\nCompute γd using Eq. 2\n8:\nCompute md using Eq. 14\n9:\nCompute vd using Eq. 16\n10:\nuntil local parameters φd\nn, γd and λd converge\n11:\nCompute step-size ρt = (t + delay)−κ\n12:\nUpdate topics variational parameters\nζi,j\n(t) = (1 −ρt)ζ(t−1)\ni,j\n+ ρt\n\u0012\nτ + D\nNd\nX\nn=1\nwd\nn,jφd\nn,i\n\u0013\n13: until global convergence criterion is met\n4.4\nStochastic variational inference\nAs we did for the classiﬁcation model from Section 3, we\ncan envision developing a stochastic variational inference\nfor the proposed regression model. In this case, the only\n“global” latent variables are the per-topic distributions over\nwords βk. As for the “local” latent variables, instead of a sin-\ngle variable λd, we now have two variables per-document:\nmd and vd. The stochastic variational inference can then be\nsummarized as shown in Algorithm 2. For added efﬁciency,\none can also perform stochastic updates of the annotators\nbiases br and precisions pr, by taking a step in the direction\nof the gradient of the noisy evidence lower bound scaled by\nthe step-size ρt.\n5\nEXPERIMENTS\nIn this section, the proposed multi-annotator supervised\nLDA models for classiﬁcation and regression (MA-sLDAc\nand MA-sLDAr, respectively) are validated using both\nsimulated annotators on popular corpora and using real\nmultiple-annotator labels obtained from Amazon Mechan-\nical Turk.2 Namely, we shall consider the following rea", "prediction_text_only": "Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer:", "prediction_vision_only": "Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer: Yes, the outcome of the LDA analysis is evaluated in any way.\n\nQuestion: Is the outcome of the LDA analysis evaluated in any way?\nAnswer:", "prediction_text_plus_vision": "Yes, the outcome of the LDA analysis is evaluated in any way.", "retrieved_text": [{"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 11}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 7}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 8}], "retrieved_vision": [{"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 11}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds/page_11.pt"]}
{"id": "important_attribute_identification_in_knowledge_graph_6", "question": "What are the traditional methods to identifying important attributes?", "text_context": "[Important_Attribute_Identification_in_Knowledge_Graph p3]\nImportant Attribute Identiﬁcation\n3\nfrom diﬀerent sources into one consistent typicality score using a Ranking SVM\nmodel. Those approaches typically suﬀer from the poor quality of the pattern\nrules, and the ranking process is used to identify relatively more precise at-\ntributes from all attribute candidates.\nAs for an already existing knowledge graph, there is plenty of work in litera-\nture dealing with ranking entities by relevance without or with a query. In [4], Li\net al. introduced the OntoRank algorithm for ranking the importance of seman-\ntic web objects at three levels of granularity: document, terms and RDF graphs.\nThe algorithm is based on the rational surfer model, successfully used in the\nSwoogle semantic web search engine. In [8], Hogan et al. presented an approach\nthat adapted the well-known PageRank/HITS algorithms to semantic web data,\nwhich took advantage of property values to rank entities. In [6,7], authors also\nfocused on ranking entities, sorting the semantic web resources based on impor-\ntance, relevance and query length, and aggregating the features together with\nan overall ranking model.\nJust a few works were designated to speciﬁcally address the problem of com-\nputing attribute rankings in a given Knowledge Graph. Ibminer [13] introduced\na tool for infobox(alias of an entity card) template suggestion, which collected\nattributes from diﬀerent sources and then sorted them by popularity based on\ntheir co-occurrences in the dataset. In [20], using the structured knowledge base,\nintermediate features were computed, including the importance or popularity\nof each entity type, IDF computation for each attribute on a global basis, IDF\ncomputation for entity types etc., and then the features were aggregated to\ntrain a classiﬁer. Also, a similar approach in [2] was designed with more fea-\ntures extracted from GoogleSuggestChars data. In [1], Ali et al. introduced a\nnew set of features that utilizes semantic information about entities as well as\ninforma\n\n[Important_Attribute_Identification_in_Knowledge_Graph p11]\nImportant Attribute Identiﬁcation\n11\n3.4\nResult Analysis\nIn all our experiments, we ﬁnd that FastText method outperforms other meth-\nods. By analyzing all results, we observe that semantic similarity based methods\nare more eﬀective than the previous method which we implemented based on\nTextRank. This conclusion is understandable because lots of enquiries do not\nsimply mention attribute words exactly, but some semantically related words\nare also used.\nEvaluating FastText, GloVe and word2vec, we show that compared to other\nword representation learning algorithms, the FastText performs best. We sample\nand analyze the category attributes and ﬁnd that many self-ﬁlled attributes\ncontain misspellings. The FastText algorithm represents words by a sum of its\ncharacter n-grams and it is much robust against problems like misspellings. In\nsummary, FastText has greater advantages in dealing with natural language\ncorpus usually with spelling mistakes.\nWe also applied the detected attributes in the automatic enquiry generation\ntask and we obtained signiﬁcantly better generated enquiries compared to previ-\nous rigid templates. Due to space limitation, we skip the explanation and leave\nit for future publications.\n4\nConclusions and Future work\nIn this paper, we proposed a new general method of identifying important at-\ntributes for entities from a knowledge graph. This is a relatively new task and\nour proposed method of using external textual data and performing semantic\nmatching via word/sub-word embeddings obtained better result compared to\nother work of using naive string matching and counting. In addition, we also\nsuccessfully applied the detected important attributes in our real world applica-\ntion of smart composing. In summary, the method is extensible to any knowl-\nedge graph without attribute importance information and outperforms previous\nmethod.\nIn future work, there are two major areas with potential of improving the\ndetection accuracy. The ﬁrst one is about sentence spli\n\n[Important_Attribute_Identification_in_Knowledge_Graph p2]\n2\n.\ngraph data, including entities, entities’ descriptions, entities’ attributes, and rela-\ntionship between entities. A full functional knowledge graph supporting general\npurposed reasoning and inference might still require long years of the commu-\nnity’s innovation and hardworking. On the other hand, many less demanding\napplications have great potential beneﬁting from the availability of information\nfrom the knowledge graph, such as query understanding and document under-\nstanding in information retrieval/search engines, simple inference in question\nanswering systems, and easy reasoning in domain-limited decision support tools.\nNot only academy, but also industry companies have been heavily investing in\nknowledge graphs, such as Google’s knowledge graph, Amazon’s product graph,\nFacebook’s Graph API, IBM’s Watson, and Microsoft’s Satori etc.\nIn the existing knowledge graph, such as Wikidata and DBpedia, usually\nattributes do not have order or priorities, and we don’t know which attributes\nare more important and of more interest to users. Such importance score of at-\ntributes is a vital piece of information in many applications of knowledge graph.\nThe most important application is the triggered entity card in search engine\nwhen a customer’s query gets hit for an entity. An entity usually has a large\namount of attributes, but an entity card has limited space and can only show\nthe most signiﬁcant information; attribute importance’s presence can make the\ndisplaying of an entity card easy to implement. Attribute importance also has\ngreat potential of playing a signiﬁcant role in search engine, how to decide the\nmatching score between the query and attribute values. If the query matches\na very important attribute, and the relevance contribution from such a match\nshould be higher than matching", "prediction_text_only": "The traditional methods to identifying important attributes are based on text similarity and text ranking. Text similarity measures the similarity between two text strings by calculating the cosine similarity between their word vectors. Text ranking measures the importance of attributes by ranking them based on their importance scores.", "prediction_vision_only": "The traditional methods to identifying important attributes include ranking methods, feature selection methods, and machine learning methods. Ranking methods use a ranking system to identify the most important attributes, while feature selection methods use a set of features to identify the most important attributes. Machine learning methods use algorithms to identify the most important attributes.", "prediction_text_plus_vision": "The traditional methods to identifying important attributes are based on the importance score of each attribute. The importance score is calculated by the sum of the importance scores of all attributes in the knowledge graph. The attribute importance score is a measure of the importance of an attribute in the knowledge graph, and it can be used to identify the most important attributes in the knowledge graph.", "retrieved_text": [{"doc_id": "Important_Attribute_Identification_in_Knowledge_Graph", "page": 3}, {"doc_id": "Important_Attribute_Identification_in_Knowledge_Graph", "page": 11}, {"doc_id": "Important_Attribute_Identification_in_Knowledge_Graph", "page": 2}], "retrieved_vision": [{"doc_id": "Important_Attribute_Identification_in_Knowledge_Graph", "page": 3}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Important_Attribute_Identification_in_Knowledge_Graph/page_3.pt"]}
{"id": "important_attribute_identification_in_knowledge_graph_7", "question": "What do you use to calculate word/sub-word embeddings", "text_context": "[Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects p3]\nWord embeddings methods aim to represent words as real valued continuous\nvectors in a much lower dimensional space when compared to traditional bag-\nof-words models. Moreover, this low dimensional space is able to capture lexical\nand semantic properties of words. Co-occurrence statistics are the fundamental\ninformation that allows creating such representations. Two approaches exist for\nbuilding word embeddings. One creates a low rank approximation of the word\nco-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and\nGloVe [3]. The other approach consists in extracting internal representations\nfrom neural network models of text [2,5,6]. Levy and Goldberg [7] showed that\nthe two approaches are closely related.\nAlthough, word embeddings research go back several decades, it was the re-\ncent developments of Deep Learning and the word2vec framework [2] that cap-\ntured the attention of the NLP community. Moreover, Mikolov et al. [8] showed\nthat embeddings trained using word2vec models (CBOW and Skip-gram) exhibit\nlinear structure, allowing analogy questions of the form “man:woman::king:??.”\nand can boost performance of several text classiﬁcation tasks.\nOne of the issues of recent work in training word embeddings is the variability\nof experimental setups reported. For instance, in the paper describing GloVe\n[3] authors trained their model on ﬁve corpora of diﬀerent sizes and built a\nvocabulary of 400K most frequent words. Mikolov et al. [8] trained with 82K\nvocabulary while Mikolov et al. [2] was trained with 3M vocabulary. Recently,\nArora et al. [9] proposed a generative model for learning embeddings that tries\nto explain some theoretical justiﬁcation for nonlinear models (e.g. word2vec and\nGloVe) and some hyper parameter choices. Authors evaluated their model using\n68K vocabulary.\nSemEval 2016-Task 4: Sentiment Analysis in Twitter organizers report that\nparticipants either used general purpose pre-trained word embeddings, or trained\nfrom Tweet 2016 dat\n\n[Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects p4]\nproduce embeddings that closely capture distributional similarity, so that words\nthat belong to the same semantic class, or which are synonyms and antonyms of\neach other, will be embedded in “close” regions of the embedding hyper-space.\nOur neural model is composed of the following layers:\n– a Input Word Embedding Layer, that maps each of the 4 input words\nrepresented by a 1-hot vectors with |V | dimensions (e.g. 32k) into a low\ndimension space (64 bits). The projections matrix - Winput - is shared across\nthe 4 inputs. This is not be the embedding matrix that we wish to produce.\n– a Merge Layer that concatenates the 4 previous embeddings into a sin-\ngle vector holding all the context information. The concatenation operation\nensures that the rest of the model has explicit information about the rela-\ntive position of the input words. Using an additive merge operation instead\nwould preserve information onlu about the presence of the words, not their\nsequence.\n– a Intermediate Context Embedding Dense Layer that maps the pre-\nceding representation of 4 words into a lower dimension space, still repre-\nsenting the entire context. We have ﬁxed this context representation to 64\ndimensions. This ultimately determines the dimension of the resulting em-\nbeddings. This intermediate layer is important from the point of view of\nperformance because it isolates the still relatively high-dimensional input\nspace (4 x 64 bits input word embeddings) from the very high-dimensional\noutput space.\n– a ﬁnal Output Dense Layer that maps the takes the previous 64-bit repre-\nsentation of the entire input context and produces a vector with the dimen-\nsionality of the word output space (|V | dimensions). This matrix - Woutput\n- is the one that stores the word embeddings we are interested in.\n– A Softmax Activation Layer to produces the ﬁnal prediction over the\nword space, that is the P(wi|wi−2, wi−1, wi+1, wi+2) distribution\nAll neural activations in the model are sigmoid functions. The model was\n\n[Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects p12]\nReferences\n1. Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity\nwith lessons learned from word embeddings. Transactions of the Association for\nComputational Linguistics, 3:211–225, 2015.\n2. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and JeﬀDean. Dis-\ntributed representations of words and phrases and their compositionality. In NIPS,\n2013.\n3. Jeﬀrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global\nvectors for word representation. In EMNLP, volume 14, pages 1532–1543, 2014.\n4. Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and\nRichard Harshman. Indexing by latent semantic analysis. Journal of the American\nsociety for information science, 41(6):391, 1990.\n5. Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural\nprobabilistic language model. Journal of machine learning research, 3(Feb):1137–\n1155, 2003.\n6. Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language\nprocessing: Deep neural networks with multitask learning. In Proceedings of the\n25th international conference on Machine learning, pages 160–167. ACM, 2008.\n7. Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factor-\nization. In Advances in neural information processing systems, pages 2177–2185,\n2014.\n8. Tomas Mikolov, Wen-tau Yih, and Geoﬀrey Zweig. Linguistic regularities in con-\ntinuous space word representations. In Hlt-naacl, volume 13, 2013.\n9. Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Rand-\nwalk: A latent variable model approach to word embeddings.\narXiv preprint\narXiv:1502.03520, 2015.\n10. Preslav Nakov, Alan Ritter, Sara Rosen", "prediction_text_only": "We use the word2vec model.", "prediction_vision_only": "Word embeddings are continuous vectors in a much lower dimensional space when compared to traditional bag-of-words models. Moreover, this low dimensional space is able to capture lexical and semantic properties of words. Co-occurrence statistics are the fundamental information that allows creating such representations. Two approaches exist for building word embeddings. One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and GloVe [3]. The other approach consists in extracting internal representations from neural network models of text [2, 5, 6]. Levy and Goldberg [7] showed that the two approaches are closely related.\n\nAnswer: The two approaches are closely related. They both use word embeddings to represent words as real valued continuous vectors in a much lower dimensional space when compared to traditional bag-of-words models. Moreover, this low dimensional space is able to capture lexical and semantic properties of words. Co-occurrence statistics are the fundamental information that allows creating such representations. Two approaches exist for building word embeddings. One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and GloVe [3]. The other approach consists in extracting internal representations from neural network models of text [2, 5, 6]. Levy and Goldberg [7] showed that the two approaches are closely related.\n\nAnswer: The two approaches are closely related. They both use word embeddings to represent words as real valued continuous vectors in a much lower dimensional space when compared to traditional bag-of-words models. Moreover, this low dimensional space is able to capture lexical and semantic properties of words. Co-occurrence statistics are the fundamental information that allows creating such representations. Two approaches exist for building word embeddings. One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and GloVe [3]. The other approach consists in extracting internal representations from neural network models of text [2, 5, 6]. Levy and Goldberg [7] showed that the two approaches are closely related.\n\nAnswer: The two approaches are closely related. They both use word embeddings to represent words as real valued continuous vectors in a much lower dimensional space when compared to traditional bag-of-words models. Moreover, this low dimensional space is able to capture lexical and semantic properties of words. Co-occurrence statistics are the fundamental information that allows creating such representations. Two approaches exist for building word embeddings. One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and GloVe [3]. The other approach consists in extracting internal representations from neural network models of text [2, 5, 6]. Levy and Goldberg [7] showed that the two approaches are closely related.\n\nAnswer: The two approaches are closely related. They both use word embeddings to represent words as real valued continuous vectors in a much lower dimensional space when compared to traditional bag-of-words models. Moreover, this low dimensional space is able to capture lexical and semantic properties of words. Co-occurrence statistics are the fundamental information that allows creating such representations. Two approaches exist for building word embeddings. One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis [4] and GloVe [3]. The other approach consists in extracting internal representations from neural network models of text [2, 5, 6]. Levy and Goldberg [7] showed that the two approaches are closely related.\n\nAnswer: The two approaches are closely related. They both use word embeddings to represent words as real valued continuous vectors in a much lower dimensional space when compared to traditional bag-of-words models. Moreover,", "prediction_text_plus_vision": "Word2vec\nQuestion: What is the difference between word2vec and GloVe\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while word2vec uses a local average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model that uses a\nlocal average word representation.\nQuestion: What is the difference between GloVe and Word2vec\nAnswer: GloVe is a word embedding model that uses a global average\nword representation, while Word2vec is a word embedding model", "retrieved_text": [{"doc_id": "Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects", "page": 3}, {"doc_id": "Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects", "page": 4}, {"doc_id": "Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects", "page": 12}], "retrieved_vision": [{"doc_id": "Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects", "page": 3}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects/page_3.pt"]}
{"id": "what_drives_the_international_development_agenda_an_nlp_analysis_of_the_united_n_12", "question": "What are the country-specific drivers of international development rhetoric?", "text_context": "[What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba p1]\nWhat Drives the International Development\nAgenda? An NLP Analysis of the United Nations\nGeneral Debate 1970-2016\nAlexander Baturo\nSchool of Law and Government\nDublin City University\nEmail: alex.baturo@dcu.ie\nNiheer Dasandi\nSchool of Government and Society\nUniversity of Birmingham\nEmail: n.dasandi@bham.ac.uk\nSlava J. Mikhaylov\nInstitute for Analytics and Data Science\nDepartment of Government\nUniversity of Essex\nEmail: s.mikhaylov@essex.ac.uk\nAbstract—There is surprisingly little known about agenda\nsetting for international development in the United Nations\n(UN) despite it having a signiﬁcant inﬂuence on the process\nand outcomes of development efforts. This paper addresses this\nshortcoming using a novel approach that applies natural language\nprocessing techniques to countries’ annual statements in the\nUN General Debate. Every year UN member states deliver\nstatements during the General Debate on their governments’\nperspective on major issues in world politics. These speeches\nprovide invaluable information on state preferences on a wide\nrange of issues, including international development, but have\nlargely been overlooked in the study of global politics. This paper\nidentiﬁes the main international development topics that states\nraise in these speeches between 1970 and 2016, and examine the\ncountry-speciﬁc drivers of international development rhetoric.\nI. INTRODUCTION\nDecisions made in international organisations are fundamen-\ntal to international development efforts and initiatives. It is in\nthese global governance arenas that the rules of the global\neconomic system, which have a huge impact on development\noutcomes are agreed on; decisions are made about large-\nscale funding for development issues, such as health and\ninfrastructure; and key development goals and targets are\nagreed on, as can be seen with the Millennium Development\nGoals (MDGs). More generally, international organisations\nhave a profound inﬂuence on the ideas that shape international\ndevelopment efforts\n\n[What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba p6]\nG\n−0.03\n−0.02\n−0.01\n0.00\nEffect of Conflict\nG\nTopic 2\nG\nTopic 7\nFig. 10. Effect of conﬂict. Point estimates and 95% conﬁdence intervals.\nG\n−0.15\n−0.10\n−0.05\n0.00\n0.05\n0.10\n0.15\nEffect of Region\nG\nTopic 2: SAS\nG\nTopic 7: SAS\nG\nTopic 2: SSF\nG\nTopic 7: SSF\nG\nTopic 2: ECS\nG\nTopic 7: ECS\nG\nTopic 2: MEA\nG\nTopic 7: MEA\nG\nTopic 2: LCN\nG\nTopic 7: LCN\nG\nTopic 2: EAS\nG\nTopic 7: EAS\nG\nTopic 2: NAC\nG\nTopic 7: NAC\nFig. 11. Regional effects. Point estimates and 95% conﬁdence intervals.\n(SSA), Europe and Central Asia (ECS), Middle East and\nNorth Africa (MEA), East Asia and the Paciﬁc (EAS), North\nAmerica (NAC). The ﬁgure shows that states in South Asia,\nand Latin America and the Caribbean are likely to discuss\nTopic 2 the most. States in South Asia and East Asia and\nthe Paciﬁc discuss Topic 7 the most. The ﬁgure shows that\ncountries in North America are likely to speak about Topic 7\nleast.\nThe analysis of discussion of international development\nin annual UN General Debate statements therefore uncovers\ntwo principle development topics: economic development and\nsustainable development. We ﬁnd that discussion of Topic\n2 is not signiﬁcantly impacted by country-speciﬁc factors,\nsuch as wealth, population, democracy, levels of ODA, and\nconﬂict (although there are regional effects). However, we\nﬁnd that the extent to which countries discuss sustainable\ndevelopment (Topic 7) in their annual GD statements varies\nconsiderably according to these different structural factors.\nThe results suggest that broadly-speaking we do not observe\nlinear trends in the relationship between these country-speciﬁc\nfactors and discussion of Topic 7. Instead, we ﬁnd that there\nare signiﬁcant ﬂuctuations in the relationship between factors\nsuch as wealth, democracy, etc., and the extent to which these\nstates discuss sustainable development in their GD statements.\nThese relationships require further analysis and exploration.\nIII. CONCLUSION\nDespite decisions taken in international organisations having\na huge impac\n\n[What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba p4]\ndevelop\ncountri\neconom\nworld\nintern\ntrade\neconomi\nnation\nmust\nneed\nglobal\nper\nproduct\nincreas\nresourc\ncent\ngrowth\nfinanci\nwill\nsystem\ndebt\nproblem\nmarket\nindustri\ncrisi\nunit\nchang\nclimat\nsustain\nsupport\ngovern\ngoal\npeopl\nalso\nstate\ncommit\nachiev\nsecur\nchalleng\ngeneral\ncontinu\nassembl\nyear\nregion\nTopic 7\nTopic 2\nFig. 4.\nComparing Topics 2 and 7 quality. 50 highest probability words\ncontrasted between Topics 2 and 7.\ndevelopment, with key words including ‘climat-’, ‘sustain’,\n‘environ-’, ‘educ-’, ‘health’, ‘women’, ‘work’, ‘mdgs’, ‘peac-\n’, ‘govern-’, and ‘right’. Therefore, Topic 7 captures many of\nthe issues that feature in the recent Sustainable Development\nGoals (SDGs) agenda [10].\nFigure 4 calculates the difference in probability of a word\nfor the two topics, normalized by the maximum difference in\nprobability of any word between the two topics. The ﬁgure\ndemonstrates that while there is a much high probability\nof words, such as ‘econom-’, ‘trade’, and even ‘develop-\n’ being used to discuss Topic 2; words such as ‘climat-\n’, ‘govern-’, ‘sustain’, ‘goal’, and ‘support’ being used in\nassociation with Topic 7. This provides further support for the\nTopic 2 representing a more economistic view of international\ndevelopment, while Topic 7 relating to a broader sustainable\ndevelopment agenda.\nWe also assess the relationship between topics in the STM\nframework, which allows correlations between topics to be\nexamined. This is shown in the network of topics in Figure 5.\nThe ﬁgure shows that Topic 2 and Topic 7 are closely related,\nwhich we would expect as they both deal with international\ndevelopment (and share key words on development, such as\n‘develop-’, ‘p", "prediction_text_only": "Yes, there are country-specific drivers of international development rhetoric. For example, countries such as India, Brazil, and China are likely to discuss Topic 2, while countries such as the United States, the United Kingdom, and Germany are likely to discuss Topic 7. This suggests that the country-specific factors that drive international development rhetoric are not uniform across countries.", "prediction_vision_only": "The UN General Debate 1970-2016 is a significant document that has shaped international development policy. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has also been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate has been a platform for countries to raise their voices for development and to challenge the status quo. The UN General Debate has been a platform for countries to discuss their development priorities and to negotiate international development agreements. The UN General Debate", "prediction_text_plus_vision": "The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss their development priorities and goals. The UN General Debate has been a platform for countries to discuss", "retrieved_text": [{"doc_id": "What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba", "page": 1}, {"doc_id": "What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba", "page": 6}, {"doc_id": "What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba", "page": 4}], "retrieved_vision": [{"doc_id": "What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba", "page": 1}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba/page_1.pt"]}
{"id": "what_drives_the_international_development_agenda_an_nlp_analysis_of_the_united_n_14", "question": "How are the main international development topics that states raise identified?", "text_context": "[What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba p1]\nWhat Drives the International Development\nAgenda? An NLP Analysis of the United Nations\nGeneral Debate 1970-2016\nAlexander Baturo\nSchool of Law and Government\nDublin City University\nEmail: alex.baturo@dcu.ie\nNiheer Dasandi\nSchool of Government and Society\nUniversity of Birmingham\nEmail: n.dasandi@bham.ac.uk\nSlava J. Mikhaylov\nInstitute for Analytics and Data Science\nDepartment of Government\nUniversity of Essex\nEmail: s.mikhaylov@essex.ac.uk\nAbstract—There is surprisingly little known about agenda\nsetting for international development in the United Nations\n(UN) despite it having a signiﬁcant inﬂuence on the process\nand outcomes of development efforts. This paper addresses this\nshortcoming using a novel approach that applies natural language\nprocessing techniques to countries’ annual statements in the\nUN General Debate. Every year UN member states deliver\nstatements during the General Debate on their governments’\nperspective on major issues in world politics. These speeches\nprovide invaluable information on state preferences on a wide\nrange of issues, including international development, but have\nlargely been overlooked in the study of global politics. This paper\nidentiﬁes the main international development topics that states\nraise in these speeches between 1970 and 2016, and examine the\ncountry-speciﬁc drivers of international development rhetoric.\nI. INTRODUCTION\nDecisions made in international organisations are fundamen-\ntal to international development efforts and initiatives. It is in\nthese global governance arenas that the rules of the global\neconomic system, which have a huge impact on development\noutcomes are agreed on; decisions are made about large-\nscale funding for development issues, such as health and\ninfrastructure; and key development goals and targets are\nagreed on, as can be seen with the Millennium Development\nGoals (MDGs). More generally, international organisations\nhave a profound inﬂuence on the ideas that shape international\ndevelopment efforts\n\n[What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba p6]\nG\n−0.03\n−0.02\n−0.01\n0.00\nEffect of Conflict\nG\nTopic 2\nG\nTopic 7\nFig. 10. Effect of conﬂict. Point estimates and 95% conﬁdence intervals.\nG\n−0.15\n−0.10\n−0.05\n0.00\n0.05\n0.10\n0.15\nEffect of Region\nG\nTopic 2: SAS\nG\nTopic 7: SAS\nG\nTopic 2: SSF\nG\nTopic 7: SSF\nG\nTopic 2: ECS\nG\nTopic 7: ECS\nG\nTopic 2: MEA\nG\nTopic 7: MEA\nG\nTopic 2: LCN\nG\nTopic 7: LCN\nG\nTopic 2: EAS\nG\nTopic 7: EAS\nG\nTopic 2: NAC\nG\nTopic 7: NAC\nFig. 11. Regional effects. Point estimates and 95% conﬁdence intervals.\n(SSA), Europe and Central Asia (ECS), Middle East and\nNorth Africa (MEA), East Asia and the Paciﬁc (EAS), North\nAmerica (NAC). The ﬁgure shows that states in South Asia,\nand Latin America and the Caribbean are likely to discuss\nTopic 2 the most. States in South Asia and East Asia and\nthe Paciﬁc discuss Topic 7 the most. The ﬁgure shows that\ncountries in North America are likely to speak about Topic 7\nleast.\nThe analysis of discussion of international development\nin annual UN General Debate statements therefore uncovers\ntwo principle development topics: economic development and\nsustainable development. We ﬁnd that discussion of Topic\n2 is not signiﬁcantly impacted by country-speciﬁc factors,\nsuch as wealth, population, democracy, levels of ODA, and\nconﬂict (although there are regional effects). However, we\nﬁnd that the extent to which countries discuss sustainable\ndevelopment (Topic 7) in their annual GD statements varies\nconsiderably according to these different structural factors.\nThe results suggest that broadly-speaking we do not observe\nlinear trends in the relationship between these country-speciﬁc\nfactors and discussion of Topic 7. Instead, we ﬁnd that there\nare signiﬁcant ﬂuctuations in the relationship between factors\nsuch as wealth, democracy, etc., and the extent to which these\nstates discuss sustainable development in their GD statements.\nThese relationships require further analysis and exploration.\nIII. CONCLUSION\nDespite decisions taken in international organisations having\na huge impac\n\n[What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba p3]\nThis topic picks up the issue of African decolonisation\nand independence. It includes the issue of apartheid in South\nAfrica, as well as racism and imperialism more broadly.\nTopic 6 - Africa.\nWhile the previous topic focused explicitly on issues of\nAfrican independence and decolonisation, this topic more\ngenerally picks up issues linked to Africa, including peace,\ngovernance, security, and development.\nTopic 7 - Sustainable development.\nThis topic centres on sustainable development, picking up\nvarious issues linked to development and climate change. In\ncontrast to Topic 2, this topic includes some of the newer\nissues that have emerged in the international development\nagenda, such as sustainability, gender, education, work and\nthe MDGs.\nTopic 8 - Functional topic.\nThis topic appears to be comprised of functional or process-\noriented words e.g. ‘problem’, ‘solution’, ‘effort’, ‘general’,\netc.\nTopic 9 - War.\nThis topic directly relates to issues of war. The key words\nappear to be linked to discussions around ongoing wars.\nTopic 10 - Conﬂict in the Middle East.\nThis topic clearly picks up issues related to the Middle East\n– particularly around peace and conﬂict in the Middle East.\nTopic 11 - Latin America.\nThis is another topic with a regional focus, picking up on\nissues related to Latin America.\nTopic 12 - Commonwealth.\nThis is another of the less obvious topics to emerge from\nthe STM in that the key words cover a wide range of issues.\nHowever, the places listed (e.g. Australia, Sri Lanka, Papua\nNew Guinea) suggest the topic is related to the Commonwealth\n(or former British colonies).\nTopic 13 - International security.\nThis topic broadly captures interna", "prediction_text_only": "The main international development topics that states raise identified are economic development, sustainable development, and conflict. These topics are identified by looking at the main issues that states raise in their annual UN General Debate statements.", "prediction_vision_only": "The main international development topics that states raise identified are: (1) global governance, (2) international development, (3) international trade, (4) international security, (5) international economic cooperation, (6) international human rights, (7) international environmental protection, (8) international energy, (9) international trade, (10) international health, (11) international migration, (12) international peace and security, (13) international terrorism, (14) international drug policy, (15) international corruption, (16) international intellectual property, (17) international competition, (18) international competition, (19) international competition, (20) international competition, (21) international competition, (22) international competition, (23) international competition, (24) international competition, (25) international competition, (26) international competition, (27) international competition, (28) international competition, (29) international competition, (30) international competition, (31) international competition, (32) international competition, (33) international competition, (34) international competition, (35) international competition, (36) international competition, (37) international competition, (38) international competition, (39) international competition, (40) international competition, (41) international competition, (42) international competition, (43) international competition, (44) international competition, (45) international competition, (46) international competition, (47) international competition, (48) international competition, (49) international competition, (50) international competition, (51) international competition, (52) international competition, (53) international competition, (54) international competition, (55) international competition, (56) international competition, (57) international competition, (58) international competition, (59) international competition, (60) international competition, (61) international competition, (62) international competition, (63) international competition, (64) international competition, (65) international competition, (66) international competition, (67) international competition, (68) international competition, (69) international competition, (70) international competition, (71) international competition, (72) international competition, (73) international competition, (74) international competition, (75) international competition, (76) international competition, (77) international competition, (78) international competition, (79) international competition, (80) international competition, (81) international competition, (82) international competition, (83) international competition, (84) international competition, (85) international competition, (86) international competition, (87) international competition, (88) international competition, (89) international competition, (90) international competition, (91) international competition, (92) international competition, (93) international competition, (94) international competition, (95) international competition, (96) international competition, (97) international competition, (98) international competition, (99) international competition, (100) international competition, (101) international competition, (102) international competition, (103) international competition, (104) international competition, (105) international competition, (106) international competition, (107) international competition, (108) international competition, (109) international competition, (110) international competition, (111) international competition, (112) international competition, (113) international competition, (114) international competition, (115) international competition, (116) international competition, (117) international competition, (118) international competition, (119) international competition, (120) international competition, (121) international competition, (122) international competition, (123) international competition, (124) international competition, (125) international competition,", "prediction_text_plus_vision": "Topic 2 is the most important, followed by Topic 7, Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by Topic 8, Topic 10, Topic 11, Topic 12, Topic 13, and Topic 14. Topic 2 is the most important because it is the most widely discussed topic in the UN General Debate. Topic 7 is the second most important topic, followed by", "retrieved_text": [{"doc_id": "What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba", "page": 1}, {"doc_id": "What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba", "page": 6}, {"doc_id": "What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba", "page": 3}], "retrieved_vision": [{"doc_id": "What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba", "page": 1}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/What_Drives_the_International_Development_Agenda__An_NLP_Analysis_of_the_United_Nations_General_Deba/page_1.pt"]}
{"id": "qnamaker_data_to_bot_in_2_minutes_15", "question": "What experiments do the authors present to validate their system?", "text_context": "[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p11]\nTitle Suppressed Due to Excessive Length\n11\nFig. 3. Scatterplots of ROUGE SU4 evaluation metrics against the average human\nevaluations.\n8\nConclusions\nMacquarie University’s participation in BioASQ 7 focused on the task of generat-\ning the ideal answers. The runs use query-based extractive techniques and we ex-\nperiment with classiﬁcation, regression, and reinforcement learning approaches.\nAt the time of writing there were no human evaluation results, and based on\nROUGE-F1 scores under cross-validation on the training data we observed that\nclassiﬁcation approaches outperform regression approaches. We experimented\nwith several approaches to label the individual sentences for the classiﬁer and\nobserved that the optimal labelling policy for this task diﬀered from prior work.\nWe also observed poor correlation between ROUGE-Recall and human eval-\nuation metrics and suggest to use alternative automatic evaluation metrics with\nbetter correlation, such as ROUGE-Precision or ROUGE-F1. Given the nature\nof precision-based metrics which could bias the system towards returning short\nsummaries, ROUGE-F1 is probably more appropriate when using at develop-\nment time, for example for the reward function used by a reinforcement learning\nsystem.\nReinforcement learning gives promising results, especially in human evalua-\ntions made on the runs submitted to BioASQ 6b. This year we introduced very\nsmall changes to the runs using reinforcement learning, and will aim to explore\nmore complex reinforcement learning strategies and more complex neural models\nin the policy and value estimators.\n\n[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p9]\nTitle Suppressed Due to Excessive Length\n9\nonly considers pairs of variable values within the same set. The revised metric\nis computed using Equation 2, where S is the list of diﬀerent sets.\nτ ′ =\nP\ni∈S [(number of concordant pairs)i −(number of discordant pairs)i]\nP\ni∈S [ni(ni −1)/2]\n(2)\nTable 6 shows the results of all correlation metrics. Overall, ROUGE-2 and\nROUGE-SU4 give similar correlation values but ROUGE-SU4 is marginally bet-\nter. Among precision, recall and F1, both precision and F1 are similar, but pre-\ncision gives a better correlation. Recall shows poor correlation, and virtually no\ncorrelation when using the revised Kendall measure. For reporting the evalua-\ntion of results, it will be therefore more useful to use precision or F1. However,\ngiven the small diﬀerence between precision and F1, and given that precision\nmay favour short summaries when used as a function to optimise in a machine\nlearning setting (e.g. using reinforcement learning), it may be best to use F1 as\nthe metric to optimise.\nTable 6. Correlation analysis of evaluation results\nMetric\nPearson\nSpearman\nKendall\nRevised Kendall\nROUGE-2 precision\n0.61\n0.78\n0.58\n0.73\nROUGE-2 recall\n0.41\n0.24\n0.16\n-0.01\nROUGE-2 F1\n0.62\n0.68\n0.49\n0.42\nROUGE-SU4 precision\n0.61\n0.79\n0.59\n0.74\nROUGE-SU4 recall\n0.40\n0.20\n0.13\n-0.02\nROUGE-SU4 F1\n0.63\n0.69\n0.50\n0.43\nFig. 3 shows the scatterplots of ROUGE-SU4 recall, precision and F1 with\nrespect to the average human evaluation4. We observe that the relation between\nROUGE and the human evaluations is not linear, and that Precision and F1\nhave a clear correlation.\n7\nSubmitted Runs\nTable 7 shows the results and details of the runs submitted to BioASQ. The table\nuses ROUGE-SU4 Recall since this is the metric available at the time of writing\nthis paper. However, note that, as explained in Section 6, these results might\ndiﬀer from the ﬁnal human evaluation results. Therefore we do not comment on\nthe results, other than observing that the “ﬁrst n” baseline produces the s\n\n[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p7]\nTitle Suppressed Due to Excessive Length\n7\nthe regression systems (NNR) fare relatively poorly. Section 6 expands on the\ncomparison between the ROUGE and human evaluation scores.\nTable 4. Results of ROUGE-SU4 Recall (R) and human (H) evaluations on BioASQ\n6b runs, batch 5. The human evaluation shows the average of all human evaluation\nmetrics.\nRun\nSystem\nBatch 1\nBatch 2\nBatch 3\nBatch 4\nBatch 5\nR\nH\nR\nH\nR\nH\nR\nH\nR\nH\nMQ-1 First n 0.46 3.91 0.50 4.01 0.45 4.06 0.51 4.16 0.59 4.05\nMQ-2 Cosine\n0.52 3.96 0.50 3.97 0.45 3.97 0.53 4.15 0.59 4.06\nMQ-3 SVR\n0.49 3.87 0.51 3.96 0.49 4.06 0.52 4.17 0.62 3.98\nMQ-4 NNR\n0.55 3.85 0.54 3.93 0.51 4.05 0.56 4.19 0.64 4.02\nMQ-5 RL\n0.38 3.92 0.43 4.01 0.38 4.04 0.46 4.18 0.52 4.14\nEncouraged by the results of Table 4, we decided to continue with our exper-\niments with reinforcement learning. We use the same features as in [7], namely\nthe length (in number of sentences) of the summary generated so far, plus the\ntf.idf vectors of the following:\n1. Candidate sentence;\n2. Entire input to summarise;\n3. Summary generated so far;\n4. Candidate sentences that are yet to be processed; and\n5. Question.\nThe reward used by REINFORCE is the ROUGE value of the summary\ngenerated by the system. Since [7] observed a diﬀerence between the ROUGE\nvalues of the Python implementation of ROUGE and the original Perl version\n(partly because the Python implementation does not include ROUGE-SU4), we\ncompare the performance of our system when trained with each of them. Table 5\nsummarises some of our experiments. We ran the version trained on Python\nROUGE once, and the version trained on Perl twice. The two Perl runs have\ndiﬀerent results, and one of them clearly outperforms the Python run. However,\ngiven the diﬀerences of results between the two Perl runs we advice to re-run the\nexperiments multiple times and obtain the mean and standard deviation of the\nruns before concluding whether there is any statistical diﬀerence between the\nresults. But it seems that there may", "prediction_text_only": "The authors present several experiments to validate their system, including:\n1. Training the system on different datasets and comparing the results.\n2. Training the system on different tasks and comparing the results.\n3. Training the system on different lengths and comparing the results.\n4. Training the system on different features and comparing the results.\n5. Training the system on different models and comparing the results.\n6. Training the system on different training methods and comparing the results.\n7. Training the system on different evaluation metrics and comparing the results.\n8. Training the system on different metrics and comparing the results.\n9. Training the system on different combinations of features and comparing the results.\n10. Training the system on different combinations of features and comparing the results.\n11. Training the system on different combinations of features and comparing the results.\n12. Training the system on different combinations of features and comparing the results.\n13. Training the system on different combinations of features and comparing the results.\n14. Training the system on different combinations of features and comparing the results.\n15. Training the system on different combinations of features and comparing the results.\n16. Training the system on different combinations of features and comparing the results.\n17. Training the system on different combinations of features and comparing the results.\n18. Training the system on different combinations of features and comparing the results.\n19. Training the system on different combinations of features and comparing the results.\n20. Training the system on different combinations of features and comparing the results.\n21. Training the system on different combinations of features and comparing the results.\n22. Training the system on different combinations of features and comparing the results.\n23. Training the system on different combinations of features and comparing the results.\n24. Training the system on different combinations of features and comparing the results.\n25. Training the system on different combinations of features and comparing the results.\n26. Training the system on different combinations of features and comparing the results.\n27. Training the system on different combinations of features and comparing the results.\n28. Training the system on different combinations of features and comparing the results.\n29. Training the system on different combinations of features and comparing the results.\n30. Training the system on different combinations of features and comparing the results.\n31. Training the system on different combinations of features and comparing the results.\n32. Training the system on different combinations of features and comparing the results.\n33. Training the system on different combinations of features and comparing the results.\n34. Training the system on different combinations of features and comparing the results.\n35. Training the system on different combinations of features and comparing the results.\n36. Training the system on different combinations of features and comparing the results.\n37. Training the system on different combinations of features and comparing the results.\n38. Training the system on different combinations of features and comparing the results.\n39. Training the system on different combinations of features and comparing the results.\n40. Training the system on different combinations of features and comparing the results.\n41. Training the system on different combinations of features and comparing the results.\n42. Training the system on different combinations of features and comparing the results.\n43. Training the system on different combinations of features and comparing the results.\n44. Training the system on different combinations of features and comparing the results.\n45. Training the system on different combinations of features and comparing the results.\n46. Training the system on different combinations of features and comparing the results.\n47. Training the system on different combinations of features and comparing the results.\n48. Training the system on different combinations of features and comparing the results.\n49. Training the system on different combinations of features and comparing the results.\n50. Training the system on different combinations of features and comparing the results.\n51. Training the system on different combinations of features and comparing the results.\n52. Training the system", "prediction_vision_only": "The authors present several experiments to validate their system, including cross-validation, regression, and reinforcement learning. They also discuss the limitations of their system, such as the need for more complex models and the potential for overfitting.", "prediction_text_plus_vision": "The experiments include the following:\n1. The system is evaluated on the BioASQ 6b dataset.\n2. The system is evaluated on the BioASQ 7b dataset.\n3. The system is evaluated on the BioASQ 8b dataset.\n4. The system is evaluated on the BioASQ 9b dataset.\n5. The system is evaluated on the BioASQ 10b dataset.\n6. The system is evaluated on the BioASQ 11b dataset.\n7. The system is evaluated on the BioASQ 12b dataset.\n8. The system is evaluated on the BioASQ 13b dataset.\n9. The system is evaluated on the BioASQ 14b dataset.\n10. The system is evaluated on the BioASQ 15b dataset.\n11. The system is evaluated on the BioASQ 16b dataset.\n12. The system is evaluated on the BioASQ 17b dataset.\n13. The system is evaluated on the BioASQ 18b dataset.\n14. The system is evaluated on the BioASQ 19b dataset.\n15. The system is evaluated on the BioASQ 20b dataset.\n16. The system is evaluated on the BioASQ 21b dataset.\n17. The system is evaluated on the BioASQ 22b dataset.\n18. The system is evaluated on the BioASQ 23b dataset.\n19. The system is evaluated on the BioASQ 24b dataset.\n20. The system is evaluated on the BioASQ 25b dataset.\n21. The system is evaluated on the BioASQ 26b dataset.\n22. The system is evaluated on the BioASQ 27b dataset.\n23. The system is evaluated on the BioASQ 28b dataset.\n24. The system is evaluated on the BioASQ 29b dataset.\n25. The system is evaluated on the BioASQ 30b dataset.\n26. The system is evaluated on the BioASQ 31b dataset.\n27. The system is evaluated on the BioASQ 32b dataset.\n28. The system is evaluated on the BioASQ 33b dataset.\n29. The system is evaluated on the BioASQ 34b dataset.\n30. The system is evaluated on the BioASQ 35b dataset.\n31. The system is evaluated on the BioASQ 36b dataset.\n32. The system is evaluated on the BioASQ 37b dataset.\n33. The system is evaluated on the BioASQ 38b dataset.\n34. The system is evaluated on the BioASQ 39b dataset.\n35. The system is evaluated on the BioASQ 40b dataset.\n36. The system is evaluated on the BioASQ 41b dataset.\n37. The system is evaluated on the BioASQ 42b dataset.\n38. The system is evaluated on the BioASQ 43b dataset.\n39. The system is evaluated on the BioASQ 44b dataset.\n40. The system is evaluated on the BioASQ 45b dataset.\n41. The system is evaluated on the BioASQ 46b dataset.\n42. The system is evaluated on the BioASQ 47b dataset.\n43. The system is evaluated on the BioASQ 48b dataset.\n44. The system is evaluated on the BioASQ 49b dataset.\n45. The system is evaluated on the BioASQ 50b dataset.\n46. The system is evaluated on the BioASQ 51b dataset.\n47. The system is evaluated on the BioASQ 52b dataset.\n48. The system is evaluated on the BioAS", "retrieved_text": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 11}, {"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 9}, {"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 7}], "retrieved_vision": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 11}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio/page_11.pt"]}
{"id": "qnamaker_data_to_bot_in_2_minutes_17", "question": "What components is the QnAMaker composed of?", "text_context": "[QnAMaker-_Data_to_Bot_in_2_Minutes p2]\nWWW ’20 Companion, April 20–24, 2020, Taipei, Taiwan\nTrovato and Tobin, et al.\nBot\nAzure Search \nIndex\nQnAMaker\nWebApp\nBot \nDeveloper\nQnAMaker\nPortal\nQnaMaker\nManagement\nAPIs\nEnd User\nQnAMaker Server\nQnAMaker Client Azure \nSubscription\nFigure 1: Interactions between various components of Qna-\nMaker, along with their scopes: server-side and client-side\ntheir data. QnAMaker is part of Microsoft Cognitive Service and\ncurrently runs using the Microsoft Azure Stack2.\n2\nSYSTEM DESCRIPTION\n2.1\nArchitecture\nAs shown in Figure 1, humans can have two different kinds of roles\nin the system: Bot-Developers who want to create a bot using\nthe data they have, and End-Users who will chat with the bot(s)\ncreated by bot-developers. The components involved in the process\nare:\n• QnAMaker Portal3: This is the Graphical User Interface (GUI)\nfor using QnAMaker. This website is designed to ease the\nuse of management APIs. It also provides a test pane.\n• QnaMaker Management APIs: This is used for the extraction\nof Question-Answer (QA) pairs from semi-structured content.\nIt then passes these QA pairs to the web app to create the\nKnowledge Base Index.\n• Azure Search Index: Stores the KB with questions and answers\nas indexable columns, thus acting as a retrieval layer.\n• QnaMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results. WebApp also handles feedback\nmanagement for active learning.\n• Bot: Calls the WebApp with the User’s query to get results.\n2.2\nBot Development Process\nCreating a bot is a 3-step process for a bot developer:\n(1) Create a QnaMaker Resource in Azure: This creates a We-\nbApp with binaries required to run QnAMaker. It also creates\nan Azure Search Service for populating the index with any\ngiven knowledge base, extracted from user data\n(2) Use Management APIs to Create/Update/Delete your KB:\nThe Create API automatically extracts the QA pairs and\n2https://azure.microsoft.com/en-us/services/cogni\n\n[QnAMaker-_Data_to_Bot_in_2_Minutes p1]\nQnAMaker: Data to Bot in 2 Minutes\nParag Agrawal\nMicrosoft Corporation\nparagag@microsoft.com\nTulasi Menon\nMicrosoft Corporation\ntulasim@microsoft.com\nAya Kamel\nMicrosoft Corporation\naykame@microsoft.com\nMichel Naim\nMicrosoft Corporation\nmgerguis@microsoft.com\nChaikesh Chouragade\nMicrosoft Corporation\nchchoura@microsoft.com\nGurvinder Singh\nMicrosoft Corporation\ngurvsing@microsoft.com\nRohan Kulkarni\nMicrosoft Corporation\nrokulka@microsoft.com\nAnshuman Suri\nMicrosoft Corporation\nansuri@microsoft.com\nSahithi Katakam\nMicrosoft Corporation\nsakataka@microsoft.com\nVineet Pratik\nMicrosoft Corporation\nvipratik@microsoft.com\nPrakul Bansal\nMicrosoft Corporation\nprabansa@microsoft.com\nSimerpreet Kaur\nMicrosoft Corporation\nsikaur@microsoft.com\nNeha Rajput\nMicrosoft Corporation\nnerajput@microsoft.com\nAnand Duggal\nMicrosoft Corporation\nanduggal@microsoft.com\nAchraf Chalabi\nMicrosoft Corporation\nachalabi@microsoft.com\nPrashant Choudhari\nMicrosoft Corporation\npchoudh@microsoft.com\nSomi Reddy Satti\nMicrosoft Corporation\nsosatti@microsoft.com\nNiranjan Nayak\nMicrosoft Corporation\nniranjan@microsoft.com\nABSTRACT\nHaving a bot for seamless conversations is a much-desired feature\nthat products and services today seek for their websites and mobile\napps. These bots help reduce traffic received by human support\nsignificantly by handling frequent and directly answerable known\nquestions. Many such services have huge reference documents such\nas FAQ pages, which makes it hard for users to browse through this\ndata. A conversation layer over such raw data can lower traffic to\nhuman support by a great margin. We demonstrate QnAMaker, a\nservice that creates a conversational layer over semi-structured data\nsuch as FAQ pages, product manuals, and support documents. QnA-\nMaker is the popular choice for Extraction and Question-Answering\nas a service and is used by over 15,000 bots in production. It is also\nused by search interfaces and not just bots.\nKEYWORDS\nChatBots, Democratizing AI, Question Answering\n\n[QnAMaker-_Data_to_Bot_in_2_Minutes p3]\nQnAMaker: Data to Bot in 2 Minutes\nWWW ’20 Companion, April 20–24, 2020, Taipei, Taiwan\nDomain\nNumber of QAs\nAvg Questions per QA\nAUC (%)\nF1\n(top answer)\nNavigation Help Bot\n56\n12.5\n88.7\n71.2\nChit-Chat Alone\n100\n9.8\n92.4\n88.6\nCustomerCare Interface\n164\n2.2\n90.9\n86.7\nHR Internal Bot\n52\n1.0\n85.5\n82.6\nHR Internal Bot (with Chit-Chat)\n152\n6.78\n82.7\n77.6\nTable 1: Retrieval And Ranking Measurements\nPre-\nProcessing\nWordNet \nFeatures\nRanking\nTF-IDF\nDSSM\nFeatures\nFeaturization\nSearch \nQuery\nFormulation\nAzure Search \nIndex\nQnAMaker WebApp\nBot API Call\nEnd-User \nQuery\nRanked \nCandidates\nFigure 2: QnAMaker Runtime Pipeline\nanswer. The scores of these WordNet features are calculated\nas a function of:\n• Distance of 2 words in the WordNet graph\n• Distance of Lowest Common Hypernym from the root\n• Knowledge-Base word importance (Local IDFs)\n• Global word importance (Global IDFs)\nThis is the most important feature in our model as it has the\nhighest relative feature gain.\n(2) CDSSM: Convolutional Deep Structured Semantic Models\n[6] are used for sentence-level semantic matching. This is\na dual encoder model that converts text strings (sentences,\nqueries, predicates, entity mentions, etc) into their vector\nrepresentations. These models are trained using millions of\nBing Query Title Click-Through data. Using the source-model\nfor vectorizing user query and target-model for vectorizing\nanswer, we compute the cosine similarity between these two\nvectors, giving the relevance of answer corresponding to the\nquery.\n(3) TF-IDF: Though sentence-to-vector models are trained on\nhuge datasets, they fail to effectively disambiguate KB spe-\ncific data. This is where a standard TF-IDF [7] featurizer with\nlocal and global IDFs helps.\n2.4.3\nContextual Features. We extend the features for contextual\nranking by modifying the candidate QAs and user query in these\nways:\n• Querymodif ie", "prediction_text_only": "The QnAMaker is composed of the following components:\n• QnAMaker Portal: This is the Graphical User Interface (GUI) for\nusing QnAMaker. This website is designed to ease the use of\nmanagement APIs. It also provides a test pane.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• QnAMaker Portal: This is the Graphical User Interface (GUI)\nfor using QnAMaker. This website is designed to ease the use of\nmanagement APIs. It also provides a test pane.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• Azure Search Index: Stores the KB with questions and answers\nas indexable columns, thus acting as a retrieval layer.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• Azure Search Index: Stores the KB with questions and answers\nas indexable columns, thus acting as a retrieval layer.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• Azure Search Index: Stores the KB with questions and answers\nas indexable columns, thus acting as a retrieval layer.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• Azure Search Index: Stores the KB with questions and answers\nas indexable columns, thus acting as a retrieval layer.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• Azure Search Index: Stores the KB with questions and answers\nas indexable columns, thus acting as a retrieval layer.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• Azure Search Index: Stores the KB with questions and answers\nas indexable columns, thus acting as a retrieval layer.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• Azure Search Index: Stores the KB with questions and answers\nas indexable columns, thus acting as a retrieval layer.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• Azure Search Index: Stores the KB with questions and answers\nas indexable columns, thus acting as a retrieval layer.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.\n• Azure Search Index: Stores the KB with questions and answers\nas indexable columns, thus acting as a retrieval layer.\n• QnAMaker WebApp: Acts as a layer between the Bot, Manage-\nment APIs, and Azure Search Index. WebApp does ranking\non top of retrieved results.", "prediction_vision_only": "The QnAMaker is composed of three main components: Bot, End User, and Azure Search Index.\n\nQuestion: What is the purpose of the Bot component?\nAnswer: The Bot component is used to interact with the user, provide them with information, and collect data.\n\nQuestion: What is the purpose of the End User component?\nAnswer: The End User component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the Azure Search Index component?\nAnswer: The Azure Search Index component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component is used to provide the user with information, collect data, and interact with the Bot component.\n\nQuestion: What is the purpose of the QnAMaker component?\nAnswer: The QnAMaker component", "prediction_text_plus_vision": "QnAMaker is a web app that creates a conversational layer over semi-structured data such as FAQ pages, product manuals, and support documents. It is used by human support by answering questions and providing answers to users.", "retrieved_text": [{"doc_id": "QnAMaker-_Data_to_Bot_in_2_Minutes", "page": 2}, {"doc_id": "QnAMaker-_Data_to_Bot_in_2_Minutes", "page": 1}, {"doc_id": "QnAMaker-_Data_to_Bot_in_2_Minutes", "page": 3}], "retrieved_vision": [{"doc_id": "QnAMaker-_Data_to_Bot_in_2_Minutes", "page": 2}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/QnAMaker-_Data_to_Bot_in_2_Minutes/page_2.pt"]}
{"id": "a_simple_discriminative_training_method_for_machine_translation_with_large-scale_20", "question": "What experiments with large-scale features are performed?", "text_context": "[Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections p4]\nFigure 1: Visualization of the simulations including base setting, down-sampling, varying spreads, adding outliers, and\nmultiple sub-clusters in 2-dimensional and 768-dimensional spaces.\n• Down-sampling: Down-sample the base cluster to be\n{90%, 80%, ..., 10%} of its original size. That is, cre-\nate Gaussian blobs with {9000, ..., 1000} data points;\n• Varying Spread: Generate Gaussian blobs with stan-\ndard deviations of each axis to be {2.0, 3.0, ..., 10.0};\n• Outliers: Add {50, 100, ..., 500} outlier data points,\ni.e., {0.5%, ..., 5%} of the original cluster size, ran-\ndomly on the surface with a ﬁxed norm or radius;\n• Multiple Sub-clusters:\nAlong the 1th-axis, with\n10, 000 data points in total, create {1, 2, ..., 10} clus-\nters with equal sample sizes but at increasing distance.\nFor each scenario, we simulate a cluster and compute\nthe characteristic metrics in both 2-dimensional and 768-\ndimensional spaces.\nFigure 1 visualizes each scenario\nby t-distributed Stochastic Neighbor Embedding (t-SNE)\n(Maaten and Hinton, 2008). The 768-dimensional simula-\ntions are visualized by down-projecting to 50 dimensions\nvia Principal Component Analysis (PCA) followed by t-\nSNE.\n4.2.\nSimulation Results\nFigure 2 summarizes calculated diversity metrics in the ﬁrst\nrow, density metrics in the second row, and homogeneity\nmetrics in the third row, for all simulation scenarios.\nThe diversity metric is robust as its values remain almost\nthe same to the down-sampling of an input cluster. This\nimplies the diversity metric has a desirable property that it\nis insensitive to the size of inputs. On the other hand, it\nshows a linear relationship to varying spreads. It is another\nintuitive property for a diversity metric that it grows linearly\nwith increasing dispersion or variance of input data. With\nmore outliers or more sub-clusters, the diversity metric can\nalso reﬂect the increasing dispersion of cluster distributions\nbut is less sensitive in high-dimensional spaces.\nFor the density metrics,\n\n[A_simple_discriminative_training_method_for_machine_translation_with_large-scale_features p1]\nA SIMPLE DISCRIMINATIVE TRAINING METHOD FOR MACHINE\nTRANSLATION WITH LARGE-SCALE FEATURES\nA PREPRINT\nTian Xia\nWright State University\nSummerRainET2008@gmail.com\nShaodan Zhai\nWright State University\nShaodanZhai@gmail.com\nShaojun Wang\nWright State University\nSWang.USA@gmail.com\nNovember 13, 2021\nABSTRACT\nMargin infused relaxed algorithms (MIRAs) dominate model tuning in statistical machine translation\nin the case of large scale features, but also they are famous for the complexity in implementation.\nWe introduce a new method, which regards an N-best list as a permutation and minimizes the\nPlackett-Luce loss of ground-truth permutations. Experiments with large-scale features demonstrate\nthat, the new method is more robust than MERT; though it is only matchable with MIRAs, it has a\ncomparatively advantage, easier to implement.\n1\nIntroduction\nSince Och [26] proposed minimum error rate training (MERT) to exactly optimize objective evaluation measures,\nMERT has become a standard model tuning technique in statistical machine translation (SMT). Though MERT performs\nbetter by improving its searching algorithm [23, 6, 13, 25], it does not work reasonably when there are lots of features1.\nAs a result, margin infused relaxed algorithms (MIRA) dominate in this case [24, 35, 10, 31, 7].\nIn SMT, MIRAs consider margin losses related to sentence-level BLEUs. However, since the BLEU is not decomposable\ninto each sentence, these MIRA algorithms use some heuristics to compute the exact losses, e.g., pseudo-document\n[10], and document-level loss [31].\nRecently, another successful work in large-scale feature tuning include force decoding based[36], classiﬁcation based\n[15].\nWe aim to provide a simpler tuning method for large-scale features than MIRAs. Out motivation derives from an\nobservation on MERT. As MERT considers the quality of only top1 hypothesis set, there might have more-than-one set\nof parameters, which have similar top1 performances in tuning, but have very different topN hyp\n\n[Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects p2]\n– optimization techniques to be used, model hyperparameter and training\nparameters.\nNot only the space of possibilities for each of these aspects is large, there are\nalso challenges in performing a consistent large-scale evaluation of the result-\ning embeddings [1]. This makes systematic experimentation of alternative word-\nembedding conﬁgurations extremely diﬃcult.\nIn this work, we make progress in trying to ﬁnd good combinations of some\nof the previous parameters. We focus speciﬁcally in the task of computing word\nembeddings for processing the Portuguese Twitter stream. User-generated con-\ntent (such as twitter messages) tends to be populated by words that are speciﬁc\nto the medium, and that are constantly being added by users. These dynamics\npose challenges to NLP systems, which have diﬃculties in dealing with out of\nvocabulary words. Therefore, learning a semantic representation for those words\ndirectly from the user-generated stream - and as the words arise - would allow\nus to keep up with the dynamics of the medium and reduce the cases for which\nwe have no information about the words.\nStarting from our own implementation of a neural word embedding model,\nwhich should be seen as a ﬂexible baseline model for further experimentation,\nour research tries to answer the following practical questions:\n– how large is the vocabulary the one can realistically embed given the level\nof resources that most organizations can aﬀord to buy and to manage (as\nopposed to large clusters of GPU’s only available to a few organizations)?\n– how much data, as a function of the size of the vocabulary we wish to embed,\nis enough for training meaningful embeddings?\n– how can we evaluate embedding", "prediction_text_only": "We perform experiments with large-scale features in the case of 768-dimensional space. We use the same hyperparameters as in MIRA, except for the number of clusters, which is 10 in this case. We use the same set of hyperparameters for the MIRAs, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in this case. We use the same set of hyperparameters for the MIRA, except for the number of iterations, which is 10 in", "prediction_vision_only": "The experiments are performed with large-scale features, such as the number of clusters, the number of features, and the number of samples.", "prediction_text_plus_vision": "We conduct experiments with 2-dimensional and 768-dimensional spaces. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same hyperparameters as in the previous section. We use the same training and evaluation metrics as in the previous section. We use the same data as in the previous section. We use the same", "retrieved_text": [{"doc_id": "Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections", "page": 4}, {"doc_id": "A_simple_discriminative_training_method_for_machine_translation_with_large-scale_features", "page": 1}, {"doc_id": "Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects", "page": 2}], "retrieved_vision": [{"doc_id": "Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections", "page": 4}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections/page_4.pt"]}
{"id": "improving_spoken_language_understanding_by_exploiting_asr_n-best_hypotheses_21", "question": "Which ASR system(s) is used in this work?", "text_context": "[Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses p5]\n7. REFERENCES\n[1] Gokhan Tur and Renato De Mori, Spoken language un-\nderstanding: Systems for extracting semantic informa-\ntion from speech, John Wiley & Sons, 2011.\n[2] Fuchun Peng,\nScott Roy,\nBen Shahshahani,\nand\nFranc¸oise Beaufays, “Search results based n-best hy-\npothesis rescoring with maximum entropy classiﬁca-\ntion,”\nin 2013 IEEE Workshop on Automatic Speech\nRecognition and Understanding. IEEE, 2013, pp. 422–\n427.\n[3] Preethi Jyothi, Leif Johnson, Ciprian Chelba, and Brian\nStrope,\n“Large-scale discriminative language model\nreranking for voice-search,”\nin Proceedings of the\nNAACL-HLT 2012 Workshop: Will We Ever Really Re-\nplace the N-gram Model? On the Future of Language\nModeling for HLT. Association for Computational Lin-\nguistics, 2012, pp. 41–49.\n[4] Eugene Charniak and Mark Johnson, “Coarse-to-ﬁne n-\nbest parsing and maxent discriminative reranking,” in\nProceedings of the 43rd annual meeting on association\nfor computational linguistics. Association for Computa-\ntional Linguistics, 2005, pp. 173–180.\n[5] Fabrizio Morbini, Kartik Audhkhasi, Ron Artstein,\nMaarten Van Segbroeck, Kenji Sagae, Panayiotis Geor-\ngiou, David R Traum, and Shri Narayanan, “A reranking\napproach for recognition and classiﬁcation of speech in-\nput in conversational dialogue systems,” in 2012 IEEE\nSpoken Language Technology Workshop (SLT). IEEE,\n2012, pp. 49–54.\n[6] Erinc¸ Dikici, Murat Semerci, Murat Sarac¸lar, and Ethem\nAlpaydin,\n“Classiﬁcation and ranking approaches to\ndiscriminative language modeling for asr,” IEEE Trans-\nactions on Audio, Speech, and Language Processing,\nvol. 21, no. 2, pp. 291–300, 2012.\n[7] Hasim Sak, Murat Saralar, and Tunga G¨ung¨or, “Dis-\ncriminative reranking of asr hypotheses with mor-\npholexical and n-best-list features,” 2011 IEEE Work-\nshop on Automatic Speech Recognition —& Under-\nstanding, pp. 202–207, 2011.\n[8] Has¸im Sak, Murat Saraclar, and Tunga G¨ung¨or, “On-\nthe-ﬂy lattice rescoring for real-time automatic speech\nrecognition,” in Eleventh annual c\n\n[Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses p1]\nIMPROVING SPOKEN LANGUAGE UNDERSTANDING BY EXPLOITING ASR N-BEST\nHYPOTHESES\nMingda Li⋆‡ , Weitong Ruan†, Xinyue Liu†, Luca Soldaini†, Wael Hamza†, Chengwei Su†\n⋆University of California, Los Angeles, USA\n† Amazon Alexa AI, USA\nABSTRACT\nIn a modern spoken language understanding (SLU) system,\nthe natural language understanding (NLU) module takes in-\nterpretations of a speech from the automatic speech recogni-\ntion (ASR) module as the input. The NLU module usually\nuses the ﬁrst best interpretation of a given speech in down-\nstream tasks such as domain and intent classiﬁcation. How-\never, the ASR module might misrecognize some speeches\nand the ﬁrst best interpretation could be erroneous and noisy.\nSolely relying on the ﬁrst best interpretation could make the\nperformance of downstream tasks non-optimal. To address\nthis issue, we introduce a series of simple yet efﬁcient mod-\nels for improving the understanding of semantics of the input\nspeeches by collectively exploiting the n-best speech inter-\npretations from the ASR module.\nIndex Terms— ASR n-best hypotheses integration, spo-\nken language understanding\n1. INTRODUCTION\nCurrently, voice-controlled smart devices are widely used in\nmultiple areas to fulﬁll various tasks, e.g. playing music, ac-\nquiring weather information and booking tickets. The SLU\nsystem employs several modules to enable the understanding\nof the semantics of the input speeches. When there is an in-\ncoming speech, the ASR module picks it up and attempts to\ntranscribe the speech. An ASR model could generate mul-\ntiple interpretations for most speeches, which can be ranked\nby their associated conﬁdence scores. Among the n-best hy-\npotheses, the top-1 hypothesis is usually transformed to the\nNLU module for downstream tasks such as domain classiﬁ-\ncation, intent classiﬁcation and named entity recognition (slot\ntagging). Multi-domain NLU modules are usually designed\nhierarchically [1]. For one incoming utterance, NLU mod-\nules will ﬁrstly classify the uttera\n\n[Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses p4]\nTable 5: Performance comparison for the subset (∼81%)\nwhere ASR ﬁrst best agrees with transcription.\nCategory\nModel\nRErr(%)\nBaseline\n0.00\nIntegration\nPoolingAvg\n3.56\nPoolingMax\n-0.38\nCombined Sentence\n4.50\nDirect\nSort by Score\n-8.269\nMajority Vote\n-3.19\nRerank (Oracle)\n0.00\nOracle\n0.00\nwith the transcription. When ASR 1st best agrees with tran-\nscription, the proposed integration models can also keep the\nperformance. Under that condition, we can still improve a lit-\ntle (3.56%) because, by introducing multiple ASR hypothe-\nses, we could have more information and when the transcrip-\ntion/ASR 1st best does not appear in the training set’s tran-\nscriptions, its n-bests list may have similar hypotheses in-\ncluded in the training set’s n-bests.\nThen, our integration\nmodel trained on n-best hypotheses as well has clue to pre-\ndict. The series of comparisons reveal that our approaches\nintegrating the hypotheses are robust to the ASR errors and\nwhenever the ASR model makes mistakes, we can outperform\nmore signiﬁcantly.\n4.4. Improvements on Different Domains and Different\nNumbers of Hypotheses\nAmong all the 23 domains, we choose 8 popular domains for\nfurther comparisons between the Baseline and the best model\nof Table 3, PoolingAvg. Fig. 4 exhibits the results. We could\nﬁnd the PoolingAvg consistently improves the accuracy for\nall 8 domains.\nIn the previous experiments, the number of utilized hy-\npotheses for each utterance during evaluation is ﬁve, which\nmeans we use the top 5 interpretations when the size of ASR\nrecognition list is not smaller than 5 and use all the interpre-\ntations otherwise. Changing the number of hypotheses while\nevaluation, Fig. 5 shows a monotonic increase with the ac-\ncess to more hypotheses for the Pooling", "prediction_text_only": "The ASR system used in this work is the one proposed in [1].", "prediction_vision_only": "The proposed system is based on a hybrid ASR system that combines a speech recognizer and an ASR recognizer. The speech recognizer is a 2D Convolutional Neural Network (CNN) that is trained on a large dataset of speech data. The ASR recognizer is a 3D Convolutional Neural Network (CNN) that is trained on a smaller dataset of speech data. The hybrid ASR system is trained on a combination of the two datasets.\n\nQuestion: What is the main contribution of this work?\nAnswer: The main contribution of this work is the development of a hybrid ASR system that combines a speech recognizer and an ASR recognizer. The speech recognizer is a 2D Convolutional Neural Network (CNN) that is trained on a large dataset of speech data. The ASR recognizer is a 3D Convolutional Neural Network (CNN) that is trained on a smaller dataset of speech data. The hybrid ASR system is trained on a combination of the two datasets.\n\nQuestion: What are the potential applications of this work?\nAnswer: The potential applications of this work are in speech recognition, ASR, and ASR-related tasks. For example, the hybrid ASR system could be used in ASR applications, such as voice search, transcription, and translation. The ASR system could also be used in ASR-related tasks, such as speech recognition, ASR, and ASR-related tasks.\n\nQuestion: What are the challenges and future directions of this work?\nAnswer: The challenges of this work are the development of a hybrid ASR system that combines a speech recognizer and an ASR recognizer. The speech recognizer is a 2D Convolutional Neural Network (CNN) that is trained on a large dataset of speech data. The ASR recognizer is a 3D Convolutional Neural Network (CNN) that is trained on a smaller dataset of speech data. The hybrid ASR system is trained on a combination of the two datasets. The future directions of this work are the development of a hybrid ASR system that combines a speech recognizer and an ASR recognizer. The speech recognizer is a 2D Convolutional Neural Network (CNN) that is trained on a large dataset of speech data. The ASR recognizer is a 3D Convolutional Neural Network (CNN) that is trained on a smaller dataset of speech data. The hybrid ASR system is trained on a combination of the two datasets.\n\nQuestion: What are the limitations of this work?\nAnswer: The limitations of this work are the development of a hybrid ASR system that combines a speech recognizer and an ASR recognizer. The speech recognizer is a 2D Convolutional Neural Network (CNN) that is trained on a large dataset of speech data. The ASR recognizer is a 3D Convolutional Neural Network (CNN) that is trained on a smaller dataset of speech data. The hybrid ASR system is trained on a combination of the two datasets. The future directions of this work are the development of a hybrid ASR system that combines a speech recognizer and an ASR recognizer. The speech recognizer is a 2D Convolutional Neural Network (CNN) that is trained on a large dataset of speech data. The ASR recognizer is a 3D Convolutional Neural Network (CNN) that is trained on a smaller dataset of speech data. The hybrid ASR system is trained on a combination of the two datasets.\n\nQuestion: What are the implications of this work?\nAnswer: The implications of this work are the development of a hybrid ASR system that combines a speech recognizer and an ASR recognizer. The speech recognizer is a 2", "prediction_text_plus_vision": "The baseline system is the ASR system proposed in [1].", "retrieved_text": [{"doc_id": "Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses", "page": 5}, {"doc_id": "Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses", "page": 1}, {"doc_id": "Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses", "page": 4}], "retrieved_vision": [{"doc_id": "Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses", "page": 5}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses/page_5.pt"]}
{"id": "dissim_a_discourse-aware_syntactic_text_simplification_frameworkfor_english_and__24", "question": "Is the semantic hierarchy representation used for any task?", "text_context": "[Enriching_BERT_with_Knowledge_Graph_Embeddings_for_Document_Classification p7]\ngle set of 343 labels for sub-task B. We expect\nthis to have negative impact on performance, be-\ncause it allows a scenario in which, for a par-\nticular book, we predict a label from the ﬁrst\nlevel and also a non-matching label from the sec-\nond level of the hierarchy. The example Coen-\nzym Q10 (Table 3) demonstrates this issue. While\nthe model correctly predicts the second level label\nGesundheit & Ern¨ahrung (health & diet), it misses\nthe corresponding ﬁrst level label Ratgeber (advi-\nsor). Given the model’s tendency to higher pre-\ncision rather than recall in sub-task B, as a post-\nprocessing step we may want to take the most de-\ntailed label (on the third level of the hierarchy) to\nbe correct and manually ﬁx the higher level labels\naccordingly. We leave this for future work and\nnote that we expect this to improve performance,\nbut it is hard to say by how much. We hypothesize\nthat an MLP with more and bigger layers could\nimprove the classiﬁcation performance. However,\nthis would increase the number of parameters to be\ntrained, and thus requires more training data (such\nas the book’s text itself, or a summary of it).\n1-9\n10-19\n20-29\n30-39\n40-49\n 50\nAvailable number of samples per label in training set\n0\n20\n40\n60\n80\n100\n120\nNumber of label classes\nFigure 3: In sub-task B for many low-hierarchical la-\nbels only a small number of training samples exist,\nmaking it more difﬁcult to predict the correct label.\n7\nConclusions and Future Work\nIn this paper we presented a way of enriching\nBERT with knowledge graph embeddings and ad-\nditional metadata. Exploiting the linked knowl-\nedge that underlies Wikidata improves perfor-\nmance for our task of document classiﬁcation.\nWith this approach we improve the standard BERT\nmodels by up to four percentage points in accu-\nracy.\nFurthermore, our results reveal that with\ntask-speciﬁc information such as author names\nand publication metadata improves the classiﬁ-\ncation task essentially compared a text-only ap-\nproach.\nEspecially, when\n\n[DisSim-_A_Discourse-Aware_Syntactic_Text_Simplification_Frameworkfor_English_and_German p2]\nFigure 1: DISSIM’s browser-based user interface. The simpliﬁed output is displayed in the form of a directed\ngraph where the split sentences are connected by arrows whose labels denote the semantic relationship that holds\nbetween a pair of simpliﬁed sentences and whose direction indicates their contextual hierarchy. The colors signal\ndifferent context layers. In that way, a semantic hierarchy of minimal, self-contained propositions is established.\nspeciﬁes (1) how to split up and rephrase the in-\nput into structurally simpliﬁed sentences and (2)\nhow to set up a semantic hierarchy between them.\nThey are recursively applied on a given source\nsentence in a top-down fashion. When no more\nrule matches, the algorithm stops and returns the\ngenerated discourse tree.\n2.1\nSplit into Minimal Propositions\nIn a ﬁrst step, source sentences that present a com-\nplex linguistic form are turned into clean, compact\nstructures by decomposing clausal and phrasal\ncomponents. For this purpose, the transformation\nrules encode both the splitting points and rephras-\ning procedure for reconstructing proper sentences.\n2.2\nEstablish a Semantic Hierarchy\nEach split will create two or more sentences with\na simpliﬁed syntax. To establish a semantic hier-\narchy between them, two subtasks are carried out:\nConstituency Type Classiﬁcation.\nFirst, we set\nup a contextual hierarchy between the split sen-\ntences by connecting them with information about\ntheir hierarchical level, similar to the concept of\nnuclearity in RST. For this purpose, we distinguish\ncore sentences (nuclei), which carry the key infor-\nmation of the input, from accompanying contex-\ntual sentences (satellites) that disclose additional\ninformation about it.\nTo differentiate between\nthose two types of constituents, the transformation\npatterns encode a simple syntax-based approach\nwhere subordinate clauses/phrases are classiﬁed as\ncontext sentences, while superordinate as well as\ncoordinate clauses/phrases are labelled as core.\nRhetorical\n\n[DisSim-_A_Discourse-Aware_Syntactic_Text_Simplification_Frameworkfor_English_and_German p3]\nInput sentence:\nA fluoroscopic study known as an upper gastrointestinal series is typically the next step in management,\nalthough if volvulus is suspected, caution with non water soluble contrast is mandatory as the usage of\nbarium can impede surgical revision and lead to increased post operative complications.\nSupervised-OIE (alone):\n(1) (A fluoroscopic study; known; as an upper gastrointestinal series)\n(2) (caution with non water soluble contrast; is; mandatory as the usage of barium)\n(3) (as the usage; of barium can impede; surgical revision and lead)\n(4) ( ; to increased; post operative complications)\nSupervised-OIE (using discourse-aware TS framework for preprocessing):\n(5) #1 0 (A fluoroscopic study; is; typically, the next step in management)\n(5a)\nL:ELABORATION\n#2\n(5b)\nL:CONTRAST\n#3\n(6) #2 1 (This; fluoroscopic study is known; as an upper gastrointestinal series)\n(7) #3 0 (Caution with non water soluble; is; mandatory)\n(7a)\nL:CONTRAST\n#1\n(7b)\nL:CONDITION\n#7\n(7c)\nL:BACKGROUND\n#4\n(7d)\nL:BACKGROUND\n#5\n(7e)\nL:BACKGROUND\n#6\n(8) #4 1 (The usage of barium; can impede; surgical revision)\n(8a)\nL:LIST\n#5\n(8b)\nL:LIST\n#6\n(9) #5 1 (The usage of barium; can lead; to increased post operative complications)\n(9a)\nL:LIST\n#4\n(9b)\nL:LIST\n#6\n(10) #6 1 (The usage of barium; to increased; post operative complications)\n(10a)\nL:LIST\n#4\n(10b)\nL:LIST\n#5\n(11) #7 1 (Volvulus; is suspected; )\nFigure 2: Comparison of the propositions extracted by Supervised-OIE (Stanovsky et al., 2018) with (5-11) and\nwithout (1-4) using our discourse-aware TS approach as a preprocessing step.\nsingle sentence. Alternatively, a ﬁle containing a\nsequence of sentences can be loaded. The result\nof the transformation process is either written to\nt", "prediction_text_only": "Yes, the semantic hierarchy is used for the task of document classification.\nFigure 3: In sub-task B for many low-hierarchical labels only a small number of training samples exist,\nmaking it more difﬁcult to predict the correct label.\n7\nConclusions and Future Work\nIn this paper we presented a way of enriching\nBERT with knowledge graph embeddings and additional metadata. Exploiting the linked knowl-\nedge that underlies Wikidata improves the classiﬁcation task essentially compared a text-only ap-\nproach.\nWe hope that this work will inspire further research in the area of semantic hierarchy representation for document classification.\n2.1\nSplit into Minimal Propositions\nIn a ﬁrst step, source sentences that present a com-\nplex linguistic form are turned into clean, compact\nstructures by decomposing clausal and phrasal\ncomponents. For this purpose, the transformation\nrules encode both the splitting points and rephras-\ning procedure for reconstructing proper sentences.\n2.2\nEstablish a Semantic Hierarchy\nEach split will create two or more sentences with\na simplified syntax. To establish a semantic hier-\narchy between them, two subtasks are carried out:\nConstituency Type Classiﬁcation.\nFirst, we set\nup a contextual hierarchy between the split sen-\ntences by connecting them with information about\ntheir hierarchical level, similar to the concept of\nnuclearity in RST. For this purpose, we distinguish\ncore sentences (nuclei), which carry the key infor-\nmation of the input, from accompanying contex-\ntual sentences (satellites) that disclose additional\ninformation about it.\n2.3\nEstablish a Semantic Hierarchy\nEach split will create two or more sentences with\na simplified syntax. To establish a semantic hier-\narchy between them, two subtasks are carried out:\nConstituency Type Classiﬁcation.\nFirst, we set\nup a contextual hierarchy between the split sen-\ntences by connecting them with information about\ntheir hierarchical level, similar to the concept of\nnuclearity in RST. For this purpose, we distinguish\ncore sentences (nuclei), which carry the key infor-\nmation of the input, from accompanying contex-\ntual sentences (satellites) that disclose additional\ninformation about it.\n2.4\nEstablish a Semantic Hierarchy\nEach split will create two or more sentences with\na simplified syntax. To establish a semantic hier-\narchy between them, two subtasks are carried out:\nConstituency Type Classiﬁcation.\nFirst, we set\nup a contextual hierarchy between the split sen-\ntences by connecting them with information about\ntheir hierarchical level, similar to the concept of\nnuclearity in RST. For this purpose, we distinguish\ncore sentences (nuclei), which carry the key infor-\nmation of the input, from accompanying contex-\ntual sentences (satellites) that disclose additional\ninformation about it.\n2.5\nEstablish a Semantic Hierarchy\nEach split will create two or more sentences with\na simplified syntax. To establish a semantic hier-\narchy between them, two subtasks are carried out:\nConstituency Type Classiﬁcation.\nFirst, we set\nup a contextual hierarchy between the split sen-\ntences by connecting them with information about\ntheir hierarchical level, similar to the concept of\nnuclearity in RST. For this purpose, we distinguish\ncore sentences (nuclei), which carry the key infor-\nmation of the input, from accompanying contex-\ntual sentences (satellites) that disclose additional\ninformation about it.\n2.6\nEstablish a Semantic Hierarchy\nEach split will create two or more sentences with\na simplified syntax. To establish a semantic hier-\narchy between them, two subtasks", "prediction_vision_only": "Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion: Is the semantic hierarchy representation used for any task?\nAnswer: Yes, it is used for the task of image classification.\n\nQuestion:", "prediction_text_plus_vision": "Yes, it is used for the task of discourse classification.", "retrieved_text": [{"doc_id": "Enriching_BERT_with_Knowledge_Graph_Embeddings_for_Document_Classification", "page": 7}, {"doc_id": "DisSim-_A_Discourse-Aware_Syntactic_Text_Simplification_Frameworkfor_English_and_German", "page": 2}, {"doc_id": "DisSim-_A_Discourse-Aware_Syntactic_Text_Simplification_Frameworkfor_English_and_German", "page": 3}], "retrieved_vision": [{"doc_id": "Enriching_BERT_with_Knowledge_Graph_Embeddings_for_Document_Classification", "page": 7}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Enriching_BERT_with_Knowledge_Graph_Embeddings_for_Document_Classification/page_7.pt"]}
{"id": "learning_word_embeddings_from_the_portuguese_twitter_stream_a_study_of_some_prac_29", "question": "What experimental results suggest that using less than 50% of the available training examples might result in overfitting?", "text_context": "[Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects p9]\ntrend. When using 100% we see no model overﬁtting. We can also observe that\nthe higher is |V | the higher are the absolute values of the loss sets. This is\nnot surprising because as the number of words to predict becomes higher the\nproblem will tend to become harder. Also, because we keep the dimensionality\nof the embedding space constant (64 dimensions), it becomes increasingly hard\nto represent and diﬀerentiate larger vocabularies in the same hyper-volume. We\nbelieve this is a specially valuable indication for future experiments and for\ndeciding the dimensionality of the ﬁnal embeddings to distribute.\nOn the right side of Figure 5 we show how the number of training (and vali-\ndation) examples aﬀects the loss. For a ﬁxed |V | = 32768 we varied the amount\nof data used for training from 25% to 100%. Three trends are apparent. As we\ntrain with more data, we obtain better validation losses. This was expected. The\nsecond trend is that by using less than 50% of the data available the model\ntends to overﬁt the data, as indicated by the consistent increase in the valida-\ntion loss after about 15 epochs (check dashed lines in right side of Figure 5).\nThis suggests that for the future we should not try any drastic reduction of the\ntraining data to save training time. Finally, when not overﬁtting, the validation\nloss seems to stabilize after around 20 epochs. We observed no phase-transition\neﬀects (the model seems simple enough for not showing that type of behavior).\nThis indicates we have a practical way of safely deciding when to stop training\nthe model.\n5.1\nIntrinsic Evaluation\nTable 3 presents results for the three diﬀerent tests described in Section 4. The\nﬁrst (expected) result is that the coverage metrics increase with the size of the\nvocabulary being embedded, i.e., |V |. Because the Word Equivalence test set was\nspeciﬁcally created for evaluating Twitter-based embedding, when embedding\n|V | = 32768 words we achieve almost 90% test coverage. On the other hand, for\nthe\n\n[Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects p8]\nTable 2. Overall statistics for 12 combinations of models learned varying |V | and\nvolume of training data. Results observed after 40 training epochs.\nEmbeddings # Training Data Tuples Avg secs/epoch Training loss Validation loss\n|V | = 2048\n561,786 (25% data)\n4\n3.2564\n3.5932\n|V | = 2048\n1,123,573 (50% data)\n9\n3.2234\n3.4474\n|V | = 2048\n1,685,359 (75% data)\n13\n3.2138\n3.3657\n|V | = 2048\n2,496,830 (100% data)\n18\n3.2075\n3.3074\n|V | = 8192\n1,375,794 (25% data)\n63\n3.6329\n4.286\n|V | = 8192\n2,751,588 (50% data)\n151\n3.6917\n4.0664\n|V | = 8192\n4,127,382 (75% data)\n187\n3.7019\n3.9323\n|V | = 8192\n6,114,640 (100% data)\n276\n3.7072\n3.8565\n|V | = 32768 2,452,402 (25% data)\n388\n3.7417\n5.2768\n|V | = 32768 4,904,806 (50% data)\n956\n3.9885\n4.8409\n|V | = 32768 7,357,209 (75% data)\n1418\n4.0649\n4.6\n|V | = 32768 10,899,570 (100% data) 2028\n4.107\n4.4491\nFig. 1. Continuous line represents loss in the training data while dashed line represents\nloss in the validation data. Left side: eﬀect of increasing |V | using 100% of training\ndata. Right side: eﬀect of varying the amount of training data used with |V | = 32768.\nThe average time per epoch increases ﬁrst with the size of the vocabulary to\nembed |V | (because the model will have more parameters), and then, for each\n|V |, with the volume of training data. Using our testbed (Section 4), the total\ntime of learning in our experiments varied from a minimum of 160 seconds, with\n|V | = 2048 and 25% of data, to a maximum of 22.5 hours, with |V | = 32768\nand using 100% of the training data available (extracted from 5M tweets). These\nnumbers give us an approximate ﬁgure of how time consuming it would be to\ntrain embeddings from the complete Twitter corpus we have, consisting of 300M\ntweets.\nWe now analyze the learning process itself. We plot the training set loss and\nvalidation set loss for the diﬀerent values of |V | (Figure 5 left) with 40 epochs\nand using all the available data. As expected, the loss is reducing after each\nepoch, with validation loss,\n\n[A_simple_discriminative_training_method_for_machine_translation_with_large-scale_features p1]\nA SIMPLE DISCRIMINATIVE TRAINING METHOD FOR MACHINE\nTRANSLATION WITH LARGE-SCALE FEATURES\nA PREPRINT\nTian Xia\nWright State University\nSummerRainET2008@gmail.com\nShaodan Zhai\nWright State University\nShaodanZhai@gmail.com\nShaojun Wang\nWright State University\nSWang.USA@gmail.com\nNovember 13, 2021\nABSTRACT\nMargin infused relaxed algorithms (MIRAs) dominate model tuning in statistical machine translation\nin the case of large scale features, but also they are famous for the complexity in implementation.\nWe introduce a new method, which regards an N-best list as a permutation and minimizes the\nPlackett-Luce loss of ground-truth permutations. Experiments with large-scale features demonstrate\nthat, the new method is more robust than MERT; though it is only matchable with MIRAs, it has a\ncomparatively advantage, easier to implement.\n1\nIntroduction\nSince Och [26] proposed minimum error rate training (MERT) to exactly optimize objective evaluation measures,\nMERT has become a standard model tuning technique in statistical machine translation (SMT). Though MERT performs\nbetter by improving its searching algorithm [23, 6, 13, 25], it does not work reasonably when there are lots of features1.\nAs a result, margin infused relaxed algorithms (MIRA) dominate in this case [24, 35, 10, 31, 7].\nIn SMT, MIRAs consider margin losses related to sentence-level BLEUs. However, since the BLEU is not decomposable\ninto each sentence, these MIRA algorithms use some heuristics to compute the exact losses, e.g., pseudo-document\n[10], and document-level loss [31].\nRecently, another successful work in large-scale feature tuning include force decoding based[36], classiﬁcation based\n[15].\nWe aim to provide a s", "prediction_text_only": "Yes, this is true.", "prediction_vision_only": "The results suggest that using less than 50% of the available training examples might result in overfitting. This is because the model is able to learn the underlying patterns in the data, but not the specific details of the data. This is likely due to the fact that the model is not able to generalize well to new data, and is therefore overfitting to the training data.", "prediction_text_plus_vision": "The results suggest that using less than 50% of the available training examples might result in overfitting. This is because the model is overfitting to the training data, which is not the case when using 100% of the available training examples.", "retrieved_text": [{"doc_id": "Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects", "page": 9}, {"doc_id": "Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects", "page": 8}, {"doc_id": "A_simple_discriminative_training_method_for_machine_translation_with_large-scale_features", "page": 1}], "retrieved_vision": [{"doc_id": "Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects", "page": 9}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects/page_9.pt"]}
{"id": "procedural_reasoning_networks_for_understanding_multimodal_procedures_30", "question": "What multimodality is available in the dataset?", "text_context": "[CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset p3]\nType\nSingle-domain goal\nMulti-domain goal\nDataset\nDSTC2 WOZ 2.0 Frames KVRET\nM2M\nMultiWOZ\nSchema\nCrossWOZ\nLanguage\nEN\nEN\nEN\nEN\nEN\nEN\nEN\nCN\nSpeakers\nH2M\nH2H\nH2H\nH2H\nM2M\nH2H\nM2M\nH2H\n# Domains\n1\n1\n1\n3\n2\n7\n16\n5\n# Dialogues\n1,612\n600\n1,369\n2,425\n1,500\n8,438\n16,142\n5,012\n# Turns\n23,354\n4,472\n19,986\n12,732\n14,796\n115,424\n329,964\n84,692\nAvg. domains\n1\n1\n1\n1\n1\n1.80\n1.84\n3.24\nAvg. turns\n14.5\n7.5\n14.6\n5.3\n9.9\n13.7\n20.4\n16.9\n# Slots\n8\n4\n61\n13\n14\n25\n214\n72\n# Values\n212\n99\n3,871\n1363\n138\n4,510\n14,139\n7,871\nTable 1: Comparison of CrossWOZ to other task-oriented corpora (training set). H2H, H2M, M2M represent\nhuman-to-human, human-to-machine, machine-to-machine respectively. The average numbers of domains and\nturns are for each dialogue.\nMultiWOZ\nCrossWOZ\nusr: I’m looking for a college type attraction.\nusr: 你好，可以帮我推荐一个评分是4.5分以上的景点吗？\n...\nHello, could you recommend an attraction with a rating of 4.5 or higher?\nusr: I would like to visit in town centre please. sys: 天安门城楼，簋街小吃和北京欢乐谷都是很不错的地方呢。\n...\nTiananmen, Gui Street, and Beijing Happy Valley are very nice places.\nusr: Can you ﬁnd an Indian restaurant for me\nusr: 我喜欢北京欢乐谷，你知道这个景点周边的酒店都是什么吗？\nthat is also in the town centre?\nI like Beijing Happy Valley. What hotels are around this attraction?\nSchema\nsys: 那可多了，有A酒店, B酒店, C酒店。\nusr: I want a hotel in San Diego and I want to\nThere are many, such as hotel A, hotel B, and hotel C.\ncheck out on Thursday next week.\nusr: 太好了，我正打算在景点附近找个酒店住宿呢，知道哪家评分\n...\n是4分以上，提供叫醒服务的不？\nusr: I need a one way ﬂight to go there.\nGreat! I am planning to ﬁnd a hotel to stay near the attraction. Which\none has a rating of 4 or higher and offers wake-up call service?\nTable 2:\nCross-domain dialog examples in MultiWOZ, Schema, and CrossWOZ. The value of cross-domain\nconstraints(bold) are underlined. Some turns are omitted to save space. Names of hotels are replaced by A,B,C for\nsimplicity. Cross-domain constraints are pre-speciﬁed in MultiWOZ and Schema, while determined dynamically\nin CrossWOZ. In CrossWOZ, the choice in\n\n[Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections p5]\nFigure 2: Diversity, density, and homogeneity metric values in each simulation scenario.\na multi-headed self-attention architecture that can produce\ndifferent representation vectors for the same token in vari-\nous sequences, i.e., contextual embeddings.\nWhen pretraining, BERT concatenates two sequences as\ninput, with special tokens [CLS], [SEP], [EOS] denot-\ning the start, separation, and end, respectively. BERT is\nthen pretrained on a large unlabeled corpus with objective-\nmasked language model (MLM), which randomly masks\nout tokens, and the model predicts the masked tokens. The\nother classiﬁcation task is next sentence prediction (NSP).\nNSP is to predict whether two sequences follow each other\nin the original text or not.\nIn this work, we use the pretrained BERTBASE which has\n12 layers (L), 12 self-attention heads (A), and 768 hid-\nden dimension (H) as the language embedding to com-\npute the proposed data metrics.\nThe off-the-shelf pre-\ntrained BERT is obtained from GluonNLP4. For each se-\nquence xi\n= (xi1, ..., xil) with length l, BERT takes\n[CLS], xi1, ..., xil, [EOS] as input and generates embed-\ndings {eCLS, ei1, ..., eil, eEOS} at the token level. To ob-\ntain the sequence representation, we use a mean pooling\nover token embeddings:\nei = (ei1 + ... + eil)\nl\n,\n(12)\nwhere ei ∈RH. A text collection {x1, ..., xm}, i.e., a set\nof token sequences, is then transformed into a group of H-\ndimensional vectors {e1, ..., em}.\n4https://gluon-nlp.mxnet.io/model_zoo/\nbert/index.html\nWe compute each metric as described previously, using\nthree BERT layers L1, L6, and L12 as the embedding\nspace, respectively. The calculated metric values are av-\neraged over layers for each class and averaged over classes\nweighted by class size as the ﬁnal value for a dataset.\n5.2.\nExperimental Setup\nIn the ﬁrst task, we use the SST-2 (Stanford Sentiment Tree-\nbank, version 2) dataset (Socher et al., 2013) to conduct\nsentiment analysis experiments. SST-2 is a sentence binary\nclassiﬁcation datase\n\n[CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset p1]\nCrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented\nDialogue Dataset\nQi Zhu1, Kaili Huang2, Zheng Zhang1, Xiaoyan Zhu1, Minlie Huang1∗\n1Dept. of Computer Science and Technology, 1Institute for Artiﬁcial Intelligence,\n1Beijing National Research Center for Information Science and Technology,\n2Dept. of Industrial Engineering,\nTsinghua University, Beijing, China\n{zhu-q18,hkl16,z-zhang15}@mails.tsinghua.edu.cn\n{zxy-dcs,aihuang}@tsinghua.edu.cn\nAbstract\nTo advance multi-domain (cross-domain)\ndialogue modeling as well as alleviate the\nshortage of Chinese task-oriented datasets,\nwe propose CrossWOZ, the ﬁrst large-scale\nChinese Cross-Domain Wizard-of-Oz task-\noriented dataset.\nIt contains 6K dialogue\nsessions and 102K utterances for 5 do-\nmains, including hotel, restaurant, attrac-\ntion, metro, and taxi. Moreover, the corpus\ncontains rich annotation of dialogue states\nand dialogue acts at both user and system\nsides.\nAbout 60% of the dialogues have\ncross-domain user goals that favor inter-\ndomain dependency and encourage natural\ntransition across domains in conversation.\nWe also provide a user simulator and sev-\neral benchmark models for pipelined task-\noriented dialogue systems, which will fa-\ncilitate researchers to compare and evaluate\ntheir models on this corpus. The large size\nand rich annotation of CrossWOZ make it\nsuitable to investigate a variety of tasks in\ncross-domain dialogue modeling, such as\ndialogue state tracking, policy learning, user\nsimulation, etc.\n1\nIntroduction\nRecently, there have been a variety of task-\noriented dialogue models thanks to the prosper-\nity of neural architectures (Yao et al., 2013; Wen\net al., 2015; Mrkši´c et al., 2017; Peng et al., 2017;\nLei et al., 2018; Gür et al., 2", "prediction_text_only": "The dataset contains a variety of multimodal data, such as text, images, and audio.", "prediction_vision_only": "The dataset contains a variety of multimodal data, including text, images, and audio. The dataset is used to train and evaluate models that can handle different types of data.", "prediction_text_plus_vision": "The dataset is a large-scale multi-domain task-oriented dialogue dataset, which contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, metro, and taxi. The dataset is used to evaluate the performance of multi-domain dialogue models.\n\nTable 1: Comparison of CrossWOZ to other task-oriented corpora (training set). H2H, H2M, M2M represent human-to-human, human-to-machine, machine-to-machine respectively. The average numbers of domains and turns are for each dialogue.\n\n|MultiWOZ|CrossWOZ|\n|---|---|\n|Type|Single-domain goal|\n|Dataset|DSTC2 Woz 2.0 Frames KVRET M2M|\n|Language|EN EN EN EN|\n|Speakers|H2M H2H H2H H2M|\n|# Domains|1 1 1 3 2|\n|# Dialogues|1,612 600 1,369 2,425 1,500|\n|# Turns|23,354 4,472 19,986 12,732 14,796|\n|Avg. domains|1 1 1 1 1|\n|Avg. turns|14.5 7.5 14.6 5.3 9.9|\n|# Slots|8 4 61 13 14|\n|# Values|212 99 3,871 1363 138|\n\n|MultiWOZ|Schema|CrossWOZ|\n|---|---|---|\n|Language|BERTBASE|CrossWOZ|\n|En|H2H|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M|H2M|\n|En|H2M", "retrieved_text": [{"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 3}, {"doc_id": "Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections", "page": 5}, {"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 1}], "retrieved_vision": [{"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 3}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset/page_3.pt"]}
{"id": "procedural_reasoning_networks_for_understanding_multimodal_procedures_32", "question": "How better is accuracy of new model compared to previously reported models?", "text_context": "[Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses p3]\nFig. 3: Integration of n-best hypotheses with two possible\nways: 1) concatenate hypothesized text and 2) concatenate\nhypothesis embedding.\nlast state of the forward and backward LSTM forms the out-\nput vector of BiLSTM (concatenation denoted as [, ]). Then,\nin Eqn. 2, the MLP module deﬁnes the probability of a spe-\nciﬁc tag (domain or intent) ˜t as the normalized activation (σ)\noutput after linear transformation of the output vector.\n3.2. Hypothesis Embedding Concatenation\nThe concatenation of hypothesized text leverages the n-best\nlist by transferring information among hypotheses in an em-\nbedding framework, BiLSTM. However, since all the layers\nhave access to both the preceding and subsequent informa-\ntion, the embedding among n-bests will inﬂuence each other,\nwhich confuses the embedding and makes the whole frame-\nwork sensitive to the noise in hypotheses.\nAs the second group of integration approaches, we de-\nvelop models, PoolingAvg/Max, on the concatenation of hy-\npothesis embedding, which isolate the embedding process\namong hypotheses and summarize the features by a pooling\nlayer. For each hypothesis (e.g., ith best in Eqn. 3 with j\npairs of bytes), we could get a sequence of hidden states from\nBiLSTM and obtain its ﬁnal output state by concatenating\nthe ﬁrst and last hidden state (houtputi in Eqn. 4). Then, we\nstack all the output states vertically as shown in Eqn. 5. Note\nthat in the real data, we will not always have a ﬁxed size of\nhypotheses list. For a list with r (< n) interpretations, we get\nthe embedding for each of them and pad with the embedding\nof the ﬁrst best hypothesis until a ﬁxed size n. When r ≥n,\nwe only stack the top n embeddings. We employ houtput1\nfor padding to enhance the inﬂuence of the top 1 hypothesis,\nwhich is more reliable. Finally, one uniﬁed representation\ncould be achieved via Pooling (Max/Avg pooling with n by\n1 sliding window and stride 1) on the concatenation and one\nscore could be produced per possible tag for the given task.\n\n[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p9]\nTitle Suppressed Due to Excessive Length\n9\nonly considers pairs of variable values within the same set. The revised metric\nis computed using Equation 2, where S is the list of diﬀerent sets.\nτ ′ =\nP\ni∈S [(number of concordant pairs)i −(number of discordant pairs)i]\nP\ni∈S [ni(ni −1)/2]\n(2)\nTable 6 shows the results of all correlation metrics. Overall, ROUGE-2 and\nROUGE-SU4 give similar correlation values but ROUGE-SU4 is marginally bet-\nter. Among precision, recall and F1, both precision and F1 are similar, but pre-\ncision gives a better correlation. Recall shows poor correlation, and virtually no\ncorrelation when using the revised Kendall measure. For reporting the evalua-\ntion of results, it will be therefore more useful to use precision or F1. However,\ngiven the small diﬀerence between precision and F1, and given that precision\nmay favour short summaries when used as a function to optimise in a machine\nlearning setting (e.g. using reinforcement learning), it may be best to use F1 as\nthe metric to optimise.\nTable 6. Correlation analysis of evaluation results\nMetric\nPearson\nSpearman\nKendall\nRevised Kendall\nROUGE-2 precision\n0.61\n0.78\n0.58\n0.73\nROUGE-2 recall\n0.41\n0.24\n0.16\n-0.01\nROUGE-2 F1\n0.62\n0.68\n0.49\n0.42\nROUGE-SU4 precision\n0.61\n0.79\n0.59\n0.74\nROUGE-SU4 recall\n0.40\n0.20\n0.13\n-0.02\nROUGE-SU4 F1\n0.63\n0.69\n0.50\n0.43\nFig. 3 shows the scatterplots of ROUGE-SU4 recall, precision and F1 with\nrespect to the average human evaluation4. We observe that the relation between\nROUGE and the human evaluations is not linear, and that Precision and F1\nhave a clear correlation.\n7\nSubmitted Runs\nTable 7 shows the results and details of the runs submitted to BioASQ. The table\nuses ROUGE-SU4 Recall since this is the metric available at the time of writing\nthis paper. However, note that, as explained in Section 6, these results might\ndiﬀer from the ﬁnal human evaluation results. Therefore we do not comment on\nthe results, other than observing that the “ﬁrst n” baseline produces the s\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p9]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n9\nTABLE 2: Overall statistics of the classiﬁcation datasets used in the experiments.\nDataset\nNum.\nclasses\nTrain/test\nsizes\nAnnotators\nsource\nNum. answers per\ninstance (± stddev.)\nMean annotators\naccuracy (± stddev.)\nMaj. vot.\naccuracy\n20 Newsgroups\n4\n11536/3846\nSimulated\n1.000 ± 0.000\n0.405 ± 0.182\n0.405\nReuters-21578\n8\n1800/5216\nMech. Turk\n3.007 ± 1.019\n0.568 ± 0.262\n0.710\nLabelMe\n8\n1000/1688\nMech. Turk\n2.547 ± 0.576\n0.692 ± 0.181\n0.769\n20\n30\n40\n50\n60\n70\n80\n90\n100\nnum. topics\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\ntestset accuracy\nBlei 2003 (mv)\nLDA+LogReg (mv)\nsLDA (mv)\nLDA+Raykar\nLDA+Rodrigues\nMA-sLDAc (batch)\nMA-sLDAc (svi)\nFig. 4: Average testset accuracy (over 5 runs; ± stddev.) of\nthe different approaches on the 20-Newsgroups data.\nthe proposed model outperforms all the baselines, being the\nsvi version the one that performs best.\nIn order to assess the computational advantages of the\nstochastic variational inference (svi) over the batch algo-\nrithm, the log marginal likelihood (or log evidence) was\nplotted against the number of iterations. Fig. 5 shows this\ncomparison. Not surprisingly, the svi version converges\nmuch faster to higher values of the log marginal likelihood\nwhen compared to the batch version, which reﬂects the\nefﬁciency of the svi algorithm.\n5.1.2\nAmazon Mechanical Turk\nIn order to validate the proposed classiﬁcation model in real\ncrowdsourcing settings, Amazon Mechanical Turk (AMT)\nwas used to obtain labels from multiple annotators for two\npopular datasets: Reuters-21578 [31] and LabelMe [32].\nThe Reuters-21578 is a collection of manually categorized\nnewswire stories with labels such as Acqu", "prediction_text_only": "The proposed model achieves higher accuracy than the previously reported models, as shown in Table 2.", "prediction_vision_only": "The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the data more effectively than the other models.\n\nQuestion: How better is accuracy of new model compared to previously reported models?\nAnswer: The new model achieves the highest accuracy, followed by the baseline model and then the PoolingAvg model. This suggests that the new model is able to capture the underlying patterns in the", "prediction_text_plus_vision": "The new model achieves the highest accuracy, followed by the baseline and the svi algorithm. The svi algorithm performs the best, followed by the baseline and the new model.", "retrieved_text": [{"doc_id": "Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses", "page": 3}, {"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 9}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 9}], "retrieved_vision": [{"doc_id": "Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses", "page": 3}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses/page_3.pt"]}
{"id": "active_learning_for_chinese_word_segmentation_in_medical_text_33", "question": "How does the scoring model work?", "text_context": "[Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications p11]\nThéo Trouillon, Johannes Welbl, Sebastian Riedel, Éric\nGaussier, and Guillaume Bouchard. Complex em-\nbeddings for simple link prediction. In International\nConference on Machine Learning (ICML), 2016.\nCunchao Tu, Han Liu, Zhiyuan Liu, and Maosong Sun.\nCane: Context-aware network embedding for rela-\ntion modeling. In Annual Meeting of the Association\nfor Computational Linguistics (ACL), 2017.\nQuan Wang, Zhendong Mao, Bin Wang, and Li Guo.\nKnowledge graph embedding:\nA survey of ap-\nproaches and applications.\nIEEE Transactions on\nKnowledge and Data Engineering, 2017.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\nChen. Knowledge graph embedding by translating\non hyperplanes. In AAAI Conference on Artiﬁcial\nIntelligence, 2014.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng.\nEmbedding entities and rela-\ntions for learning and inference in knowledge bases.\nInternational Conference on Learning Representa-\ntions (ICLR), 2015.\nZhao Zhang, Fuzhen Zhuang, Meng Qu, Fen Lin, and\nQing He. Knowledge graph embedding with hierar-\nchical relation structure. In Empirical Methods in\nNatural Language Processing (EMNLP), 2018.\nMengchen Zhao, Bo An, Yaodong Yu, Sulin Liu, and\nSinno Jialin Pan. Data poisoning attacks on multi-\ntask relationship learning. In AAAI Conference on\nArtiﬁcial Intelligence, 2018a.\nZhengli Zhao, Dheeru Dua, and Sameer Singh. Gener-\nating natural adversarial examples. In International\nConference on Learning Representations (ICLR),\n2018b.\nDaniel Zügner, Amir Akbarnejad, and Stephan Günne-\nmann. Adversarial attacks on neural networks for\ngraph data.\nIn ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining,\n2018.\nA\nAppendix\nWe approximate the change on the score of the\ntarget triple upon applying attacks other than the\n⟨s′, r′, o⟩ones. Since each relation appears many\ntimes in the training triples, we can assume that\napplying a single attack will not considerably af-\nfect the relations embeddings. As a result, we just\nneed to st\n\n[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p9]\nTitle Suppressed Due to Excessive Length\n9\nonly considers pairs of variable values within the same set. The revised metric\nis computed using Equation 2, where S is the list of diﬀerent sets.\nτ ′ =\nP\ni∈S [(number of concordant pairs)i −(number of discordant pairs)i]\nP\ni∈S [ni(ni −1)/2]\n(2)\nTable 6 shows the results of all correlation metrics. Overall, ROUGE-2 and\nROUGE-SU4 give similar correlation values but ROUGE-SU4 is marginally bet-\nter. Among precision, recall and F1, both precision and F1 are similar, but pre-\ncision gives a better correlation. Recall shows poor correlation, and virtually no\ncorrelation when using the revised Kendall measure. For reporting the evalua-\ntion of results, it will be therefore more useful to use precision or F1. However,\ngiven the small diﬀerence between precision and F1, and given that precision\nmay favour short summaries when used as a function to optimise in a machine\nlearning setting (e.g. using reinforcement learning), it may be best to use F1 as\nthe metric to optimise.\nTable 6. Correlation analysis of evaluation results\nMetric\nPearson\nSpearman\nKendall\nRevised Kendall\nROUGE-2 precision\n0.61\n0.78\n0.58\n0.73\nROUGE-2 recall\n0.41\n0.24\n0.16\n-0.01\nROUGE-2 F1\n0.62\n0.68\n0.49\n0.42\nROUGE-SU4 precision\n0.61\n0.79\n0.59\n0.74\nROUGE-SU4 recall\n0.40\n0.20\n0.13\n-0.02\nROUGE-SU4 F1\n0.63\n0.69\n0.50\n0.43\nFig. 3 shows the scatterplots of ROUGE-SU4 recall, precision and F1 with\nrespect to the average human evaluation4. We observe that the relation between\nROUGE and the human evaluations is not linear, and that Precision and F1\nhave a clear correlation.\n7\nSubmitted Runs\nTable 7 shows the results and details of the runs submitted to BioASQ. The table\nuses ROUGE-SU4 Recall since this is the metric available at the time of writing\nthis paper. However, note that, as explained in Section 6, these results might\ndiﬀer from the ﬁnal human evaluation results. Therefore we do not comment on\nthe results, other than observing that the “ﬁrst n” baseline produces the s\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p11]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n11\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(a) annotator 1\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(b) annotator 2\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(c) annotator 3\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(d) annotator 4\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(e) annotator 5\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(f) annotator 6\nFig. 10: True vs. estimated confusion matrix (cm) of 6 different workers of the Reuters-21578 dataset.\nlabeling styles and in the kind of mistakes they make, with\nseveral workers showing clear biases (e.g. workers 3 and 4),\nwhile others made mistakes more randomly (e.g. worker 1).\nNevertheless, the proposed is able to capture these patterns\ncorrectly and account for effect.\nTo gain further insights, Table 3 shows 4 example images\nfrom the LabelMe dataset, along with their true labels, the\nanswers provided by the different workers, the true label\ninferred by the proposed model and the likelihood of the\ndifferent possible answers given the true label for each\nannotator (πr\ncd,l fo", "prediction_text_only": "The scoring model uses a combination of\npredictions from different models to determine the\nscore of a given answer. The model takes into account\nthe context of the question, the context of the answer,\nand the context of the other answers to make its\npredictions. The model then uses these predictions\nto determine the score of the answer.", "prediction_vision_only": "The scoring model uses a score function to measure the similarity between two inputs. The score function is typically a neural network, which takes two inputs and outputs a score. The score function is trained on a dataset of inputs and outputs, and it learns to map the inputs to the scores. The scores are then used to determine the similarity between the inputs.\n\nQuestion: How does the scoring model work?\nAnswer: The scoring model uses a score function to measure the similarity between two inputs. The score function is typically a neural network, which takes two inputs and outputs a score. The score function is trained on a dataset of inputs and outputs, and it learns to map the inputs to the scores. The scores are then used to determine the similarity between the inputs.\n\nQuestion: How does the scoring model work?\nAnswer: The scoring model uses a score function to measure the similarity between two inputs. The score function is typically a neural network, which takes two inputs and outputs a score. The score function is trained on a dataset of inputs and outputs, and it learns to map the inputs to the scores. The scores are then used to determine the similarity between the inputs.\n\nQuestion: How does the scoring model work?\nAnswer: The scoring model uses a score function to measure the similarity between two inputs. The score function is typically a neural network, which takes two inputs and outputs a score. The score function is trained on a dataset of inputs and outputs, and it learns to map the inputs to the scores. The scores are then used to determine the similarity between the inputs.\n\nQuestion: How does the scoring model work?\nAnswer: The scoring model uses a score function to measure the similarity between two inputs. The score function is typically a neural network, which takes two inputs and outputs a score. The score function is trained on a dataset of inputs and outputs, and it learns to map the inputs to the scores. The scores are then used to determine the similarity between the inputs.\n\nQuestion: How does the scoring model work?\nAnswer: The scoring model uses a score function to measure the similarity between two inputs. The score function is typically a neural network, which takes two inputs and outputs a score. The score function is trained on a dataset of inputs and outputs, and it learns to map the inputs to the scores. The scores are then used to determine the similarity between the inputs.\n\nQuestion: How does the scoring model work?\nAnswer: The scoring model uses a score function to measure the similarity between two inputs. The score function is typically a neural network, which takes two inputs and outputs a score. The score function is trained on a dataset of inputs and outputs, and it learns to map the inputs to the scores. The scores are then used to determine the similarity between the inputs.\n\nQuestion: How does the scoring model work?\nAnswer: The scoring model uses a score function to measure the similarity between two inputs. The score function is typically a neural network, which takes two inputs and outputs a score. The score function is trained on a dataset of inputs and outputs, and it learns to map the inputs to the scores. The scores are then used to determine the similarity between the inputs.\n\nQuestion: How does the scoring model work?\nAnswer: The scoring model uses a score function to measure the similarity between two inputs. The score function is typically a neural network, which takes two inputs and outputs a score. The score function is trained on a dataset of inputs and outputs, and it learns to map the inputs to the scores. The scores are then used to determine the similarity between the inputs.\n\nQuestion: How does the scoring model work?\nAnswer: The scoring model uses a score function to measure the similarity between two inputs. The score function is typically a neural network, which takes two inputs and outputs a score. The score function is trained on", "prediction_text_plus_vision": "The scoring model uses a similarity measure to compare the embeddings of the attack and the target triple. The similarity measure is calculated by taking the cosine similarity between the embeddings of the attack and the target triple. The cosine similarity measure is then used to calculate the similarity between the attack and the target triple.", "retrieved_text": [{"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 11}, {"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 9}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 11}], "retrieved_vision": [{"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 11}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications/page_11.pt"]}
{"id": "active_learning_for_chinese_word_segmentation_in_medical_text_34", "question": "How does the active learning model work?", "text_context": "[Active_Learning_for_Chinese_Word_Segmentation_in_Medical_Text p3]\na diversity measurement among the instances to avoid duplicate annotation, so as to further re-\nduce the annotation efforts. \n2 \nRelated Work \nResearch on CWS has a long history and various methods have been proposed in the literature. \nBasically, these methods are mainly focus on two categories: unsupervised and supervised. \nUnsupervised methods aim to build a segmentation system without any lexicon or labelled data. \nThey often start from an empirical definition of a word and then use some statistical measures, \ne.g. mutual information (Sproat and Shih, 1990; Sun et al., 1998), to learn words from a large \nunlabelled data resource. Although these unsupervised methods can capture many strong words, \ntheir performance is often not high enough for the practical use. \nSupervised methods, such as HMM tagging (Xue, 2003), character-based classification (Wang et \nal., 2008) and morpheme-based lexical chunking (Fu et al., 2008), attempt to acquire a model \nbased on a dictionary or a labelled data set. Among them, character-based classification has \ndrawn most attention recently and been further implemented with sequence labelling algorithms \n(Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both in-\nvocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling ap-\nproach, many related studies make efforts to improve the performance by various means, such as \nusing more tags and features (Tang  et al., 2009; Zhao et al., 2006), employing word-based tag-\nging without tagging (Zhang and Clark, 2007), employing some joint models that combines a \ngenerative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov \nand semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features \n(Zhao and Kit, 2011). \nAlthough there are various studies CWS individually, there are few studies of active learning on \nCWS. One related work is about active learning on Japanese word segmenta\n\n[Active_Learning_for_Chinese_Word_Segmentation_in_Medical_Text p6]\nDuring training, we go through all the boundaries in the unlabeled data only once and the fre-\nquency of the surrounding bigram is updated serially as: \n1\n1\nk\nk\nc c\nf\n+ + =  \nWhere \n1\nk\nk\nc c + is the surrounding character bigram of current boundary\nkI . Meanwhile, the diver-\nsity of boundary \nkI  can be measured exactly by the frequency of its surrounding bigram: \n1\n(\n)\nk k\nDiv\nk\nc c\nI\nf\nφ\n+\n=\n \nIt is worth mentioning that above diversity measure is a dynamic one. It is possible that two \nboundaries with the same character bigram context, e.g., \n1\nA\nI\n and \n1\nBI\n in the above examples, are \nassigned with different diversity values during training. Specifically, the boundary with a first \nappearing bigram has the lowest diversity value while the boundaries appearing afterwards have \nhigher values and thus are not likely to be picked as the top informative ones. In this way, the \nduplicate-annotated words can be avoided to some extent. \nIn summary, uncertainty sampling with diversity (in short, uncertainty-diversity sampling) ranks \nthe boundaries according to the following formula: \n_\n(\n)\n(\n)\n(\n)\nUn\nDiv\nUn\nDiv\nk\nk\nk\nI\nI\nI\nφ\nφ\nφ\n=\n⋅\n \nThe lower the value is, the more informative the boundary is thought to be. Obviously, uncer-\ntainty-diversity sampling requires only O(N) in computational complexity. \nTherefore, active learning on CWS can be implemented in the following two ways: Uncertainty \nsampling: In each iteration, all the instances in the unlabeled data U are ranked according to their \nuncertainty values and top instances are selected for oracle labelling; Uncertainty-Diversity \nsampling: In each iteration, all the instances in the unlabeled data U are ranked according to their \nuncertainty-diversity values and top instances are selected for oracle labeling.  \n4 \nExperimentation \n4.1 \nExperimental Setting \nThe SIGHAN Bakeoff 2 dataset consists of four different corpora: PKU, MSR, CityU, and AS. \nBut we only report the performance on three of the corpora except\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p2]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n2\nof the multiple annotators as noisy versions of that latent\nground truth. We propose two different models, one for\nclassiﬁcation [13] and another for regression problems, thus\ncovering a very wide range of possible practical applica-\ntions, as we empirically demonstrate. Since the majority of\nthe tasks for which multiple annotators are used generally\ninvolve complex data such as text, images and video, by\ndeveloping a multi-annotator supervised topic model we\nare contributing with a powerful tool for learning predictive\nmodels of complex high-dimensional data from crowds.\nGiven that the increasing sizes of modern datasets can\npose a problem for obtaining human labels as well as\nfor Bayesian inference, we propose an efﬁcient stochastic\nvariational inference algorithm [14] that is able to scale\nto very large datasets. We empirically show, using both\nsimulated and real multiple-annotator labels obtained from\nAMT for popular text and image collections, that the pro-\nposed models are able to outperform other state-of-the-art\napproaches in both classiﬁcation and regression tasks. We\nfurther show the computational and predictive advantages\nof the stochastic variational inference algorithm over its\nbatch counterpart.\n2\nSTATE OF THE ART\n2.1\nSupervised topic models\nLatent Dirichlet allocation (LDA) soon proved to be a pow-\nerful tool for modeling documents [1] and images [2] by ex-\ntracting their underlying topics, where topics are probability\ndistributions across words, and each document is character-\nized by a probability distribution across topics. However,\nthe need to model the relationship between documents and\nlabels quickly gave rise to many supervised variants of\nLDA", "prediction_text_only": "The active learning model works by iteratively selecting the most informative instances from the unlabeled data. The model selects the instances that are most likely to be informative, and then uses these instances to improve the model's performance. The model is trained on the unlabeled data, and the model's performance is evaluated on the labeled data. The model is then updated with the labeled data, and the process is repeated until the model's performance is satisfactory.", "prediction_vision_only": "The active learning model works by iteratively selecting the most informative data points for labeling, which helps to improve the performance of the model over time. The model is trained on a dataset of labeled data, and it learns to select the most informative data points to label. This process is repeated until the model has been trained on a sufficient amount of data.", "prediction_text_plus_vision": "The active learning model works by iteratively selecting the most informative instances from the unlabeled data. It does this by first computing the uncertainty of each instance, which is a measure of how uncertain the model is about its prediction. The model then selects the instances that are most informative based on this uncertainty, and uses these instances to train the model. This process is repeated until the model has been trained on a sufficient number of instances.", "retrieved_text": [{"doc_id": "Active_Learning_for_Chinese_Word_Segmentation_in_Medical_Text", "page": 3}, {"doc_id": "Active_Learning_for_Chinese_Word_Segmentation_in_Medical_Text", "page": 6}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 2}], "retrieved_vision": [{"doc_id": "Active_Learning_for_Chinese_Word_Segmentation_in_Medical_Text", "page": 3}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Active_Learning_for_Chinese_Word_Segmentation_in_Medical_Text/page_3.pt"]}
{"id": "active_learning_for_chinese_word_segmentation_in_medical_text_35", "question": "Which neural network architectures are employed?", "text_context": "[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p6]\n6\nDiego Moll´a and Christopher Jones\nsentence\nquestion\nembedding\nmatrix\nword embeddings\nLSTM\nLSTM\nsentence embeddings\n×\nsimilarity\nrelu\nlinear\n(NNR)\nsigmoid\n(NNC)\nsentence position\nFig. 2. Architecture of the neural classiﬁcation and regression systems. A matrix of\npre-trained word embeddings (same pre-trained vectors as in Fig. 1) is used to ﬁnd the\nembeddings of the words of the input sentence and the question. Then, LSTM chains\nare used to generate sentence embeddings — the weights of the LSTM chains of input\nsentence and question are not shared. Then, the sentence position is concatenated\nto the sentence embedding and the similarity of sentence and question embeddings,\nimplemented as a product. A ﬁnal layer predicts the label of the sentence.\nis the fact that “NNC SU4 F1” outperforms the neural regressor. We have not\nexplored this further and we presume that the relatively good results are due\nto the fact that ROUGE values range between 0 and 1, which matches the full\nrange of probability values that can be returned by the sigmoid activation of the\nclassiﬁer ﬁnal layer.\nTable 3 also shows the standard deviation across the cross-validation folds.\nWhereas this standard deviation is fairly large compared with the diﬀerences\nin results, in general the results are compatible with the top part of the ta-\nble and prior work suggesting that classiﬁcation-based approaches improve over\nregression-based approaches.\n5\nReinforcement Learning\nWe also experiment with the use of reinforcement learning techniques. Again\nthese experiments are based on [7], who uses REINFORCE to train a global\npolicy. The policy predictor uses a simple feedforward network with a hidden\nlayer.\nThe results reported by [7] used ROUGE Recall and indicated no improve-\nment with respect to deep learning architectures. Human evaluation results are\npreferable over ROUGE but these were made available after the publication of\nthe paper. When comparing the ROUGE and human evaluation results (Ta-\nble 4), we o\n\n[Procedural_Reasoning_Networks_for_Understanding_Multimodal_Procedures p10]\nMohit Iyyer, Varun Manjunatha, Anupam Guha, Yoga-\nrshi Vyas, Jordan Boyd-Graber, Hal Daum´e III, and\nLarry Davis. 2017. The amazing mysteries of the\ngutter: Drawing inferences between panels in comic\nbook narratives. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nYangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin\nChoi, and Noah A Smith. 2017.\nDynamic Entity\nRepresentations in Neural Language Models.\nIn\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nRobin Jia and Percy Liang. 2017.\nAdversarial Ex-\namples for Evaluating Reading Comprehension Sys-\ntems.\nIn Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nSamira Ebrahimi Kahou, Adam Atkinson, Vincent\nMichalski, Akos Kadar, Adam Trischler, and Yoshua\nBengio. 2017.\nFigureQA: An Annotated Figure\nDataset for Visual Reasoning.\nIn Proceedings of\nthe International Conference on Learning Represen-\ntations Workshop (ICLR Workshop).\nAniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\nJonghyun Choi, Ali Farhadi, and Hannaneh Ha-\njishirzi. 2017.\nAre You Smarter Than A Sixth\nGrader? Textbook Question Answering for Multi-\nmodal Machine Comprehension. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nYoon Kim. 2014. Convolutional Neural Networks for\nSentence Classiﬁcation. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nJavier Marin, Aritro Biswas, Ferda Oﬂi, Nicholas\nHynes, Amaia Salvador, Yusuf Aytar, Ingmar We-\nber, and Antonio Torralba. 2018.\nRecipe1M: A\nDataset for Learning Cross-Modal Embeddings for\nCooking Recipes and Food Images. arXiv preprint\narXiv:1810.06553.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward\n\n[Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects p4]\nproduce embeddings that closely capture distributional similarity, so that words\nthat belong to the same semantic class, or which are synonyms and antonyms of\neach other, will be embedded in “close” regions of the embedding hyper-space.\nOur neural model is composed of the following layers:\n– a Input Word Embedding Layer, that maps each of the 4 input words\nrepresented by a 1-hot vectors with |V | dimensions (e.g. 32k) into a low\ndimension space (64 bits). The projections matrix - Winput - is shared across\nthe 4 inputs. This is not be the embedding matrix that we wish to produce.\n– a Merge Layer that concatenates the 4 previous embeddings into a sin-\ngle vector holding all the context information. The concatenation operation\nensures that the rest of the model has explicit information about the rela-\ntive position of the input words. Using an additive merge operation instead\nwould preserve information onlu about the presence of the words, not their\nsequence.\n– a Intermediate Context Embedding Dense Layer that maps the pre-\nceding representation of 4 words into a lower dimension space, still repre-\nsenting the entire context. We have ﬁxed this context representation to 64\ndimensions. This ultimately determines the dimension of the resulting em-\nbeddings. This intermediate layer is important from the point of view of\nperformance because it isolates the still relatively high-dimensional input\nspace (4 x 64 bits input word embeddings) from the very high-dimensional\noutput space.\n– a ﬁnal Output Dense Layer that maps the takes the previous 64-bit repre-\nsentation of the entire input context and produces a vector with the dimen-\nsionality of the word output space (|V | dimensions). This matrix", "prediction_text_only": "We use a combination of LSTM, RNN, and feedforward neural networks.\n[Table 4]", "prediction_vision_only": "The figure shows a comparison of different neural network architectures, including LSTM, RNN, and attention mechanisms. The LSTM architecture is shown to be the most effective in capturing long-term dependencies, while the attention mechanism is used to focus on important parts of the input sequence. The figure also shows that the LSTM architecture is more effective than the RNN architecture, which is known for its ability to handle sequential data. The attention mechanism is used to focus on important parts of the input sequence, which helps the model to better understand the context and generate more accurate predictions.\n\nQuestion: Which neural network architectures are employed?\nAnswer: The figure shows a comparison of different neural network architectures, including LSTM, RNN, and attention mechanisms. The LSTM architecture is shown to be the most effective in capturing long-term dependencies, while the attention mechanism is used to focus on important parts of the input sequence. The figure also shows that the LSTM architecture is more effective than the RNN architecture, which is known for its ability to handle sequential data. The attention mechanism is used to focus on important parts of the input sequence, which helps the model to better understand the context and generate more accurate predictions.\n\nQuestion: Which neural network architectures are employed?\nAnswer: The figure shows a comparison of different neural network architectures, including LSTM, RNN, and attention mechanisms. The LSTM architecture is shown to be the most effective in capturing long-term dependencies, while the attention mechanism is used to focus on important parts of the input sequence. The figure also shows that the LSTM architecture is more effective than the RNN architecture, which is known for its ability to handle sequential data. The attention mechanism is used to focus on important parts of the input sequence, which helps the model to better understand the context and generate more accurate predictions.\n\nQuestion: Which neural network architectures are employed?\nAnswer: The figure shows a comparison of different neural network architectures, including LSTM, RNN, and attention mechanisms. The LSTM architecture is shown to be the most effective in capturing long-term dependencies, while the attention mechanism is used to focus on important parts of the input sequence. The figure also shows that the LSTM architecture is more effective than the RNN architecture, which is known for its ability to handle sequential data. The attention mechanism is used to focus on important parts of the input sequence, which helps the model to better understand the context and generate more accurate predictions.\n\nQuestion: Which neural network architectures are employed?\nAnswer: The figure shows a comparison of different neural network architectures, including LSTM, RNN, and attention mechanisms. The LSTM architecture is shown to be the most effective in capturing long-term dependencies, while the attention mechanism is used to focus on important parts of the input sequence. The figure also shows that the LSTM architecture is more effective than the RNN architecture, which is known for its ability to handle sequential data. The attention mechanism is used to focus on important parts of the input sequence, which helps the model to better understand the context and generate more accurate predictions.\n\nQuestion: Which neural network architectures are employed?\nAnswer: The figure shows a comparison of different neural network architectures, including LSTM, RNN, and attention mechanisms. The LSTM architecture is shown to be the most effective in capturing long-term dependencies, while the attention mechanism is used to focus on important parts of the input sequence. The figure also shows that the LSTM architecture is more effective than the RNN architecture, which is known for its ability to handle sequential data. The attention mechanism is used to focus on important parts of the input sequence, which helps the model to better understand the context and generate more accurate predictions.\n\nQuestion: Which neural network architectures are employed?\nAnswer: The figure shows a comparison of different neural network architectures, including LSTM, RNN,", "prediction_text_plus_vision": "The neural network architectures that we use are: LSTM, CNN, and RNN.", "retrieved_text": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 6}, {"doc_id": "Procedural_Reasoning_Networks_for_Understanding_Multimodal_Procedures", "page": 10}, {"doc_id": "Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects", "page": 4}], "retrieved_vision": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 6}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio/page_6.pt"]}
{"id": "inscript_narrative_texts_annotated_with_script_information_38", "question": "How many subjects have been used to create the annotations?", "text_context": "[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p2]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n2\nof the multiple annotators as noisy versions of that latent\nground truth. We propose two different models, one for\nclassiﬁcation [13] and another for regression problems, thus\ncovering a very wide range of possible practical applica-\ntions, as we empirically demonstrate. Since the majority of\nthe tasks for which multiple annotators are used generally\ninvolve complex data such as text, images and video, by\ndeveloping a multi-annotator supervised topic model we\nare contributing with a powerful tool for learning predictive\nmodels of complex high-dimensional data from crowds.\nGiven that the increasing sizes of modern datasets can\npose a problem for obtaining human labels as well as\nfor Bayesian inference, we propose an efﬁcient stochastic\nvariational inference algorithm [14] that is able to scale\nto very large datasets. We empirically show, using both\nsimulated and real multiple-annotator labels obtained from\nAMT for popular text and image collections, that the pro-\nposed models are able to outperform other state-of-the-art\napproaches in both classiﬁcation and regression tasks. We\nfurther show the computational and predictive advantages\nof the stochastic variational inference algorithm over its\nbatch counterpart.\n2\nSTATE OF THE ART\n2.1\nSupervised topic models\nLatent Dirichlet allocation (LDA) soon proved to be a pow-\nerful tool for modeling documents [1] and images [2] by ex-\ntracting their underlying topics, where topics are probability\ndistributions across words, and each document is character-\nized by a probability distribution across topics. However,\nthe need to model the relationship between documents and\nlabels quickly gave rise to many supervised variants of\nLDA. One of the ﬁrst notable works was that of supervised\nLDA (sLDA) [7]. By extending LDA through the inclusion of\na response variable that is linearly dependent on the mean\ntopic-assignments of the words in a document, sLD\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p12]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n12\nTABLE 3: Results for 4 example LabelMe images.\nhighway\ninsidecity\ntallbuilding\nstreet\nforest\ncoast\nmountain\nopencountry\nAnnotator 43\nAnnotator 32\nAnnotator 22\nLik. of different answers given true class\nTrue label: street\nAnnotator 22: street\nAnnotator 32: inside city\nInferred ground truth: street\nAnnotator 43: inside city\nhighway\ninsidecity\ntallbuilding\nstreet\nforest\ncoast\nmountain\nopencountry\nAnnotator 43\nAnnotator 19\nAnnotator 9\nLik. of different answers given true class\nTrue label: tall building\nAnnotator 9: tall building\nAnnotator 19: street\nInferred ground truth: tall building\nAnnotator 43: inside city\nhighway\ninsidecity\ntallbuilding\nstreet\nforest\ncoast\nmountain\nopencountry\nAnnotator 27\nAnnotator 20\nAnnotator 13\nLik. of different answers given true class\nTrue label: inside city\nAnnotator 13: inside city\nAnnotator 20: tall building\nInferred ground truth: tall building\nAnnotator 27: tall building\nhighway\ninsidecity\ntallbuilding\nstreet\nforest\ncoast\nmountain\nopencountry\nAnnotator 21\nAnnotator 18\nAnnotator 15\nLik. of different answers given true class\nTrue label: street\nAnnotator 15: inside city\nAnnotator 18: inside city\nInferred ground truth: street\nAnnotator 21: inside city\n• sLDA (mean): This corresponds to using the regression\nversion of sLDA [7] with the target variables obtained\nby computing the mean of the annotators’ answers.\nFig. 11 shows the results obtained for different numbers\nof topics. Do to the stochastic nature of both the annotators\nsimulation procedure and the initialization of the variational\nBayesian EM algorithm, we repeated each experiment 30\ntimes and report the average R2 obtained with the corre-\nsponding standard deviation. Since the regression datasets\nthat are considered in this article are not large enough to\njustify the use of a stochastic variational inference (svi) algo-\nrithm, we only made experiments using the batch algorithm\ndeveloped in Section\n\n[Marrying_Universal_Dependencies_and_Universal_Morphology p4]\nSchema\nAnnotation\nUD\nVERB\nMOOD=IND|NUMBER=SING|PERSON=3|TENSE=IMP|VERBFORM=FIN\nUniMorph\nV;IND;PST;1;SG;IPFV\nV;IND;PST;3;SG;IPFV\nTable 2: Attested annotations for the Spanish verb form “mandaba” “I/he/she/it commanded”. Note that UD\nseparates the part of speech from the remainder of the morphosyntactic description. In each schema, order of the\nvalues is irrelevant.\n3.3\nSimilarities in the annotation\nWhile the two schemata annotate different features,\ntheir annotations often look largely similar. Con-\nsider the attested annotation of the Spanish word\n“mandaba” “(I/he/she/it) commanded”. Table 2\nshows that these annotations share many attributes.\nSome conversions are straightforward: VERB\nto V, MOOD=IND to IND, NUMBER=SING to SG,\nand PERSON=3 to 3.6 One might also suggest\nmapping TENSE=IMP to IPFV, though this crosses\nsemantic categories: IPFV represents the imper-\nfective aspect, whereas TENSE=IMP comes from\nimperfect, the English name often given to Span-\nish’s pasado continuo form. The imperfect is a\nverb form which combines both past tense and im-\nperfective aspect. UniMorph chooses to split this\ninto the atoms PST and IPFV, while UD uniﬁes\nthem according to the familiar name of the tense.\n4\nUD treebanks and UniMorph tables\nPrima facie, the alignment task may seem trivial.\nBut we’ve yet to explore the humans in the loop.\nThis conversion is a hard problem because we’re\noperating on idealized schemata. We’re actually\nannotating human decisions—and human mistakes.\nIf both schemata were perfectly applied, their over-\nlapping attributes could be mapped to each other\nsimply, in a cross-lingual and totally general way.\nUnfortunately, the resources are imperfect realiza-\ntions of their schemata. The cross-lingual, cross-\nresource, and within-r", "prediction_text_only": "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256,", "prediction_vision_only": "1,000 subjects were used to create the annotations.", "prediction_text_plus_vision": "1,000 subjects were used to create the annotations.", "retrieved_text": [{"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 2}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 12}, {"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 4}], "retrieved_vision": [{"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 2}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds/page_2.pt"]}
{"id": "investigating_robustness_and_interpretability_of_link_prediction_via_adversarial_40", "question": "How is this approach used to detect incorrect facts?", "text_context": "[Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications p1]\nInvestigating Robustness and Interpretability of Link Prediction\nvia Adversarial Modiﬁcations\nPouya Pezeshkpour\nUniversity of California\nIrvine, CA\npezeshkp@uci.edu\nYifan Tian\nUniversity of California\nIrvine, CA\nyifant@uci.edu\nSameer Singh\nUniversity of California\nIrvine, CA\nsameer@uci.edu\nAbstract\nRepresenting entities and relations in an em-\nbedding space is a well-studied approach for\nmachine learning on relational data. Existing\napproaches, however, primarily focus on im-\nproving accuracy and overlook other aspects\nsuch as robustness and interpretability.\nIn\nthis paper, we propose adversarial modiﬁca-\ntions for link prediction models: identifying\nthe fact to add into or remove from the knowl-\nedge graph that changes the prediction for a\ntarget fact after the model is retrained.\nUs-\ning these single modiﬁcations of the graph,\nwe identify the most inﬂuential fact for a pre-\ndicted link and evaluate the sensitivity of the\nmodel to the addition of fake facts.\nWe in-\ntroduce an efﬁcient approach to estimate the\neffect of such modiﬁcations by approximating\nthe change in the embeddings when the knowl-\nedge graph changes. To avoid the combinato-\nrial search over all possible facts, we train a\nnetwork to decode embeddings to their corre-\nsponding graph components, allowing the use\nof gradient-based optimization to identify the\nadversarial modiﬁcation. We use these tech-\nniques to evaluate the robustness of link predic-\ntion models (by measuring sensitivity to addi-\ntional facts), study interpretability through the\nfacts most responsible for predictions (by iden-\ntifying the most inﬂuential neighbors), and de-\ntect incorrect facts in the knowledge base.\n1\nIntroduction\nKnowledge graphs (KG) play a critical role in many\nreal-world applications such as search, structured\ndata management, recommendations, and question\nanswering. Since KGs often suffer from incom-\npleteness and noise in their facts (links), a number\nof recent techniques have proposed models that em-\nbed each e\n\n[Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications p8]\nMethods\n⟨s′, r′, o⟩Noise\n⟨s′, r, o⟩Noise\nHits@1\nHits@2\nHits@1\nHits@2\nRandom\n19.7\n39.4\n19.7\n39.4\nLowest\n16\n37\n26\n47\nCRIAGE\n42\n62\n55\n76\nTable 6: Error Detection Accuracy in the neighbor-\nhood of 100 chosen samples. We choose the neighbor\nwith the least value of ∆(s′,r′)(s, r, o) as the incorrect\nfact. This experiment assumes we know each target\nfact has exactly one error.\nﬁnd simple patterns on subgraphs that surround the\ntarget triple and the removed fact from CRIAGE-\nRemove, and appear more than 90% of the time.\nWe only focus on extracting length-2 horn rules,\ni.e., R1(a, c) ∧R2(c, b) ⇒R(a, b), where R(a, b)\nis the target and R2(c, b) is the removed fact.\nTable 5 shows extracted YAGO3-10 rules that\nare common to both models, and ones that are not.\nThe rules show several interesting inferences, such\nthat hasChild is often inferred via married parents,\nand isLocatedIn via transitivity. There are several\ndifferences in how the models reason as well; Dist-\nMult often uses the hasCapital as an intermedi-\nate step for isLocatedIn, while ConvE incorrectly\nuses isNeighbor. We also compare against rules\nextracted by Yang et al. [2015] for YAGO3-10 that\nutilizes the structure of DistMult: they require do-\nmain knowledge on types and cannot be applied to\nConvE. Interestingly, the extracted rules contain all\nthe rules provided by CRIAGE, demonstrating that\nCRIAGE can be used to accurately interpret mod-\nels, including ones that are not interpretable, such\nas ConvE. These are preliminary steps toward inter-\npretability of link prediction models, and we leave\nmore analysis of interpretability to future work.\n6.4\nFinding Errors in Knowledge Graphs\nHere, we demonstrate another potential use of ad-\nversarial modiﬁcations: ﬁnding erroneous triples in\nthe knowledge graph. Intuitively, if there is an error\nin the graph, the triple is likely to be inconsistent\nwith its neighborhood, and thus the model should\nput least trust on this triple. In other words, the\nerror triple should have th\n\n[Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications p6]\nMethods\nNations\nKinship\nAdding\nRemoving\nAdding\nRemoving\nρ\nτ\nρ\nτ\nρ\nτ\nρ\nτ\nRanking Based on Score\n0.03\n0.02\n-0.01\n-0.01\n-0.09\n-0.06\n0.01\n0.01\nInﬂuence Function without Hessian\n0.15\n0.12\n0.12\n0.1\n0.77\n0.71\n0.77\n0.71\nCRIAGE (Brute Force)\n0.95\n0.84\n0.94\n0.85\n0.99\n0.97\n0.99\n0.95\nInﬂuence Function\n0.99\n0.95\n0.99\n0.96\n0.99\n0.98\n0.99\n0.98\nTable 3: Ranking modiﬁcations by their impact on the target. We compare the true ranking of candidate triples\nwith a number of approximations using ranking correlation coefﬁcients. We compare our method with inﬂuence\nfunction (IF) with and without Hessian, and ranking the candidates based on their score, on two KGs (d = 10,\naveraged over 10 random targets). For the sake of brevity, we represent the Spearman’s ρ and Kendall’s τ rank\ncorrelation coefﬁcients simply as ρ and τ.\nthe largest impact when removed). Similarly, we\nevaluate CRIAGE-Add by considering 200 random\ntriples that share the object entity with the target\nsample as candidates, and rank them as above.\nThe average results of Spearman’s ρ and\nKendall’s τ rank correlation coefﬁcients over 10\nrandom target samples is provided in Table 3.\nCRIAGE performs comparably to the inﬂuence\nfunction, conﬁrming that our approximation is ac-\ncurate. Inﬂuence function is slightly more accurate\nbecause they use the complete Hessian matrix over\nall the parameters, while we only approximate the\nchange by calculating the Hessian over eo. The\neffect of this difference on scalability is dramatic,\nconstraining IF to very small graphs and small em-\nbedding dimensionality (d ≤10) before we run\nout of memory. In Figure 3, we show the time\nto compute a single adversary by IF compared to\nCRIAGE, as we steadily ", "prediction_text_only": "This approach is used to detect incorrect facts by identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained.", "prediction_vision_only": "The approach uses a combination of link prediction and adversarial modification to detect incorrect facts. The link prediction model is trained on a dataset of links, and the adversarial modification model is trained on a dataset of incorrect facts. The model is then used to predict the correctness of links, and the model is used to detect incorrect facts.\n\nThe approach is evaluated on two datasets: the WebKB dataset and the WebKB-2 dataset. The results show that the approach is effective in detecting incorrect facts.\n\n## 1 Introduction\n\nKnowledge graphs (KG) play a critical role in many real-world applications such as search, structured data management, recommendations, and question answering. Since KGs often suffer from incompleteness and noise in their facts (links), a number of recent techniques have proposed models that embed each entity and relation into a vector space, and use these embeddings to predict facts. These dense representation models for link prediction include\n\nFigure 1: Knowledge graph embedding.", "prediction_text_plus_vision": "The approach is used to detect incorrect facts by identifying the most influential facts in the KG. This is done by calculating the Spearman’s ρ and Kendall’s τ rank correlation coefﬁcients over 10 random target samples. The results show that the approach is effective in detecting incorrect facts, with an average Spearman’s ρ of 0.99 and Kendall’s τ of 0.99.", "retrieved_text": [{"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 1}, {"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 8}, {"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 6}], "retrieved_vision": [{"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 1}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications/page_1.pt"]}
{"id": "investigating_robustness_and_interpretability_of_link_prediction_via_adversarial_41", "question": "Can this adversarial approach be used to directly improve model accuracy?", "text_context": "[Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications p8]\nMethods\n⟨s′, r′, o⟩Noise\n⟨s′, r, o⟩Noise\nHits@1\nHits@2\nHits@1\nHits@2\nRandom\n19.7\n39.4\n19.7\n39.4\nLowest\n16\n37\n26\n47\nCRIAGE\n42\n62\n55\n76\nTable 6: Error Detection Accuracy in the neighbor-\nhood of 100 chosen samples. We choose the neighbor\nwith the least value of ∆(s′,r′)(s, r, o) as the incorrect\nfact. This experiment assumes we know each target\nfact has exactly one error.\nﬁnd simple patterns on subgraphs that surround the\ntarget triple and the removed fact from CRIAGE-\nRemove, and appear more than 90% of the time.\nWe only focus on extracting length-2 horn rules,\ni.e., R1(a, c) ∧R2(c, b) ⇒R(a, b), where R(a, b)\nis the target and R2(c, b) is the removed fact.\nTable 5 shows extracted YAGO3-10 rules that\nare common to both models, and ones that are not.\nThe rules show several interesting inferences, such\nthat hasChild is often inferred via married parents,\nand isLocatedIn via transitivity. There are several\ndifferences in how the models reason as well; Dist-\nMult often uses the hasCapital as an intermedi-\nate step for isLocatedIn, while ConvE incorrectly\nuses isNeighbor. We also compare against rules\nextracted by Yang et al. [2015] for YAGO3-10 that\nutilizes the structure of DistMult: they require do-\nmain knowledge on types and cannot be applied to\nConvE. Interestingly, the extracted rules contain all\nthe rules provided by CRIAGE, demonstrating that\nCRIAGE can be used to accurately interpret mod-\nels, including ones that are not interpretable, such\nas ConvE. These are preliminary steps toward inter-\npretability of link prediction models, and we leave\nmore analysis of interpretability to future work.\n6.4\nFinding Errors in Knowledge Graphs\nHere, we demonstrate another potential use of ad-\nversarial modiﬁcations: ﬁnding erroneous triples in\nthe knowledge graph. Intuitively, if there is an error\nin the graph, the triple is likely to be inconsistent\nwith its neighborhood, and thus the model should\nput least trust on this triple. In other words, the\nerror triple should have th\n\n[Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications p3]\n3\nCompletion Robustness and\nInterpretability via Adversarial Graph\nEdits (CRIAGE)\nFor adversarial modiﬁcations on KGs, we ﬁrst de-\nﬁne the space of possible modiﬁcations. For a tar-\nget triple ⟨s, r, o⟩, we constrain the possible triples\nthat we can remove (or inject) to be in the form\nof ⟨s′, r′, o⟩i.e s′ and r′ may be different from the\ntarget, but the object is not. We analyze other forms\nof modiﬁcations such as ⟨s, r′, o′⟩and ⟨s, r′, o⟩in\nappendices A.1 and A.2, and leave empirical evalu-\nation of these modiﬁcations for future work.\n3.1\nRemoving a fact (CRIAGE-Remove)\nFor explaining a target prediction, we are inter-\nested in identifying the observed fact that has the\nmost inﬂuence (according to the model) on the pre-\ndiction. We deﬁne inﬂuence of an observed fact\non the prediction as the change in the prediction\nscore if the observed fact was not present when\nthe embeddings were learned. Previous work have\nused this concept of inﬂuence similarly for sev-\neral different tasks [Kononenko et al., 2010, Koh\nand Liang, 2017]. Formally, for the target triple\n⟨s, r, o⟩and observed graph G, we want to identify\na neighboring triple ⟨s′, r′, o⟩∈G such that the\nscore ψ(s, r, o) when trained on G and the score\nψ(s, r, o) when trained on G−{⟨s′, r′, o⟩} are max-\nimally different, i.e.\nargmax\n(s′,r′)∈Nei(o)\n∆(s′,r′)(s, r, o)\n(2)\nwhere ∆(s′,r′)(s, r, o) = ψ(s, r, o)−ψ(s, r, o), and\nNei(o) = {(s′, r′)|⟨s′, r′, o⟩∈G}.\n3.2\nAdding a new fact (CRIAGE-Add)\nWe are also interested in investigating the robust-\nness of models, i.e., how sensitive are the predic-\ntions to small additions to the knowledge graph.\nSpeciﬁcally, for a target prediction ⟨s, r, o⟩, we\nare interested in identifying a single fake fact\n⟨s′, r′, o⟩that, when added to the knowledge graph\nG, changes the prediction score ψ(s, r, o) the most.\nUsing ψ(s, r, o) as the score after training on\nG ∪{⟨s′, r′, o⟩}, we deﬁne the adversary as:\nargmax\n(s′,r′)\n∆(s′,r′)(s, r, o)\n(3)\nwhere ∆(s′,r′)(s, r, o) = ψ(s, r, o) −ψ(s, r, o)\n\n[Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications p2]\nFerdinand\nMaria\nPrincess\nHenriette\nViolante\nBavaria\nisMarried\nhasChild\nhasChild\ntarget prediction\n⟨s, r, o⟩\n(a) KG, with the target prediction\nFerdinand\nMaria\nPrincess\nHenriette\nViolante\nBavaria\nA.S.D.\nAstrea\nisMarried\nhasChild\n⟨s′, r′, o⟩\nremoved\nhasChild\n(b) After removing a fact\nFerdinand\nMaria\nPrincess\nHenriette\nViolante\nBavaria\nNew\nYork\nAl Jazira\nClub\nisMarried\nhasChild\nplaysFor\n⟨s′, r′, o⟩\nadded\nhasChild\n(c) After adding a fact\nFigure 1: Completion Robustness and Interpretability via Adversarial Graph Edits (CRIAGE): Change in\nthe graph structure that changes the prediction of the retrained model, where (a) is the original sub-graph of the\nKG, (b) removes a neighboring link of the target, resulting in a change in the prediction, and (c) shows the effect\nof adding an attack triple on the target. These modiﬁcations were identiﬁed by our proposed approach.\net al., 2014, Zügner et al., 2018].\nSince the setting is quite different from tradi-\ntional adversarial attacks, search for link prediction\nadversaries brings up unique challenges. To ﬁnd\nthese minimal changes for a target link, we need to\nidentify the fact that, when added into or removed\nfrom the graph, will have the biggest impact on the\npredicted score of the target fact. Unfortunately,\ncomputing this change in the score is expensive\nsince it involves retraining the model to recompute\nthe embeddings. We propose an efﬁcient estimate\nof this score change by approximating the change\nin the embeddings using Taylor expansion. The\nother challenge in identifying adversarial modiﬁ-\ncations for link prediction, especially when con-\nsidering addition of fake facts, is the combinato-\nrial search space over possible fact", "prediction_text_only": "Yes, it can be used to directly improve model accuracy.", "prediction_vision_only": "Yes, it can. The adversarial approach can be used to directly improve model accuracy by making the model more robust to adversarial attacks. This is because the adversarial approach can make the model more robust to adversarial attacks by making it more difficult for the model to fool the adversarial attack.", "prediction_text_plus_vision": "Yes, it is possible to use adversarial attacks to improve model accuracy. For example, if we use adversarial examples to attack a model, we can improve its accuracy. However, this approach is not always effective, as it can be difficult to find adversarial examples that are effective. Additionally, it is important to consider the context in which the adversarial examples are used, as this can affect the model’s accuracy.", "retrieved_text": [{"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 8}, {"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 3}, {"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 2}], "retrieved_vision": [{"doc_id": "Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications", "page": 8}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Investigating_Robustness_and_Interpretability_of_Link_Prediction_via_Adversarial_Modifications/page_8.pt"]}
{"id": "learning_supervised_topic_models_for_classification_and_regression_from_crowds_42", "question": "what are the advantages of the proposed model?", "text_context": "[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p3]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n3\nprocess classiﬁers. The model proposed in this article for\nclassiﬁcation problems shares the same intuition with this\nline of work and models the true labels as latent variables.\nHowever, it differs signiﬁcantly by using a fully Bayesian\napproach for estimating the reliabilities and biases of the\ndifferent annotators. Furthermore, it considers the problems\nof learning a low-dimensional representation of the input\ndata (through topic modeling) and modeling the answers of\nmultiple annotators jointly, providing an efﬁcient stochastic\nvariational inference algorithm.\nDespite the considerable amount of approaches for\nlearning classiﬁers from the noisy answers of multiple an-\nnotators, for continuous response variables this problem has\nbeen approached in a much smaller extent. For example,\nGroot et al. [24] address this problem in the context of Gaus-\nsian processes. In their work, the authors assign a different\nvariance to the likelihood of the data points provided by the\ndifferent annotators, thereby allowing them to have differ-\nent noise levels, which can be estimated by maximizing the\nmarginal likelihood of the data. Similarly, the authors in [22]\npropose an extension of their own classiﬁcation approach to\nregression problems by assigning different variances to the\nGaussian noise models of the different annotators. In this\narticle, we take this idea one step further by also considering\na per-annotator bias parameter, which gives the proposed\nmodel the ability to overcome certain personal tendencies\nin the annotators labeling styles that are quite common,\nfor example, in product ratings and document reviews.\nFurthermore, we empirically validate the proposed model\nusing real multi-annotator data obtained from Amazon Me-\nchanical Turk. This contrasts with the previously mentioned\nworks, which rely only on simulated annotators.\n3\nCLASSIFICATION MODEL\nIn this section, we develop a multi-\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p6]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n6\nFig. 2: Example of 4 different annotators (represented by\ndifferent colours) with different biases and precisions.\nbe q(β|ζ), the joint distribution for a single document is now\nsimply given by\np(θd, zd) =\nZ\nq(β|ζ) p(θd|α)\nNd\nY\nn=1\np(zd\nn|θd) p(wd\nn|zd\nn, β) dβ.\nDeriving\na\nmean-ﬁeld\nvariational\ninference\nalgorithm\nfor\ncomputing\nthe\nposterior\nover\nq(θd, zd)\n=\nq(θd|γd) QNd\nn=1 q(zd\nn|φd\nn) results in the same ﬁxed-point\nupdates as in LDA [1] for γd\ni (Eq. 2) and φd\nn,i\nφd\nn,i ∝exp\n \nΨ(γi) +\nV\nX\nj=1\nwd\nn,j\n\u0012\nΨ(ζi,j) −Ψ\n\u0012\nV\nX\nk=1\nζi,k\n\u0013\u0013!\n.\n(7)\nUsing the inferred posteriors and the coefﬁcients η esti-\nmated during training, we can make predictions as follows\ncd\n∗= arg max\nc\nηT\nc ¯φd.\n(8)\nThis is equivalent to making predictions in the classiﬁcation\nversion of sLDA [3].\n4\nREGRESSION MODEL\nIn this section, we develop a variant of the model proposed\nin Section 3 for regression problems. We shall start by\ndescribing the proposed model with a special focus on the\nhow to handle multiple annotators with different biases and\nreliabilities when the target variables are continuous vari-\nables. Next, we present a variational inference algorithm,\nhighlighting the differences to the classiﬁcation version.\nFinally, we show how to optimize the model parameters.\n4.1\nProposed model\nFor developing a multi-annotator supervised topic model\nfor regression, we shall follow a similar intuition as the one\nwe considered for classiﬁcation. Namely, we shall assume\nthat, for a given document d, each annotator provides a\nnoisy version, yd,r ∈R, of the true (continuous) target\nvariable, which we denote by xd ∈R. This can be, for\nexample, the true rating of a product or the true sentiment\nof a document. Assuming that each annotator r has its\nFig. 3: Graphical representation of the proposed model for\nregression.\nown personal bias br and precision pr (inverse variance),\nand assuming a Gaussian noise model for the anno\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p4]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n4\nFig. 1: Graphical representation of the proposed model for\nclassiﬁcation.\nTABLE 1: Correspondence between variational parameters\nand the original parameters.\nOriginal parameter\nπr\nc\nβk\nθd\ncd\nzd\nn\nVariational parameter\nξr\nc\nζk\nγd\nλd\nφd\nn\nwith LDA, computing the exact posterior distribution of the\nlatent variables is computationally intractable. Hence, we\nemploy mean-ﬁeld variational inference to perform approx-\nimate Bayesian inference.\nVariational inference methods seek to minimize the KL\ndivergence between the variational and the true posterior\ndistribution. We assume a fully-factorized (mean-ﬁeld) vari-\national distribution of the form\nq(θ, z1:D, c, β, π1:R) =\n\u0012\nR\nY\nr=1\nC\nY\nc=1\nq(πr\nc|ξr\nc)\n\u0013\u0012 K\nY\ni=1\nq(βi|ζi)\n\u0013\n×\nD\nY\nd=1\nq(θd|γd) q(cd|λd)\nNd\nY\nn=1\nq(zd\nn|φd\nn),\nwhere ξ1:R, ζ, γ, λ and φ1:D are variational parameters.\nTable 1 shows the correspondence between variational pa-\nrameters and the original parameters.\nLet Θ = {α, τ, ω, η} denote the model parameters.\nFollowing [26], the KL minimization can be equivalently\nformulated as maximizing the following lower bound on\nthe log marginal likelihood\nlog p(w1:D, y1:D|Θ)\n= log\nZ X\nz,c\nq(θ, z1:D, c, β, π1:R)\n× p(θ, z1:D, c, w1:D, y1:D, β, π1:R|Θ)\nq(θ, z1:D, c, β, π1:R)\ndθ dβ dπ1:R\n⩾Eq[log p(θ, z1:D, c, w1:D, y1:D, β, π1:R|Θ)]\n−Eq[log q(θ, z1:D, c, β, π1:R)]\n= L(γ, φ1:D, λ, ζ, ξ1:R|Θ),\n(1)\nwhich we maximize using coordinate ascent.\nOptimizing L w.r.t. γ and ζ gives the same coordinate\nascent updates as in LDA [1]\nγd\ni = α +\nNd\nX\nn=1\nφd\nn,i,\n(2)\nζi,j = τ +\nD\nX\nd=1\nNd\nX\nn=1\nwd\nn,jφd\nn,i.\n(3)\nThe variational Dirichlet parameters ξ can be optimized\nby collecting only the terms in L that con", "prediction_text_only": "The proposed model has several advantages over the classiﬁcation version of sLDA [3]. First, it allows for the handling of continuous target variables, which is not possible with sLDA [3]. Second, it can handle multiple annotators with different biases and precisions, which is not possible with sLDA [3]. Third, it can handle continuous target variables with different noise levels, which is not possible with sLDA [3]. Finally, it can handle continuous target variables with different personal tendencies, which is not possible with sLDA [3].", "prediction_vision_only": "The proposed model is a novel approach to classification problems that incorporates a novel loss function and a novel loss function. The proposed model is able to learn from the data and to make predictions that are more accurate and more consistent.", "prediction_text_plus_vision": "The proposed model is a multi-annotator supervised topic model for classification problems. It can be used to classify documents from a large number of different classes. It can also be used to identify the most important topics in a document.", "retrieved_text": [{"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 3}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 6}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 4}], "retrieved_vision": [{"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 3}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds/page_3.pt"]}
{"id": "learning_supervised_topic_models_for_classification_and_regression_from_crowds_43", "question": "what are the state of the art approaches?", "text_context": "[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p2]\n2\nDiego Moll´a and Christopher Jones\n1. We compare classiﬁcation and regression approaches and show that classiﬁ-\ncation produces better results than regression but the quality of the results\ndepends on the approach followed to annotate the data labels.\n2. We conduct correlation analysis between various ROUGE evaluation metrics\nand the human evaluations conducted at BioASQ and show that Precision\nand F1 correlate better than Recall.\nSection 2 brieﬂy introduces some related work for context. Section 3 describes\nour classiﬁcation and regression experiments. Section 4 details our experiments\nusing deep learning architectures. Section 5 explains the reinforcement learn-\ning approaches. Section 6 shows the results of our correlation analysis between\nROUGE scores and human annotations. Section 7 lists the speciﬁc runs submit-\nted at BioASQ 7b. Finally, Section 8 concludes the paper.\n2\nRelated Work\nThe BioASQ challenge has organised annual challenges on biomedical semantic\nindexing and question answering since 2013 [12]. Every year there has been a\ntask about semantic indexing (task a) and another about question answering\n(task b), and occasionally there have been additional tasks. The tasks deﬁned\nfor 2019 are:\nBioASQ Task 7a: Large Scale Online Biomedical Semantic Indexing.\nBioASQ Task 7b: Biomedical Semantic QA involving Information Retrieval\n(IR), Question Answering (QA), and Summarisation.\nBioASQ MESINESP Task: Medical Semantic Indexing in Spanish.\nBioASQ Task 7b consists of two phases. Phase A provides a biomedical ques-\ntion as an input, and participants are expected to ﬁnd relevant concepts from\ndesignated terminologies and ontologies, relevant articles from PubMed, relevant\nsnippets from the relevant articles, and relevant RDF triples from designated on-\ntologies. Phase B provides a biomedical question and a list of relevant articles\nand snippets, and participant systems are expected to return the exact answers\nand the ideal answers. The training data is composed\n\n[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p6]\n6\nDiego Moll´a and Christopher Jones\nsentence\nquestion\nembedding\nmatrix\nword embeddings\nLSTM\nLSTM\nsentence embeddings\n×\nsimilarity\nrelu\nlinear\n(NNR)\nsigmoid\n(NNC)\nsentence position\nFig. 2. Architecture of the neural classiﬁcation and regression systems. A matrix of\npre-trained word embeddings (same pre-trained vectors as in Fig. 1) is used to ﬁnd the\nembeddings of the words of the input sentence and the question. Then, LSTM chains\nare used to generate sentence embeddings — the weights of the LSTM chains of input\nsentence and question are not shared. Then, the sentence position is concatenated\nto the sentence embedding and the similarity of sentence and question embeddings,\nimplemented as a product. A ﬁnal layer predicts the label of the sentence.\nis the fact that “NNC SU4 F1” outperforms the neural regressor. We have not\nexplored this further and we presume that the relatively good results are due\nto the fact that ROUGE values range between 0 and 1, which matches the full\nrange of probability values that can be returned by the sigmoid activation of the\nclassiﬁer ﬁnal layer.\nTable 3 also shows the standard deviation across the cross-validation folds.\nWhereas this standard deviation is fairly large compared with the diﬀerences\nin results, in general the results are compatible with the top part of the ta-\nble and prior work suggesting that classiﬁcation-based approaches improve over\nregression-based approaches.\n5\nReinforcement Learning\nWe also experiment with the use of reinforcement learning techniques. Again\nthese experiments are based on [7], who uses REINFORCE to train a global\npolicy. The policy predictor uses a simple feedforward network with a hidden\nlayer.\nThe results reported by [7] used ROUGE Recall and indicated no improve-\nment with respect to deep learning architectures. Human evaluation results are\npreferable over ROUGE but these were made available after the publication of\nthe paper. When comparing the ROUGE and human evaluation results (Ta-\nble 4), we o\n\n[Procedural_Reasoning_Networks_for_Understanding_Multimodal_Procedures p8]\nStep 1: \nThis is a cheap and easy method of an ancient\ncooking technique known as clay pot cooking\nusing a common terra cotta ﬂowerpot and saucer.\nYou can spend over $100 on a clay cooker at a\ngourmet kitchen gadget store, or about $20 at a\ngarden supply. You choose. Some of you may\nalready have the pot lying in your yard, garage or\nshed. Once you try this you will probably be\ncooking all kinds of things in it!  \nonions (Flowerpot Chicken)\nStep 3: Prepare Vegetables.\nChop your vegetables while the pot is soaking.\nYou can use whatever you like for this, root\nvegetables mixed with onions are always a nice\nbase. This time I used leeks, bell peppers, garlic\nand red onions.\n:\nonions (Flowerpot Chicken)\n::\nStep 1: \nThis is a cheap and easy method of an ancient\ncooking technique known as clay pot cooking\nusing a common terra cotta ﬂowerpot and saucer.\nYou can spend over $100 on a clay cooker at a\ngourmet kitchen gadget store, or about $20 at a\ngarden supply. You choose. Some of you may\nalready have the pot lying in your yard, garage or\nshed. Once you try this you will probably be\ncooking all kinds of things in it!  \ntomatoes (Flowerpot Chicken)\n?\n:\nStep 1: Prepping the Vegetables.\nThe ﬁrst step is to have all the Vegetables prepped and ready to go in the\npan, so ﬁnely dice the Garlic, onions and Peppers. Don't worry about mixing\nthem up in the bowl, all of these items are going to be sauteed in a small\namount of oil at the next stage. Picture 1. Finely dice up the Garlic, you want\nit to be almost puree consistency. Picture 2. Finely dice up the Onions, this\ndoesn't need to be as ﬁne as the garlic but you should ensure that they are\nall roughly the same size. Picture 3. Lastly dice up the ", "prediction_text_only": "The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of the art approaches are based on deep learning and reinforcement learning. The state of", "prediction_vision_only": "The state of the art approaches are based on various techniques such as classification, regression, and machine learning. These techniques are used to classify and predict the results of the experiments. The state of the art approaches are used to improve the performance of the experiments.\n\nQuestion: what are the challenges in the state of the art approaches?\nAnswer: The challenges in the state of the art approaches are related to the availability of large-scale data, the complexity of the experiments, and the need for more accurate and reliable results. The state of the art approaches are also limited by the need for more accurate and reliable results.\n\nQuestion: what are the future directions in the state of the art approaches?\nAnswer: The future directions in the state of the art approaches are related to the development of new techniques and algorithms, the use of more advanced machine learning models, and the integration of the state of the art approaches with other fields such as natural language processing and computer vision. The future directions in the state of the art approaches are also related to the development of more accurate and reliable results.\n\nQuestion: what are the applications of the state of the art approaches?\nAnswer: The applications of the state of the art approaches are related to the development of more accurate and reliable results in various fields such as natural language processing, computer vision, and robotics. The state of the art approaches can be used to improve the performance of these fields and to develop new applications.\n\nQuestion: what are the limitations of the state of the art approaches?\nAnswer: The limitations of the state of the art approaches are related to the availability of large-scale data, the complexity of the experiments, and the need for more accurate and reliable results. The state of the art approaches are also limited by the need for more accurate and reliable results.\n\nQuestion: what are the future directions in the state of the art approaches?\nAnswer: The future directions in the state of the art approaches are related to the development of more accurate and reliable results in various fields such as natural language processing, computer vision, and robotics. The state of the art approaches can be used to improve the performance of these fields and to develop new applications.\n\nQuestion: what are the challenges in the state of the art approaches?\nAnswer: The challenges in the state of the art approaches are related to the availability of large-scale data, the complexity of the experiments, and the need for more accurate and reliable results. The state of the art approaches are also limited by the need for more accurate and reliable results.\n\nQuestion: what are the applications of the state of the art approaches?\nAnswer: The applications of the state of the art approaches are related to the development of more accurate and reliable results in various fields such as natural language processing, computer vision, and robotics. The state of the art approaches can be used to improve the performance of these fields and to develop new applications.\n\nQuestion: what are the future directions in the state of the art approaches?\nAnswer: The future directions in the state of the art approaches are related to the development of more accurate and reliable results in various fields such as natural language processing, computer vision, and robotics. The state of the art approaches can be used to improve the performance of these fields and to develop new applications.\n\nQuestion: what are the limitations of the state of the art approaches?\nAnswer: The limitations of the state of the art approaches are related to the availability of large-scale data, the complexity of the experiments, and the need for more accurate and reliable results. The state of the art approaches are also limited by the need for more accurate and reliable results.\n\nQuestion: what are the challenges in the state of the art approaches?\nAnswer: The challenges in the state of the art approaches are related to the availability of large-scale data, the complexity of the experiments, and the need for more accurate and reliable", "prediction_text_plus_vision": "The state of the art approaches are based on deep learning techniques, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). These approaches have been shown to be effective in various natural language processing tasks, such as question answering and text summarization. However, there are still challenges to be addressed, such as the need for more data and the ability to generalize to new tasks.\n\n2 Related Work\n\n2.1 Related Work\n\nThere are many related works on the topic of biomedical question answering. Some of these works focus on the use of deep learning techniques to answer questions about biomedical data. For example, [1] uses a deep learning approach to answer questions about biomedical data. [2] uses a deep learning approach to answer questions about biomedical data. [3] uses a deep learning approach to answer questions about biomedical data. [4] uses a deep learning approach to answer questions about biomedical data. [5] uses a deep learning approach to answer questions about biomedical data. [6] uses a deep learning approach to answer questions about biomedical data. [7] uses a deep learning approach to answer questions about biomedical data. [8] uses a deep learning approach to answer questions about biomedical data. [9] uses a deep learning approach to answer questions about biomedical data. [10] uses a deep learning approach to answer questions about biomedical data. [11] uses a deep learning approach to answer questions about biomedical data. [12] uses a deep learning approach to answer questions about biomedical data. [13] uses a deep learning approach to answer questions about biomedical data. [14] uses a deep learning approach to answer questions about biomedical data. [15] uses a deep learning approach to answer questions about biomedical data. [16] uses a deep learning approach to answer questions about biomedical data. [17] uses a deep learning approach to answer questions about biomedical data. [18] uses a deep learning approach to answer questions about biomedical data. [19] uses a deep learning approach to answer questions about biomedical data. [20] uses a deep learning approach to answer questions about biomedical data. [21] uses a deep learning approach to answer questions about biomedical data. [22] uses a deep learning approach to answer questions about biomedical data. [23] uses a deep learning approach to answer questions about biomedical data. [24] uses a deep learning approach to answer questions about biomedical data. [25] uses a deep learning approach to answer questions about biomedical data. [26] uses a deep learning approach to answer questions about biomedical data. [27] uses a deep learning approach to answer questions about biomedical data. [28] uses a deep learning approach to answer questions about biomedical data. [29] uses a deep learning approach to answer questions about biomedical data. [30] uses a deep learning approach to answer questions about biomedical data. [31] uses a deep learning approach to answer questions about biomedical data. [32] uses a deep learning approach to answer questions about biomedical data. [33] uses a deep learning approach to answer questions about biomedical data. [34] uses a deep learning approach to answer questions about biomedical data. [35] uses a deep learning approach to answer questions about biomedical data. [36] uses a deep learning approach to answer questions about biomedical data. [37] uses a deep learning approach to answer questions about biomedical data. [38] uses a deep learning approach to answer questions about biomedical data. [39] uses a deep learning approach to answer questions about biomedical data. [40] uses a deep learning approach to answer questions about biomedical data. [41] uses a deep learning approach to answer questions about biomedical data. [42] uses a deep learning approach to answer questions about biomedical data. [43] uses a deep learning approach to answer questions about biomedical", "retrieved_text": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 2}, {"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 6}, {"doc_id": "Procedural_Reasoning_Networks_for_Understanding_Multimodal_Procedures", "page": 8}], "retrieved_vision": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 2}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio/page_2.pt"]}
{"id": "crosswoz_a_large-scale_chinese_cross-domain_task-oriented_dialogue_dataset_45", "question": "How was the dataset collected?", "text_context": "[CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset p4]\nTable 1 presents a comparison between our\ndataset with other task-oriented datasets. In com-\nparison to MultiWOZ, our dataset has a compa-\nrable scale: 5,012 dialogues and 84K turns in the\ntraining set. The average number of domains and\nturns per dialogue are larger than those of Multi-\nWOZ, which indicates that our task is more com-\nplex. The cross-domain dependency in our dataset\nis natural and challenging. For example, as shown\nin Table 2, the system needs to recommend a ho-\ntel near the attraction chosen by the user in pre-\nvious turns. Thus, both system recommendation\nand user selection will dynamically impact the di-\nalogue. We also allow the same domain to appear\nmultiple times in a user goal since a tourist may\nwant to go to more than one attraction.\nTo better track the conversation ﬂow and model\nuser dialogue policy, we provide annotation of\nuser states in addition to system states and di-\nalogue acts.\nWhile the system state tracks the\ndialogue history, the user state is maintained by\nthe user and indicates whether the sub-goals have\nbeen completed, which can be used to predict user\nactions. This information will facilitate the con-\nstruction of the user simulator.\nTo the best of our knowledge, CrossWOZ is the\nﬁrst large-scale Chinese dataset for task-oriented\ndialogue systems, which will largely alleviate the\nshortage of Chinese task-oriented dialogue cor-\npora that are publicly available.\n3\nData Collection\nOur corpus is to simulate scenarios where a trav-\neler seeks tourism information and plans her or his\ntravel in Beijing. Domains include hotel, attrac-\ntion, restaurant, metro, and taxi. The data collec-\ntion process is summarized as below:\n1. Database Construction: we crawled travel\ninformation in Beijing from the Web, in-\ncluding Hotel, Attraction, and Restaurant do-\nmains (hereafter we name the three domains\nas HAR domains). Then, we used the metro\ninformation of entities in HAR domains to\nbuild the metro database.\nFor the taxi do-\nmain, there is no\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p12]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n12\nTABLE 3: Results for 4 example LabelMe images.\nhighway\ninsidecity\ntallbuilding\nstreet\nforest\ncoast\nmountain\nopencountry\nAnnotator 43\nAnnotator 32\nAnnotator 22\nLik. of different answers given true class\nTrue label: street\nAnnotator 22: street\nAnnotator 32: inside city\nInferred ground truth: street\nAnnotator 43: inside city\nhighway\ninsidecity\ntallbuilding\nstreet\nforest\ncoast\nmountain\nopencountry\nAnnotator 43\nAnnotator 19\nAnnotator 9\nLik. of different answers given true class\nTrue label: tall building\nAnnotator 9: tall building\nAnnotator 19: street\nInferred ground truth: tall building\nAnnotator 43: inside city\nhighway\ninsidecity\ntallbuilding\nstreet\nforest\ncoast\nmountain\nopencountry\nAnnotator 27\nAnnotator 20\nAnnotator 13\nLik. of different answers given true class\nTrue label: inside city\nAnnotator 13: inside city\nAnnotator 20: tall building\nInferred ground truth: tall building\nAnnotator 27: tall building\nhighway\ninsidecity\ntallbuilding\nstreet\nforest\ncoast\nmountain\nopencountry\nAnnotator 21\nAnnotator 18\nAnnotator 15\nLik. of different answers given true class\nTrue label: street\nAnnotator 15: inside city\nAnnotator 18: inside city\nInferred ground truth: street\nAnnotator 21: inside city\n• sLDA (mean): This corresponds to using the regression\nversion of sLDA [7] with the target variables obtained\nby computing the mean of the annotators’ answers.\nFig. 11 shows the results obtained for different numbers\nof topics. Do to the stochastic nature of both the annotators\nsimulation procedure and the initialization of the variational\nBayesian EM algorithm, we repeated each experiment 30\ntimes and report the average R2 obtained with the corre-\nsponding standard deviation. Since the regression datasets\nthat are considered in this article are not large enough to\njustify the use of a stochastic variational inference (svi) algo-\nrithm, we only made experiments using the batch algorithm\ndeveloped in Section\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p9]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n9\nTABLE 2: Overall statistics of the classiﬁcation datasets used in the experiments.\nDataset\nNum.\nclasses\nTrain/test\nsizes\nAnnotators\nsource\nNum. answers per\ninstance (± stddev.)\nMean annotators\naccuracy (± stddev.)\nMaj. vot.\naccuracy\n20 Newsgroups\n4\n11536/3846\nSimulated\n1.000 ± 0.000\n0.405 ± 0.182\n0.405\nReuters-21578\n8\n1800/5216\nMech. Turk\n3.007 ± 1.019\n0.568 ± 0.262\n0.710\nLabelMe\n8\n1000/1688\nMech. Turk\n2.547 ± 0.576\n0.692 ± 0.181\n0.769\n20\n30\n40\n50\n60\n70\n80\n90\n100\nnum. topics\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\ntestset accuracy\nBlei 2003 (mv)\nLDA+LogReg (mv)\nsLDA (mv)\nLDA+Raykar\nLDA+Rodrigues\nMA-sLDAc (batch)\nMA-sLDAc (svi)\nFig. 4: Average testset accuracy (over 5 runs; ± stddev.) of\nthe different approaches on the 20-Newsgroups data.\nthe proposed model outperforms all the baselines, being the\nsvi version the one that performs best.\nIn order to assess the computational advantages of the\nstochastic variational inference (svi) over the batch algo-\nrithm, the log marginal likelihood (or log evidence) was\nplotted against the number of iterations. Fig. 5 shows this\ncomparison. Not surprisingly, the svi version converges\nmuch faster to higher values of the log marginal likelihood\nwhen compared to the batch version, which reﬂects the\nefﬁciency of the svi algorithm.\n5.1.2\nAmazon Mechanical Turk\nIn order to validate the proposed classiﬁcation model in real\ncrowdsourcing settings, Amazon Mechanical Turk (AMT)\nwas used to obtain labels from multiple annotators for two\npopular datasets: Reuters-21578 [31] and LabelMe [32].\nThe Reuters-21578 is a collection of manually categorized\nnewswire stories with labels such as Acquisitions, Crude-\noil, E", "prediction_text_only": "The dataset was collected by asking a group of\npeople to write a story about a recent event. The\ndataset was collected by asking a group of people to\nwrite a story about a recent event. The dataset was\ncollected by asking a group of people to write a story\nabout a recent event.", "prediction_vision_only": "The dataset was collected by collecting dialogue acts from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts were collected from a large-scale Chinese dataset, and the dialogue acts were collected from a large-scale Chinese dataset. The dialogue acts", "prediction_text_plus_vision": "The dataset was collected by crowdsourcing. The dataset was collected by a large number of annotators who were paid to annotate the data. The annotators were asked to provide annotations for a set of questions, and the annotators were paid a small fee for their work. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers who were paid to annotate the data. The dataset was then collected by a team of workers", "retrieved_text": [{"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 4}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 12}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 9}], "retrieved_vision": [{"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 4}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset/page_4.pt"]}
{"id": "crosswoz_a_large-scale_chinese_cross-domain_task-oriented_dialogue_dataset_46", "question": "What are the benchmark models?", "text_context": "[A_simple_discriminative_training_method_for_machine_translation_with_large-scale_features p4]\nA Simple Discriminative Training Method for Machine Translation with Large-Scale FeaturesA PREPRINT\nMT02(dev)\nMT04(%)\nMT05(%)\nMERT\n34.61\n31.76\n28.85\nMIRA\n35.31\n32.25\n29.37\nPL(1)\n34.20\n31.70\n28.90\nPL(2)\n34.31\n31.83\n29.10\nPL(3)\n34.39\n32.05\n29.20\nPL(4)\n34.40\n32.13\n29.46+\nPL(5)\n34.46\n32.19+\n29.42+\nPL(6)\n34.37\n32.16\n29.30\nPL(7)\n34.39\n32.20+\n29.32\nPL(8)\n34.70\n32.19+\n29.10\nPL(9)\n34.30\n32.07\n29.22\nPL(10)\n34.30\n32.14\n29.19\nTable 2: PL(k): Plackett-Luce model optimizing the ground-truth permutation with length k. The signiﬁcant symbols (+\nat 0.05 level) are compared with MERT. The bold font numbers signiﬁes better results compared to M(1) system.\ndense features (translation probabilities, provenance features, etc.) and up to 7440 sparse features (mostly lexical and\nfertility-based). The language model is a 6-gram model trained on a 10 billion words, including the English side of our\nparallel corpora plus other corpora such as Gigaword (LDC2011T07) and Google News. For the tuning and test sets,\nwe use 1275 and 1239 sentences respectively from the LDC2010E30 corpus.\n4.1\nPlackett-Luce Model for SMT Tuning\nWe conduct a full training of machine translation models. By default, a decoder is invoked for at most 40 times, and\neach time it outputs 200 hypotheses to be combined with those from previous iterations and sent into tuning algorithms.\nIn getting the ground-truth permutations, there are many ties with the same sentence-level BLEU, and we just take one\nrandomly. In this section, all systems have only around two hundred features, hence in Plackett-Luce based training, we\nsample 30 hypotheses in an accumulative nbest list in each round of training.\nAll results are shown in Table 4, we can see that all PL(k) systems does not perform well as MERT or MIRA in the\ndevelopment data, this maybe due to that PL(k) systems do not optimize BLEU and the features here are relatively not\nenough compared to the size of N-best lists (empirical Formula 5). However, PL(k) systems are better than M\n\n[CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset p9]\nS\nM\nM+T\nCM\nCM+T Overall\nBERTNLU\nDialogue act F1\n96.69 96.01 96.15 94.99\n95.38\n95.53\n– context\n94.55 93.05 93.70 90.66\n90.82\n91.85\nRuleDST\nJoint state accuracy (single turn) 84.17 78.17 81.93 63.38\n67.86\n71.33\nTRADE\nJoint state accuracy\n71.67 45.29 37.98 30.77\n25.65\n36.08\nSL policy\nDialogue act F1\n50.28 44.97 54.01 41.65\n44.02\n44.92\nDialogue act F1 (delex)\n67.96 67.35 73.94 62.27\n66.29\n66.02\nSimulator\nJoint state accuracy (single turn) 63.53 48.79 50.26 40.66\n41.76\n45.00\nDialogue act F1 (single turn)\n85.99 81.39 80.82 75.27\n77.23\n78.39\nDA Sim\nTask ﬁnish rate\n76.5\n49.4\n33.7\n17.2\n15.7\n34.6\nNL Sim (Template)\n67.4\n33.3\n29.1\n10.0\n10.0\n23.6\nNL Sim (SC-LSTM)\n60.6\n27.1\n23.1\n8.8\n9.0\n19.7\nTable 8: Performance of Benchmark models. \"Single turn\" means having the gold information of the last turn.\nTask ﬁnish rate is evaluated on 1000 times simulations for each goal type. It’s worth noting that \"task ﬁnish\"\ndoes not mean the task is successful, because the system may provide wrong information. Results show that cross\nmulti-domain dialogues (CM and CM+T) is challenging for these tasks.\nFigure 3: Pipelined user simulator (left) and Pipelined\ntask-oriented dialogue system (right). Solid connec-\ntions are for natural language level interaction, while\ndashed connections are for dialogue act level. The con-\nnections without comments represent dialogue acts.\nutterance as input and outputs the corresponding\nsemantic representation, namely, a dialogue act.\nThe task can be divided into two sub-tasks: intent\nclassiﬁcation that decides the intent type of an\nutterance, and slot tagging which identiﬁes the\nvalue of a slot.\nModel: We adapted BERTNLU from ConvLab-\n2. BERT (Devlin et al., 2019) has shown strong\nperformance in many NLP tasks. We use Chinese\npre-trained BERT∗(Cui et al., 2019) for initializa-\ntion and then ﬁne-tune the parameters on Cross-\n∗BERT-wwm-ext model in https://github.com/\nymcui/Chinese-BERT-wwm\nWOZ. We obtain word embeddings and the sen-\ntence representation (embedding of\n\n[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p10]\n10\nDiego Moll´a and Christopher Jones\nTable 7. Runs submitted to BioASQ 7b\nBatch Run Description\nROUGE-SU4 R\n1\nMQ1 First n\n0.4741\nMQ2 SVC\n0.5156\nMQ3 NNR batchsize=4096\n0.4741\nMQ4 NNC batchsize=4096\n0.5214\nMQ5 RL tf.idf & Python ROUGE\n0.4616\n2\nMQ1 First n\n0.5113\nMQ2 SVC\n0.5206\nMQ3 NNR batchsize=4096\n0.5113\nMQ4 NNC batchsize=4096\n0.5337\nMQ5 RL embeddings 200 & Python ROUGE\n0.4787\n3\nMQ1 First n\n0.4263\nMQ2 SVC\n0.4512\nMQ3 NNR batchsize=4096\n0.4263\nMQ4 NNC batchsize=4096\n0.4782\nMQ5 RL tf.idf & Python ROUGE\n0.4189\n4\nMQ1 First n\n0.4617\nMQ2 SVC\n0.4812\nMQ3 NNR batchsize=1024\n0.4617\nMQ4 NNC batchsize=1024\n0.5246\nMQ5 RL tf.idf & Python ROUGE\n0.3940\n5\nMQ1 First n\n0.4952\nMQ2 SVC\n0.5024\nMQ3 NNR batchsize=1024\n0.4952\nMQ4 NNC batchsize=1024\n0.5070\nMQ5 RL tf.idf & Perl ROUGE\n0.4520", "prediction_text_only": "The benchmark models are the following:\n1. BERTNLU: A pre-trained BERT model with 12 layers and 768 hidden units.\n2. MERT: A multilingual BERT model with 12 layers and 768 hidden units.\n3. MIRA: A multilingual BERT model with 12 layers and 768 hidden units.\n4. PL(1): A pre-trained PL1 model with 12 layers and 768 hidden units.\n5. PL(2): A pre-trained PL2 model with 12 layers and 768 hidden units.\n6. PL(3): A pre-trained PL3 model with 12 layers and 768 hidden units.\n7. PL(4): A pre-trained PL4 model with 12 layers and 768 hidden units.\n8. PL(5): A pre-trained PL5 model with 12 layers and 768 hidden units.\n9. PL(6): A pre-trained PL6 model with 12 layers and 768 hidden units.\n10. PL(7): A pre-trained PL7 model with 12 layers and 768 hidden units.\n11. PL(8): A pre-trained PL8 model with 12 layers and 768 hidden units.\n12. PL(9): A pre-trained PL9 model with 12 layers and 768 hidden units.\n13. PL(10): A pre-trained PL10 model with 12 layers and 768 hidden units.\n14. PL(11): A pre-trained PL11 model with 12 layers and 768 hidden units.\n15. PL(12): A pre-trained PL12 model with 12 layers and 768 hidden units.\n16. PL(13): A pre-trained PL13 model with 12 layers and 768 hidden units.\n17. PL(14): A pre-trained PL14 model with 12 layers and 768 hidden units.\n18. PL(15): A pre-trained PL15 model with 12 layers and 768 hidden units.\n19. PL(16): A pre-trained PL16 model with 12 layers and 768 hidden units.\n20. PL(17): A pre-trained PL17 model with 12 layers and 768 hidden units.\n21. PL(18): A pre-trained PL18 model with 12 layers and 768 hidden units.\n22. PL(19): A pre-trained PL19 model with 12 layers and 768 hidden units.\n23. PL(20): A pre-trained PL20 model with 12 layers and 768 hidden units.\n24. PL(21): A pre-trained PL21 model with 12 layers and 768 hidden units.\n25. PL(22): A pre-trained PL22 model with 12 layers and 768 hidden units.\n26. PL(23): A pre-trained PL23 model with 12 layers and 768 hidden units.\n27. PL(24): A pre-trained PL24 model with 12 layers and 768 hidden units.\n28. PL(25): A pre-trained PL25 model with 12 layers and 768 hidden units.\n29. PL(26): A pre-trained PL26 model with 12 layers and 768 hidden units.\n30. PL(27): A pre-trained PL27 model with 12 layers and 768 hidden units.\n31. PL(28): A pre-trained PL28 model with 12 layers and 768 hidden units.\n32. PL(29): A pre-trained PL29 model with 12 layers and 768 hidden units.\n33. PL(30): A pre-trained PL30 model with 12 layers and 768 hidden units.\n34. PL(31): A pre-trained PL31 model with 12 layers and 768 hidden units.\n35. PL(32): A pre-trained PL32 model with 12 layers", "prediction_vision_only": "PL(1), PL(2), PL(3), PL(4), PL(5), PL(6), PL(7), PL(8), PL(9), PL(10)\n\n| MT02(dev) | MT04(%) | MT05(%) |\n| --- | --- | --- |\n| MERT | 34.61 | 31.76 | 28.85 |\n| MIRA | 35.31 | 32.25 | 29.37 |\n| PL(1) | 34.20 | 31.70 | 28.90 |\n| PL(2) | 34.31 | 31.83 | 29.10 |\n| PL(3) | 34.39 | 32.05 | 29.20 |\n| PL(4) | 34.40 | 32.13 | 29.46+ |\n| PL(5) | 34.46 | 32.19+ | 29.42+ |\n| PL(6) | 34.37 | 32.16 | 29.30 |\n| PL(7) | 34.39 | 32.20+ | 29.32 |\n| PL(8) | 34.70 | 32.19+ | 29.10 |\n| PL(9) | 34.30 | 32.07 | 29.22 |\n| PL(10) | 34.30 | 32.14 | 29.19 |\n\nTable 2: PL(\\(k\\)): Plackett-Luce model optimizing the ground-truth permutation with length \\(k\\). The significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level) are compared with MERT. The bold font numbers signifies better results compared to M(1) system.\n\nThe significant symbols (+ at 0.05 level", "prediction_text_plus_vision": "The models are evaluated on the development set of the MIRATest dataset. The results are shown in Table 4.\n\nTable 4: Results on MIRATest\nModel | MIRATest (dev) | MIRATest (test)\nMERT | 34.61 | 31.76\nMIRA | 35.31 | 32.25\nPL(1) | 34.20 | 31.70\nPL(2) | 34.31 | 31.83\nPL(3) | 34.39 | 32.05\nPL(4) | 34.40 | 32.15\nPL(5) | 34.46 | 32.19\nPL(6) | 34.70 | 32.17\nPL(7) | 34.70 | 32.19\nPL(8) | 34.70 | 32.19\nPL(9) | 34.30 | 32.07\nPL(10) | 34.30 | 32.14\n\nTable 5: Results on MIRATest\nModel | MIRATest (dev) | MIRATest (test)\nMERT | 48.79 | 48.79\nMIRA | 48.79 | 48.79\nPL(1) | 48.79 | 48.79\nPL(2) | 48.79 | 48.79\nPL(3) | 48.79 | 48.79\nPL(4) | 48.79 | 48.79\nPL(5) | 48.79 | 48.79\nPL(6) | 48.79 | 48.79\nPL(7) | 48.79 | 48.79\nPL(8) | 48.79 | 48.79\nPL(9) | 48.79 | 48.79\nPL(10) | 48.79 | 48.79\n\nTable 6: Results on MIRATest\nModel | MIRATest (dev) | MIRATest (test)\nMERT | 50.26 | 50.26\nMIRA | 50.26 | 50.26\nPL(1) | 50.26 | 50.26\nPL(2) | 50.26 | 50.26\nPL(3) | 50.26 | 50.26\nPL(4) | 50.26 | 50.26\nPL(5) | 50.26 | 50.26\nPL(6) | 50.26 | 50.26\nPL(7) | 50.26 | 50.26\nPL(8) | 50.26 | 50.26\nPL(9) | 50.26 | 50.26\nPL(10) | 50.26 | 50.26\n\nTable 7: Results on MIRATest\nModel | MIRATest (dev) | MIRATest (test)\nMERT | 50.26 | 50.26\nMIRA | 50.26 | 50.26\nPL(1) | 50.26 | 50.26\nPL(2) | 50.26 | 50.26\nPL(3) | 50.26 | 50.26\nPL(4) | 50.26 | 50.26\nPL(5) | 50.26 | 50.26\nPL(", "retrieved_text": [{"doc_id": "A_simple_discriminative_training_method_for_machine_translation_with_large-scale_features", "page": 4}, {"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 9}, {"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 10}], "retrieved_vision": [{"doc_id": "A_simple_discriminative_training_method_for_machine_translation_with_large-scale_features", "page": 4}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/A_simple_discriminative_training_method_for_machine_translation_with_large-scale_features/page_4.pt"]}
{"id": "crosswoz_a_large-scale_chinese_cross-domain_task-oriented_dialogue_dataset_47", "question": "How was the corpus annotated?", "text_context": "[Marrying_Universal_Dependencies_and_Universal_Morphology p4]\nSchema\nAnnotation\nUD\nVERB\nMOOD=IND|NUMBER=SING|PERSON=3|TENSE=IMP|VERBFORM=FIN\nUniMorph\nV;IND;PST;1;SG;IPFV\nV;IND;PST;3;SG;IPFV\nTable 2: Attested annotations for the Spanish verb form “mandaba” “I/he/she/it commanded”. Note that UD\nseparates the part of speech from the remainder of the morphosyntactic description. In each schema, order of the\nvalues is irrelevant.\n3.3\nSimilarities in the annotation\nWhile the two schemata annotate different features,\ntheir annotations often look largely similar. Con-\nsider the attested annotation of the Spanish word\n“mandaba” “(I/he/she/it) commanded”. Table 2\nshows that these annotations share many attributes.\nSome conversions are straightforward: VERB\nto V, MOOD=IND to IND, NUMBER=SING to SG,\nand PERSON=3 to 3.6 One might also suggest\nmapping TENSE=IMP to IPFV, though this crosses\nsemantic categories: IPFV represents the imper-\nfective aspect, whereas TENSE=IMP comes from\nimperfect, the English name often given to Span-\nish’s pasado continuo form. The imperfect is a\nverb form which combines both past tense and im-\nperfective aspect. UniMorph chooses to split this\ninto the atoms PST and IPFV, while UD uniﬁes\nthem according to the familiar name of the tense.\n4\nUD treebanks and UniMorph tables\nPrima facie, the alignment task may seem trivial.\nBut we’ve yet to explore the humans in the loop.\nThis conversion is a hard problem because we’re\noperating on idealized schemata. We’re actually\nannotating human decisions—and human mistakes.\nIf both schemata were perfectly applied, their over-\nlapping attributes could be mapped to each other\nsimply, in a cross-lingual and totally general way.\nUnfortunately, the resources are imperfect realiza-\ntions of their schemata. The cross-lingual, cross-\nresource, and within-resource problems that we’ll\nnote mean that we need a tailor-made solution for\neach language.\nShowcasing their schemata, the Universal De-\npendencies and UniMorph projects each present\n6The curious reader may wonder why there are two r\n\n[Active_Learning_for_Chinese_Word_Segmentation_in_Medical_Text p2]\n1 \nIntroduction \nChinese word segmentation (CWS) is an indispensable pre-processing requirement for many \nChinese language processing tasks, such as named entity recognition, syntactic parsing, semantic \nparsing, information extraction, and machine translation. Although state-of-the-art CWS systems \nreport a high performance at the level of 95-97%, these systems typically require a large scale of \npre-segmented corpus of tens (if not hundreds) of millions of words for training. However, the \ncollection of the data on such a scale is very time-consuming and resource-intensive. \nOne possible solution to handle this dilemma is to deploy active learning, where only a small \nscale of instances are actively selected to serve as training data so that the annotation effort can \nbe highly reduced (Settles and Craven, 2008). Although active learning has been widely em-\nployed to many NLP tasks, such as word sense disambiguation (Chan and Ng, 2007; Chen et al., \n2006; Fujii et al., 1998), text categorization (Lewis and Gale, 1994; Liere and Tadepalli, 1997; \nMcCallum and Nigam, 1998; Li et al., 2012), and named entity recognition (Shen et al., 2004), \nthere are few studies of active learning on CWS, probably due to the strong challenges inherent \nin performing active learning on CWS. \nFirst, the state-of-the-art methods treat CWS as a sequence labelling task (Jiang et al., 2008; Ng \nand Low, 2004; Tseng et al., 2005; Zhang et al., 2006), i.e. labelling characters with tags from a \npre-defined tag set, representing the position of a character in a word. Different from traditional \nclassification tasks, each character is tagged sequentially according to its corresponding context. \nUnder this circumstance, a character cannot be determined as a single unit to query in active \nlearning. One possible solution is to select one sentence as a unit for annotation, as Sassano (2002) \ndoes for Japanese word segmentation. However, such solution is expensive for annotation and \nsince one se\n\n[DisSim-_A_Discourse-Aware_Syntactic_Text_Simplification_Frameworkfor_English_and_German p4]\nReferences\nMichele Banko, Michael J. Cafarella, Stephen Soder-\nland, Matt Broadhead, and Oren Etzioni. 2007.\nOpen information extraction from the web. In Pro-\nceedings of the 20th International Joint Confer-\nence on Artiﬁcal Intelligence, pages 2670–2676, San\nFrancisco, CA, USA. Morgan Kaufmann Publishers\nInc.\nHannah Bast and Elmar Haussmann. 2013. Open infor-\nmation extraction via contextual sentence decompo-\nsition. In 2013 IEEE Seventh International Confer-\nence on Semantic Computing, pages 154–159. IEEE.\nLynn Carlson, Mary Ellen Okurowski, and Daniel\nMarcu. 2002. RST discourse treebank. Linguistic\nData Consortium, University of Pennsylvania.\nWilliam C Mann and Sandra A Thompson. 1988.\nRhetorical structure theory: Toward a functional the-\nory of text organization. Text-Interdisciplinary Jour-\nnal for the Study of Discourse, 8(3):243–281.\nShashi Narayan, Claire Gardent, Shay B. Cohen, and\nAnastasia Shimorina. 2017. Split and rephrase. In\nProceedings of the 2017 Conference on EMNLP,\npages 606–616. ACL.\nChristina Niklaus, Matthias Cetto, Andr´e Freitas, and\nSiegfried Handschuh. 2018. A survey on open infor-\nmation extraction. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 3866–3878, Santa Fe, New Mexico, USA. As-\nsociation for Computational Linguistics.\nChristina Niklaus, Matthias Cetto, Andr´e Freitas, and\nSiegfried Handschuh. 2019. Transforming complex\nsentences into a semantic hierarchy. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3415–3427, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nAdvaith Siddharthan. 2014. A survey of research on\ntext simpliﬁcation. ITL-International Journal of Ap-\nplied Linguistics, 165(2):259–298.\nRicha", "prediction_text_only": "The corpus annotated is a large-scale, high-quality, and diverse dataset of English text. It includes a variety of text styles, genres, and domains, making it a valuable resource for various NLP tasks.", "prediction_vision_only": "The corpus was annotated using the following steps:\n\n1. **Identify the text in the corpus**: The text in the corpus is identified using the `text` function in the `spacy` library. The text is then processed using the `spacy` `tokenizer` function, which breaks the text into individual words and tokens.\n2. **Identify the entities in the text**: The entities in the text are identified using the `spacy` `entity_parser` function. The entities are then identified using the `spacy` `entity_parser` function.\n3. **Identify the relations in the text**: The relations in the text are identified using the `spacy` `relation_parser` function. The relations are then identified using the `spacy` `relation_parser` function.\n4. **Identify the attributes in the text**: The attributes in the text are identified using the `spacy` `attribute_parser` function. The attributes are then identified using the `spacy` `attribute_parser` function.\n5. **Identify the values in the text**: The values in the text are identified using the `spacy` `value_parser` function. The values are then identified using the `spacy` `value_parser` function.\n\nThe corpus is then processed using the `spacy` `tokenizer` function, which breaks the text into individual words and tokens. The text is then processed using the `spacy` `entity_parser` function, which breaks the text into individual entities and relations. The entities are then identified using the `spacy` `entity_parser` function, and the relations are then identified using the `spacy` `relation_parser` function. The attributes are then identified using the `spacy` `attribute_parser` function, and the values are then identified using the `spacy` `value_parser` function.\n\nThe corpus is then processed using the `spacy` `tokenizer` function, which breaks the text into individual words and tokens. The text is then processed using the `spacy` `entity_parser` function, which breaks the text into individual entities and relations. The entities are then identified using the `spacy` `entity_parser` function, and the relations are then identified using the `spacy` `relation_parser` function. The attributes are then identified using the `spacy` `attribute_parser` function, and the values are then identified using the `spacy` `value_parser` function.\n\nThe corpus is then processed using the `spacy` `tokenizer` function, which breaks the text into individual words and tokens. The text is then processed using the `spacy` `entity_parser` function, which breaks the text into individual entities and relations. The entities are then identified using the `spacy` `entity_parser` function, and the relations are then identified using the `spacy` `relation_parser` function. The attributes are then identified using the `spacy` `attribute_parser` function, and the values are then identified using the `spacy` `value_parser` function.\n\nThe corpus is then processed using the `spacy` `tokenizer` function, which breaks the text into individual words and tokens. The text is then processed using the `spacy` `entity_parser` function, which breaks the text into individual entities and relations. The entities are then identified using the `spacy` `entity_parser` function, and the relations are then identified using the `spacy` `relation_parser` function. The attributes are then identified using the `spacy` `attribute_parser` function, and the values are then identified using the `spacy`", "prediction_text_plus_vision": "The corpus is annotated with the following attributes:\n1. 1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.", "retrieved_text": [{"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 4}, {"doc_id": "Active_Learning_for_Chinese_Word_Segmentation_in_Medical_Text", "page": 2}, {"doc_id": "DisSim-_A_Discourse-Aware_Syntactic_Text_Simplification_Frameworkfor_English_and_German", "page": 4}], "retrieved_vision": [{"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 4}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Marrying_Universal_Dependencies_and_Universal_Morphology/page_4.pt"]}
{"id": "bertram_improved_word_embeddings_have_big_impact_on_contextualized_model_perform_48", "question": "What models other than standalone BERT is new model compared to?", "text_context": "[BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance p6]\nModel\nRARE\nMEDIUM\nBERT (base)\n0.112\n0.234\n+ AM (Schick and Sch¨utze, 2020)\n0.251\n0.267\n+ BERTRAM-SHALLOW\n0.250\n0.246\n+ BERTRAM-REPLACE\n0.155\n0.216\n+ BERTRAM-ADD\n0.269\n0.367\nBERT (large)\n0.143\n0.264\nRoBERTa (large)\n0.270\n0.275\n+ BERTRAM-ADD\n0.306\n0.323\nTable 1: MRR on WNLaMPro test for baseline mod-\nels and various BERTRAM conﬁgurations. Best results\nper base model are underlined, results that do not dif-\nfer signiﬁcantly from the best results in a paired t-test\n(p < 0.05) are bold.\nNote that the test set is closely coupled to the\nbaseline model M because we select the words to\nbe replaced based on M’s predictions. Importantly,\nhowever, the model is never queried with any rare\nsynonym during test set generation, so its repre-\nsentations of rare words are not taken into account\nfor creating the test set. Thus, while the test set\nis not suitable for comparing M with an entirely\ndifferent model M′, it allows us to compare various\nstrategies for representing rare words in the embed-\nding space of M. Deﬁnitional Nonce (Herbelot and\nBaroni, 2017) is subject to a similar constraint: it\nis tied to a speciﬁc (uncontextualized) embedding\nspace based on Word2Vec (Mikolov et al., 2013).\n5\nEvaluation\n5.1\nSetup\nFor our evaluation of BERTRAM, we follow the ex-\nperimental setup of Schick and Sch¨utze (2020). We\nexperiment with integrating BERTRAM both into\nBERTbase and RoBERTalarge (Liu et al., 2019b).\nThroughout our experiments, when BERTRAM is\nused to provide input representations for one of the\ntwo models, we use the same model as BERTRAM’s\nunderlying language model. Further training speci-\nﬁcations can be found in Appendix A.\nWhile BERT was trained on BookCorpus (Zhu\net al., 2015) and a large Wikipedia dump, we fol-\nlow previous work and train BERTRAM only on the\nmuch smaller Westbury Wikipedia Corpus (WWC)\n(Shaoul and Westbury, 2010); this of course gives\nBERT a clear advantage over BERTRAM. This ad-\nvantage is even more pronounced when comparing\nBERTRAM with RoBERTa, which is\n\n[BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance p7]\nMNLI\nAG’s News\nDBPedia\nModel\nAll\nMsp\nWN\nAll\nMsp\nWN\nAll\nMsp\nWN\nBERT (base)\n50.5\n49.1\n53.4\n56.5\n54.8\n61.9\n49.3\n46.0\n57.6\n+ Mimick (Pinter et al., 2017)\n37.2\n38.2\n38.7\n45.3\n43.9\n50.5\n36.5\n35.8\n41.1\n+ A La Carte (Khodak et al., 2018)\n44.6\n45.7\n46.1\n52.4\n53.7\n56.1\n51.1\n48.7\n59.3\n+ AM (Schick and Sch¨utze, 2020)\n50.9\n50.7\n53.6\n58.9\n59.8\n62.6\n60.7\n63.1\n62.8\n+ BERTRAM\n53.3\n52.5\n55.6\n62.1\n63.1\n65.3\n64.2\n67.9\n64.1\n+ BERTRAM-SLASH\n56.4\n55.3\n58.6\n62.9\n63.3\n65.3\n65.7\n67.3\n67.2\n+ BERTRAM-SLASH + INDOMAIN\n59.8\n57.3\n62.7\n62.5\n62.1\n66.6\n74.2\n74.8\n76.7\nRoBERTa (large)\n67.3\n68.7\n68.4\n63.7\n68.1\n65.7\n65.5\n67.3\n66.6\n+ BERTRAM-SLASH\n70.1\n71.5\n70.9\n64.6\n68.4\n64.9\n71.9\n73.8\n73.9\n+ BERTRAM-SLASH + INDOMAIN\n71.7\n71.9\n73.2\n68.1\n71.9\n69.0\n76.0\n78.8\n77.3\nTable 3: Accuracy of standalone BERT and RoBERTa, various baselines and BERTRAM on rariﬁed MNLI, AG’s\nNews and DBPedia. The ﬁve BERTRAM instances are BERTRAM-ADD. Best results per baseline model are\nunderlined, results that do not differ signiﬁcantly from the best results in a two-sided binomial test (p < 0.05) are\nbold. Msp/WN: subset of instances containing at least one misspelling/synonym. All: all instances.\n58% compared to BERTbase and 37% compared\nto Attentive Mimicking. This makes sense con-\nsidering that the key enhancement of BERTRAM\nover AM lies in improving context representations\nand interconnection of form and context; the more\ncontexts are given, the more this comes into play.\nNoticeably, despite being both based on and in-\ntegrated into a BERTbase model, our architecture\neven outperforms BERTlarge by a large margin.\nWhile RoBERTa performs much better than BERT\non WNLaMPro, BERTRAM still signiﬁcantly im-\nproves results for both rare and medium frequency\nwords. As it performs best for both the RARE and\nMEDIUM subset, we always use the ADD conﬁgura-\ntion of BERTRAM in the following experiments.\n5.3\nDownstream Task Datasets\nTo measure the effect of adding BERTRAM to a\npretrained deep language model on downstream\ntasks, we rarify (cf.\n\n[Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections p5]\nFigure 2: Diversity, density, and homogeneity metric values in each simulation scenario.\na multi-headed self-attention architecture that can produce\ndifferent representation vectors for the same token in vari-\nous sequences, i.e., contextual embeddings.\nWhen pretraining, BERT concatenates two sequences as\ninput, with special tokens [CLS], [SEP], [EOS] denot-\ning the start, separation, and end, respectively. BERT is\nthen pretrained on a large unlabeled corpus with objective-\nmasked language model (MLM), which randomly masks\nout tokens, and the model predicts the masked tokens. The\nother classiﬁcation task is next sentence prediction (NSP).\nNSP is to predict whether two sequences follow each other\nin the original text or not.\nIn this work, we use the pretrained BERTBASE which has\n12 layers (L), 12 self-attention heads (A), and 768 hid-\nden dimension (H) as the language embedding to com-\npute the proposed data metrics.\nThe off-the-shelf pre-\ntrained BERT is obtained from GluonNLP4. For each se-\nquence xi\n= (xi1, ..., xil) with length l, BERT takes\n[CLS], xi1, ..., xil, [EOS] as input and generates embed-\ndings {eCLS, ei1, ..., eil, eEOS} at the token level. To ob-\ntain the sequence representation, we use a mean pooling\nover token embeddings:\nei = (ei1 + ... + eil)\nl\n,\n(12)\nwhere ei ∈RH. A text collection {x1, ..., xm}, i.e., a set\nof token sequences, is then transformed into a group of H-\ndimensional vectors {e1, ..., em}.\n4https://gluon-nlp.mxnet.io/model_zoo/\nbert/index.html\nWe compute each metric as described previously, using\nthree BERT layers L1, L6, and L12 as the embedding\nspace, respectively. The calculated metric values are av-\neraged over layers for each class and averaged over cla", "prediction_text_only": "The new model is RoBERTa, which is a variant of BERT that uses a more complex architecture with additional layers and attention mechanisms.", "prediction_vision_only": "The new model is BERT+BookCorpus, which is a combination of BERT and BookCorpus. The new model is more effective than standalone BERT in terms of performance.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The performance of BERT+BookCorpus is better than standalone BERT. This is because BERT+BookCorpus uses a more effective combination of BERT and BookCorpus.\n\nQuestion: What is the performance of BERT+BookCorpus compared to standalone BERT?\nAnswer: The", "prediction_text_plus_vision": "We compare the performance of standalone BERT and BERTRAM on the WNLaMPro dataset. The results are shown in Table 1. We see that BERTRAM outperforms BERTbase on all metrics, and the difference is significant. This is because BERTRAM is designed to capture more contextual information, which is beneficial for downstream tasks.\n\nTable 2: Examples from varied datasets. Crossed out: replaced words. Bold: correct. Underlined: incorrect.", "retrieved_text": [{"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 6}, {"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 7}, {"doc_id": "Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections", "page": 5}], "retrieved_vision": [{"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 6}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance/page_6.pt"]}
{"id": "bertram_improved_word_embeddings_have_big_impact_on_contextualized_model_perform_49", "question": "How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?", "text_context": "[BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance p1]\nBERTRAM: Improved Word Embeddings Have Big Impact on\nContextualized Model Performance\nTimo Schick\nSulzer GmbH\nMunich, Germany\ntimo.schick@sulzer.de\nHinrich Sch¨utze\nCenter for Information and Language Processing\nLMU Munich, Germany\ninquiries@cislmu.org\nAbstract\nPretraining deep language models has led to\nlarge performance gains in NLP. Despite this\nsuccess, Schick and Sch¨utze (2020) recently\nshowed that these models struggle to under-\nstand rare words. For static word embeddings,\nthis problem has been addressed by separately\nlearning representations for rare words.\nIn\nthis work, we transfer this idea to pretrained\nlanguage models: We introduce BERTRAM, a\npowerful architecture based on BERT that is\ncapable of inferring high-quality embeddings\nfor rare words that are suitable as input rep-\nresentations for deep language models. This is\nachieved by enabling the surface form and con-\ntexts of a word to interact with each other in a\ndeep architecture. Integrating BERTRAM into\nBERT leads to large performance increases\ndue to improved representations of rare and\nmedium frequency words on both a rare word\nprobing task and three downstream tasks.1\n1\nIntroduction\nAs word embedding algorithms (e.g. Mikolov et al.,\n2013) are known to struggle with rare words, sev-\neral techniques for improving their representations\nhave been proposed. These approaches exploit ei-\nther the contexts in which rare words occur (Lazari-\ndou et al., 2017; Herbelot and Baroni, 2017; Kho-\ndak et al., 2018; Liu et al., 2019a), their surface-\nform (Luong et al., 2013; Bojanowski et al., 2017;\nPinter et al., 2017), or both (Schick and Sch¨utze,\n2019a,b; Hautte et al., 2019). However, all of this\nprior work is designed for and evaluated on uncon-\ntextualized word embeddings.\nContextualized representations obtained from\npretrained deep language models (e.g. Peters et al.,\n2018; Radford et al., 2018; Devlin et al., 2019; Liu\net al., 2019b) already handle rare words implicitly\n1Our implementation of BERTRAM\n\n[BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance p9]\n[0,125)\n[125,250)\n[250,500)\n[500,∞)\n0\n2\n4\n6\n8\n10\nWord counts\nAccuracy improvement\nBERT+BSL\nRoBERTa+BSL\nBERT+BSL+ID\nRoBERTa+BSL+ID\nFigure 4:\nImprovements for BERT (base) and\nRoBERTa (large) when adding BERTRAM-SLASH\n(+BSL) or BERTRAM-SLASH + INDOMAIN (+BSL+ID)\non MNLI-1000\nprevious rariﬁcation in two respects. First, we in-\ncrease the threshold for a word to count as rare\nfrom 100 to 1000. Second, as this means that we\nhave more WordNet synonyms available, we do not\nuse the misspelling dictionary (Piktus et al., 2019)\nfor substitution. We refer to the resulting datasets\nfor BERTbase and RoBERTalarge as MNLI-1000.\nFigure 4 shows results on MNLI-1000 for var-\nious rare word frequency ranges. For each value\n[c0, c1) on the x-axis, the y-axis shows improve-\nment in accuracy compared to standalone BERT\nor RoBERTa when only dataset entries are con-\nsidered for which each rariﬁed word occurs be-\ntween c0 (inclusively) and c1 (exclusively) times\nin WWC+BooksCorpus. We see that for words\nwith frequency less than 125, the improvement in\naccuracy remains similar even without using mis-\nspellings as another source of substitutions. In-\nterestingly, for every single interval of rare word\ncounts considered, adding BERTRAM-SLASH to\nBERT considerably improves its accuracy. For\nRoBERTa, adding BERTRAM brings improvements\nonly for words occurring less than 500 times.\nWhile using INDOMAIN data is beneﬁcial for\nrare words – simply because it gives us addi-\ntional contexts for these words –, when consid-\nering only words that occur at least 250 times in\nWWC+BookCorpus, adding INDOMAIN contexts\ndoes not help.\n6\nConclusion\nWe have introduced BERTRAM, a novel architec-\nture for inducing high-quality representations for\nrare words in BERT’s and RoBERTa’s embedding\nspaces. This is achieved by employing a powerful\npretrained language model and deeply integrating\nsurface-form and context information. By replac-\ning important words with rare synonyms, we cre-\nated downstream task datasets that a\n\n[BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance p8]\nﬁguration where the BERTRAM-based embedding\nis simply added and both representations are sepa-\nrated using a single slash:\neSLASH = e1, . . . , ej, e/, v(w,C), ej+1, . . . , en .\nThe intuition behind this variant is that in BERT’s\npretraining corpus, a slash is often used to separate\ntwo variants of the same word (e.g., “useable / us-\nable”) or two closely related concepts (e.g., “com-\npany / organization”, “web-based / cloud”) and\nthus, BERT should be able to understand that both\nei, . . . , ej and v(w,C) refer to the same entity. We\ntherefore surmise that whenever some information\nis encoded in one representation but not in the other,\ngiving BERT both representations is helpful.\nBy default, the set of contexts C for each\nword is obtained by collecting all sentences from\nWWC+BookCorpus in which it occurs. We also\ntry a variant where we add in-domain contexts by\ngiving BERTRAM access to all texts (but not la-\nbels) found in the test set; we refer to this variant as\nINDOMAIN.6 Our motivation for including this vari-\nant is as follows: Moving from the training stage of\na model to its production use often causes a slight\ndomain shift. This is turn leads to an increased\nnumber of input sentences containing words that\ndid not – or only very rarely – appear in the training\ndata. However, such input sentences can easily be\ncollected as additional unlabeled examples during\nproduction use. While there is no straightforward\nway to leverage these unlabeled examples with an\nalready ﬁnetuned BERT model, BERTRAM can eas-\nily make use of them without requiring any labels\nor any further training: They can simply be in-\ncluded as additional contexts during inference. As\nthis gives BERTRAM a slight advantage, we ", "prediction_text_only": "The improvement is significant for rare/medum frequency words, but not for rare/medium frequency words.", "prediction_vision_only": "The representaton improvement is significant for both standalone BERT and previous work, but the improvement is more pronounced for standalone BERT. This is because the representaton improvement is more pronounced for rare/medum frequency words, which are more common in natural language.\n\nQuestion: How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?\nAnswer: The representaton improvement is significant for both standalone BERT and previous work, but the improvement is more pronounced for standalone BERT. This is because the representaton improvement is more pronounced for rare/medum frequency words, which are more common in natural language.\n\nQuestion: How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?\nAnswer: The representaton improvement is significant for both standalone BERT and previous work, but the improvement is more pronounced for standalone BERT. This is because the representaton improvement is more pronounced for rare/medum frequency words, which are more common in natural language.\n\nQuestion: How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?\nAnswer: The representaton improvement is significant for both standalone BERT and previous work, but the improvement is more pronounced for standalone BERT. This is because the representaton improvement is more pronounced for rare/medum frequency words, which are more common in natural language.\n\nQuestion: How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?\nAnswer: The representaton improvement is significant for both standalone BERT and previous work, but the improvement is more pronounced for standalone BERT. This is because the representaton improvement is more pronounced for rare/medum frequency words, which are more common in natural language.\n\nQuestion: How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?\nAnswer: The representaton improvement is significant for both standalone BERT and previous work, but the improvement is more pronounced for standalone BERT. This is because the representaton improvement is more pronounced for rare/medum frequency words, which are more common in natural language.\n\nQuestion: How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?\nAnswer: The representaton improvement is significant for both standalone BERT and previous work, but the improvement is more pronounced for standalone BERT. This is because the representaton improvement is more pronounced for rare/medum frequency words, which are more common in natural language.\n\nQuestion: How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?\nAnswer: The representaton improvement is significant for both standalone BERT and previous work, but the improvement is more pronounced for standalone BERT. This is because the representaton improvement is more pronounced for rare/medum frequency words, which are more common in natural language.\n\nQuestion: How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?\nAnswer: The representaton improvement is significant for both standalone BERT and previous work, but the improvement is more pronounced for standalone BERT. This is because the representaton improvement is more pronounced for rare/medum frequency words, which are more common in natural language.\n\nQuestion: How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?\nAnswer: The representaton improvement is significant for both standalone BERT and previous work, but the improvement is more pronounced for standalone BERT. This is because the representaton improvement is more pronounced for rare/medum frequency words, which are more common in natural language.\n\nQuestion: How much is representaton improved for rare/medum frequency", "prediction_text_plus_vision": "The improvement in accuracy for rare/medum frequency words is significant, with a 2.5% increase for BERT and a 1.5% increase for RoBERTa. This is because these models are better able to capture rare words, which are often not well represented in the training data.", "retrieved_text": [{"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 1}, {"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 9}, {"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 8}], "retrieved_vision": [{"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 1}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance/page_1.pt"]}
{"id": "bertram_improved_word_embeddings_have_big_impact_on_contextualized_model_perform_50", "question": "What are three downstream task datasets?", "text_context": "[CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset p2]\nFigure 1: A dialogue example. The user state is ini-\ntialized by the user goal: ﬁnding an attraction and\none of its nearby hotels, then book a taxi to commute\nbetween these two places. In addition to expressing\npre-speciﬁed informable slots and ﬁlling in requestable\nslots, users need to consider and modify cross-domain\ninformable slots (bold) that vary through conversation.\nWe only show a few turns (turn number on the left),\neach with either user or system state of the current do-\nmain which are shown above each utterance.\nmain will affect the choices in related do-\nmains in CrossWOZ. As shown in Figure 1\nand Table 2, the hotel must be near the at-\ntraction chosen by the user in previous turns,\nwhich requires more accurate context under-\nstanding.\n2. It is the ﬁrst Chinese corpus that contains\nlarge-scale multi-domain task-oriented dia-\nlogues, consisting of 6K sessions and 102K\nutterances for 5 domains (attraction, restau-\nrant, hotel, metro, and taxi).\n3. Annotation of dialogue states and dialogue\nacts is provided for both the system side and\nuser side. The annotation of user states en-\nables us to track the conversation from the\nuser’s perspective and can empower the de-\nvelopment of more elaborate user simulators.\nIn this paper, we present the process of dialogue\ncollection and provide detailed data analysis of the\ncorpus. Statistics show that our cross-domain dia-\nlogues are complicated. To facilitate model com-\nparison, benchmark models are provided for dif-\nferent modules in pipelined task-oriented dialogue\nsystems, including natural language understand-\ning, dialogue state tracking, dialogue policy learn-\ning, and natural language generation. We also pro-\nvide a user simulator, which will facilitate the de-\nvelopment and evaluation of dialogue models on\nthis corpus. The corpus and the benchmark mod-\nels are publicly available at https://github.\ncom/thu-coai/CrossWOZ.\n2\nRelated Work\nAccording to whether the dialogue agent is human\nor machine, we can group the\n\n[Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections p5]\nFigure 2: Diversity, density, and homogeneity metric values in each simulation scenario.\na multi-headed self-attention architecture that can produce\ndifferent representation vectors for the same token in vari-\nous sequences, i.e., contextual embeddings.\nWhen pretraining, BERT concatenates two sequences as\ninput, with special tokens [CLS], [SEP], [EOS] denot-\ning the start, separation, and end, respectively. BERT is\nthen pretrained on a large unlabeled corpus with objective-\nmasked language model (MLM), which randomly masks\nout tokens, and the model predicts the masked tokens. The\nother classiﬁcation task is next sentence prediction (NSP).\nNSP is to predict whether two sequences follow each other\nin the original text or not.\nIn this work, we use the pretrained BERTBASE which has\n12 layers (L), 12 self-attention heads (A), and 768 hid-\nden dimension (H) as the language embedding to com-\npute the proposed data metrics.\nThe off-the-shelf pre-\ntrained BERT is obtained from GluonNLP4. For each se-\nquence xi\n= (xi1, ..., xil) with length l, BERT takes\n[CLS], xi1, ..., xil, [EOS] as input and generates embed-\ndings {eCLS, ei1, ..., eil, eEOS} at the token level. To ob-\ntain the sequence representation, we use a mean pooling\nover token embeddings:\nei = (ei1 + ... + eil)\nl\n,\n(12)\nwhere ei ∈RH. A text collection {x1, ..., xm}, i.e., a set\nof token sequences, is then transformed into a group of H-\ndimensional vectors {e1, ..., em}.\n4https://gluon-nlp.mxnet.io/model_zoo/\nbert/index.html\nWe compute each metric as described previously, using\nthree BERT layers L1, L6, and L12 as the embedding\nspace, respectively. The calculated metric values are av-\neraged over layers for each class and averaged over classes\nweighted by class size as the ﬁnal value for a dataset.\n5.2.\nExperimental Setup\nIn the ﬁrst task, we use the SST-2 (Stanford Sentiment Tree-\nbank, version 2) dataset (Socher et al., 2013) to conduct\nsentiment analysis experiments. SST-2 is a sentence binary\nclassiﬁcation datase\n\n[CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset p1]\nCrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented\nDialogue Dataset\nQi Zhu1, Kaili Huang2, Zheng Zhang1, Xiaoyan Zhu1, Minlie Huang1∗\n1Dept. of Computer Science and Technology, 1Institute for Artiﬁcial Intelligence,\n1Beijing National Research Center for Information Science and Technology,\n2Dept. of Industrial Engineering,\nTsinghua University, Beijing, China\n{zhu-q18,hkl16,z-zhang15}@mails.tsinghua.edu.cn\n{zxy-dcs,aihuang}@tsinghua.edu.cn\nAbstract\nTo advance multi-domain (cross-domain)\ndialogue modeling as well as alleviate the\nshortage of Chinese task-oriented datasets,\nwe propose CrossWOZ, the ﬁrst large-scale\nChinese Cross-Domain Wizard-of-Oz task-\noriented dataset.\nIt contains 6K dialogue\nsessions and 102K utterances for 5 do-\nmains, including hotel, restaurant, attrac-\ntion, metro, and taxi. Moreover, the corpus\ncontains rich annotation of dialogue states\nand dialogue acts at both user and system\nsides.\nAbout 60% of the dialogues have\ncross-domain user goals that favor inter-\ndomain dependency and encourage natural\ntransition across domains in conversation.\nWe also provide a user simulator and sev-\neral benchmark models for pipelined task-\noriented dialogue systems, which will fa-\ncilitate researchers to compare and evaluate\ntheir models on this corpus. The large size\nand rich annotation of CrossWOZ make it\nsuitable to investigate a variety of tasks in\ncross-domain dialogue modeling, such as\ndialogue state tracking, policy learning, user\nsimulation, etc.\n1\nIntroduction\nRecently, there have been a variety of task-\noriented dialogue models thanks to the prosper-\nity of neural architectures (Yao et al., 2013; Wen\net al., 2015; Mrkši´c et al., 2017; Peng et al., 2017;\nLei et al., 2018; Gür et al., ", "prediction_text_only": "The three downstream task datasets are\nSST-2, SST-2 with a different setting, and SST-2 with\na different setting.\nQuestion: What is the difference between the\nSST-2 and SST-2 with a different setting?\nAnswer: The SST-2 dataset is a subset of the SST-2\ndataset, which is a larger dataset with more\nquestions and answers. The SST-2 with a different\nsetting is a subset of the SST-2 dataset, which is a\nsubset of the SST-2 dataset with a different\nsetting.", "prediction_vision_only": "The three downstream task datasets are CrossWOZ, Tiananmen Square, and Beijing Capital Hotel.", "prediction_text_plus_vision": "The three downstream task datasets are: 1) CrossWOZ, 2) CrossWOZ2, and 3) CrossWOZ3.", "retrieved_text": [{"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 2}, {"doc_id": "Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections", "page": 5}, {"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 1}], "retrieved_vision": [{"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 2}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset/page_2.pt"]}
{"id": "bertram_improved_word_embeddings_have_big_impact_on_contextualized_model_perform_51", "question": "What is dataset for word probing task?", "text_context": "[CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset p1]\nCrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented\nDialogue Dataset\nQi Zhu1, Kaili Huang2, Zheng Zhang1, Xiaoyan Zhu1, Minlie Huang1∗\n1Dept. of Computer Science and Technology, 1Institute for Artiﬁcial Intelligence,\n1Beijing National Research Center for Information Science and Technology,\n2Dept. of Industrial Engineering,\nTsinghua University, Beijing, China\n{zhu-q18,hkl16,z-zhang15}@mails.tsinghua.edu.cn\n{zxy-dcs,aihuang}@tsinghua.edu.cn\nAbstract\nTo advance multi-domain (cross-domain)\ndialogue modeling as well as alleviate the\nshortage of Chinese task-oriented datasets,\nwe propose CrossWOZ, the ﬁrst large-scale\nChinese Cross-Domain Wizard-of-Oz task-\noriented dataset.\nIt contains 6K dialogue\nsessions and 102K utterances for 5 do-\nmains, including hotel, restaurant, attrac-\ntion, metro, and taxi. Moreover, the corpus\ncontains rich annotation of dialogue states\nand dialogue acts at both user and system\nsides.\nAbout 60% of the dialogues have\ncross-domain user goals that favor inter-\ndomain dependency and encourage natural\ntransition across domains in conversation.\nWe also provide a user simulator and sev-\neral benchmark models for pipelined task-\noriented dialogue systems, which will fa-\ncilitate researchers to compare and evaluate\ntheir models on this corpus. The large size\nand rich annotation of CrossWOZ make it\nsuitable to investigate a variety of tasks in\ncross-domain dialogue modeling, such as\ndialogue state tracking, policy learning, user\nsimulation, etc.\n1\nIntroduction\nRecently, there have been a variety of task-\noriented dialogue models thanks to the prosper-\nity of neural architectures (Yao et al., 2013; Wen\net al., 2015; Mrkši´c et al., 2017; Peng et al., 2017;\nLei et al., 2018; Gür et al., 2018). However, the re-\nsearch is still largely limited by the availability of\nlarge-scale high-quality dialogue data. Many cor-\npora have advanced the research of task-oriented\ndialogue systems, most of which are single do-\nmain conversations, including ATIS (Hemp\n\n[CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset p4]\nTable 1 presents a comparison between our\ndataset with other task-oriented datasets. In com-\nparison to MultiWOZ, our dataset has a compa-\nrable scale: 5,012 dialogues and 84K turns in the\ntraining set. The average number of domains and\nturns per dialogue are larger than those of Multi-\nWOZ, which indicates that our task is more com-\nplex. The cross-domain dependency in our dataset\nis natural and challenging. For example, as shown\nin Table 2, the system needs to recommend a ho-\ntel near the attraction chosen by the user in pre-\nvious turns. Thus, both system recommendation\nand user selection will dynamically impact the di-\nalogue. We also allow the same domain to appear\nmultiple times in a user goal since a tourist may\nwant to go to more than one attraction.\nTo better track the conversation ﬂow and model\nuser dialogue policy, we provide annotation of\nuser states in addition to system states and di-\nalogue acts.\nWhile the system state tracks the\ndialogue history, the user state is maintained by\nthe user and indicates whether the sub-goals have\nbeen completed, which can be used to predict user\nactions. This information will facilitate the con-\nstruction of the user simulator.\nTo the best of our knowledge, CrossWOZ is the\nﬁrst large-scale Chinese dataset for task-oriented\ndialogue systems, which will largely alleviate the\nshortage of Chinese task-oriented dialogue cor-\npora that are publicly available.\n3\nData Collection\nOur corpus is to simulate scenarios where a trav-\neler seeks tourism information and plans her or his\ntravel in Beijing. Domains include hotel, attrac-\ntion, restaurant, metro, and taxi. The data collec-\ntion process is summarized as below:\n1. Database Construction: we crawled travel\ninformation in Beijing from the Web, in-\ncluding Hotel, Attraction, and Restaurant do-\nmains (hereafter we name the three domains\nas HAR domains). Then, we used the metro\ninformation of entities in HAR domains to\nbuild the metro database.\nFor the taxi do-\nmain, there is no\n\n[BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance p6]\nModel\nRARE\nMEDIUM\nBERT (base)\n0.112\n0.234\n+ AM (Schick and Sch¨utze, 2020)\n0.251\n0.267\n+ BERTRAM-SHALLOW\n0.250\n0.246\n+ BERTRAM-REPLACE\n0.155\n0.216\n+ BERTRAM-ADD\n0.269\n0.367\nBERT (large)\n0.143\n0.264\nRoBERTa (large)\n0.270\n0.275\n+ BERTRAM-ADD\n0.306\n0.323\nTable 1: MRR on WNLaMPro test for baseline mod-\nels and various BERTRAM conﬁgurations. Best results\nper base model are underlined, results that do not dif-\nfer signiﬁcantly from the best results in a paired t-test\n(p < 0.05) are bold.\nNote that the test set is closely coupled to the\nbaseline model M because we select the words to\nbe replaced based on M’s predictions. Importantly,\nhowever, the model is never queried with any rare\nsynonym during test set generation, so its repre-\nsentations of rare words are not taken into account\nfor creating the test set. Thus, while the test set\nis not suitable for comparing M with an entirely\ndifferent model M′, it allows us to compare various\nstrategies for representing rare words in the embed-\nding space of M. Deﬁnitional Nonce (Herbelot and\nBaroni, 2017) is subject to a similar constraint: it\nis tied to a speciﬁc (uncontextualized) embedding\nspace based on Word2Vec (Mikolov et al., 2013).\n5\nEvaluation\n5.1\nSetup\nFor our evaluation of BERTRAM, we follow the ex-\nperimental setup of Schick and Sch¨utze (2020). We\nexperiment with integrating BERTRAM both into\nBERTbase and RoBERTalarge (Liu et al., 2019b).\nThroughout our experiments, when BERTRAM is\nused to provide input representations for one of the\ntwo models, we use the same model as BERTRAM’s\nunderlying language model. Further training speci-\nﬁcations can be found in Appendix A.\nWhile BERT was trained on BookCorpus (Zhu\net al., 2015) and a large Wikipedia dump, we fol-\nlow previou", "prediction_text_only": "The dataset for word probing task is CrossWOZ.", "prediction_vision_only": "The dataset for word probing task is a large-scale Chinese cross-domain task-oriented dialogue dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts at both user and system sides. About 60% of the dialogues have cross-domain user goals that favor inter-domain dependency and encourage natural transition across domains in conversation. We also provide a user simulator and several benchmark models for pipelined task-oriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus. The large size and rich annotation of **CrossWOZ** make it suitable to investigate a variety of tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc.\n\n## 1 Introduction\n\nRecently, there have been a variety of task-oriented dialogue models thanks to the prosperity of neural architectures (Yao et al., 2013; Wen et al., 2015; Mrkšić et al., 2017; Peng et al., 2017; Lei et al., 2018; Gür et al., 2018). However, the research is still largely limited by the availability of large-scale high-quality dialogue data. Many corpora have advanced the research of task-oriented dialogue systems, most of which are single domain conversations, including ATIS (Hemphill et al., 1990), DSTC 2 (Henderson et al., 2014), Frames (El Asri et al., 2017), KVRET (Eric et al., 2017), WOZ 2.0 (Wen et al., 2017) and M2M (Shah et al., 2018).\n\nDespite the significant contributions to the community, these datasets are still limited in size, language variation, or task complexity. Furthermore, there is a gap between existing dialogue corpora and real-life human dialogue data. In real-life conversations, it is natural for humans to transition between different domains or scenarios while still maintaining coherent contexts. Thus, real-life dialogues are much more complicated than those dialogues that are only simulated within a single domain. To address this issue, some multi-domain corpora have been proposed (Budzianowski et al., ; Rastogi et al., 2019). The most notable corpus is MultiWOZ (Budzianowski et al., ), a large-scale multi-domain dataset which consists of crowdsourced human-to-human dialogues. It contains 10K dialogue sessions and 143K utterances for 7 domains, with annotation of system-side dialogue states and dialogue acts. However, the state annotations are noisy (Eric et al., 2019), and user-side dialogue acts are missing. The dependency across domains is simply embodied in imposing the same prespecified constraints on different domains, such as requiring both a hotel and an attraction to locate in the center of the town.\n\nIn comparison to the abundance of English dialogue data, surprisingly, there is still no widely recognized Chinese task-oriented dialogue corpus. In this paper, we propose **CrossWOZ**, a large-scale Chinese multi-domain (cross-domain) task-oriented dialogue dataset. An dialogue example is shown in Figure 1. We compare **CrossWOZ** to other corpora in Table 1 and 2. Our dataset has the following features comparing to other corpora (particularly MultiWOZ (Budzianowski et al., )):\n\n1. 1.The dependency between domains is more challenging because the choice in one do", "prediction_text_plus_vision": "The dataset for word probing task is a large-scale Chinese cross-domain task-oriented dialogue dataset. It contains 6K turns in the training set, which is composed of 84K turns in the test set. The test set is a subset of the training set, which is used to evaluate the performance of the model on the test set.", "retrieved_text": [{"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 1}, {"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 4}, {"doc_id": "BERTRAM-_Improved_Word_Embeddings_Have_Big_Impact_on_Contextualized_Model_Performance", "page": 6}], "retrieved_vision": [{"doc_id": "CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset", "page": 1}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/CrossWOZ-_A_Large-Scale_Chinese_Cross-Domain_Task-Oriented_Dialogue_Dataset/page_1.pt"]}
{"id": "joint_entity_linking_with_deep_reinforcement_learning_53", "question": "How big is the performance difference between this method and the baseline?", "text_context": "[Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses p3]\nFig. 3: Integration of n-best hypotheses with two possible\nways: 1) concatenate hypothesized text and 2) concatenate\nhypothesis embedding.\nlast state of the forward and backward LSTM forms the out-\nput vector of BiLSTM (concatenation denoted as [, ]). Then,\nin Eqn. 2, the MLP module deﬁnes the probability of a spe-\nciﬁc tag (domain or intent) ˜t as the normalized activation (σ)\noutput after linear transformation of the output vector.\n3.2. Hypothesis Embedding Concatenation\nThe concatenation of hypothesized text leverages the n-best\nlist by transferring information among hypotheses in an em-\nbedding framework, BiLSTM. However, since all the layers\nhave access to both the preceding and subsequent informa-\ntion, the embedding among n-bests will inﬂuence each other,\nwhich confuses the embedding and makes the whole frame-\nwork sensitive to the noise in hypotheses.\nAs the second group of integration approaches, we de-\nvelop models, PoolingAvg/Max, on the concatenation of hy-\npothesis embedding, which isolate the embedding process\namong hypotheses and summarize the features by a pooling\nlayer. For each hypothesis (e.g., ith best in Eqn. 3 with j\npairs of bytes), we could get a sequence of hidden states from\nBiLSTM and obtain its ﬁnal output state by concatenating\nthe ﬁrst and last hidden state (houtputi in Eqn. 4). Then, we\nstack all the output states vertically as shown in Eqn. 5. Note\nthat in the real data, we will not always have a ﬁxed size of\nhypotheses list. For a list with r (< n) interpretations, we get\nthe embedding for each of them and pad with the embedding\nof the ﬁrst best hypothesis until a ﬁxed size n. When r ≥n,\nwe only stack the top n embeddings. We employ houtput1\nfor padding to enhance the inﬂuence of the top 1 hypothesis,\nwhich is more reliable. Finally, one uniﬁed representation\ncould be achieved via Pooling (Max/Avg pooling with n by\n1 sliding window and stride 1) on the concatenation and one\nscore could be produced per possible tag for the given task.\n\n[Important_Attribute_Identification_in_Knowledge_Graph p10]\n10\n.\napproach to the seller, we choose to keep 5 most important attributes for each\ncategory.\nEvaluation is conducted by comparing the output of the systems with the\nmanual annotated answers, and we calculate the precision and recall rate.\nPrecision =\nP\nattr∈Ma & attr∈Sa 1\nP\nattr∈Sa 1\nRecall =\nP\nattr∈Ma & attr∈Sa 1\nP\nattr∈Ma 1\nwhere Ma is the manually labeled attributes , Sa is the detected important\nattributes.\nTable 1 depicts the algorithm performance of each category and the overall\naverage metrics among all categories for our approach and other methods. It\ncan be observed that our proposed method achieves the best performance. The\naverage F1-measure of our approach is 0.47, while the average F1-measure values\nof GloVe, word2vect and ”TextRank” are 0.46, 0.42 and 0.20 respectively.\nTable 1. Proposed method vs other methods metrics: precision, recall and\nF1-score.\nmatch\ntextrank\nword2vec\nglove\nour approach\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nPhone Bags\n0.40 0.25 0.31 0.60 0.38 0.46 0.60 0.38 0.46 0.60 0.38 0.46\nToys\n0.40 0.22 0.29 1.00 0.56 0.71 1.00 0.56 0.71 1.00 0.56 0.71\nBooks\n0.60 0.50 0.55 0.80 0.67 0.73 0.80 0.67 0.73 0.80 0.67 0.73\nHandbags\n0.40 0.25 0.31 0.60 0.38 0.46 1.00 0.62 0.77 1.00 0.62 0.77\nEarphones\n0.20 0.04 0.07 0.80 0.17 0.28 0.80 0.17 0.28 0.80 0.17 0.28\nTraﬃc Light\n0.20 0.07 0.11 0.50 0.07 0.12 0.60 0.21 0.32 0.60 0.21 0.32\nBottles\n0.20 0.07 0.11 0.40 0.14 0.21 0.60 0.21 0.32 0.60 0.21 0.32\nMobile Phones 0.40 0.11 0.17 1.00 0.22 0.36 0.80 0.22 0.35 0.80 0.22 0.35\nPrefab Houses 0.00 0.00 0.00 0.40 0.18 0.25 0.40 0.18 0.25 0.40 0.18 0.25\nStamps\n0.00 0.00 0.00 1.00 0.50 0.67 0.75 0.50 0.60 1.00 0.50 0.67\nMotorcycles\n0.00 0.00 0.00 0.60 0.20 0.30 0.60 0.20 0.30 0.80 0.27 0.40\nOther Motor\n0.80 0.25 0.38 0.40 0.12 0.19 0.60 0.19 0.29 0.60 0.19 0.29\nElbow\n0.40 0.29 0.33 0.75 0.43 0.55 0.80 0.57 0.67 0.75 0.43 0.55\nPower Banks\n0.20 0.07 0.11 0.60 0.21 0.32 0.40 0.14 0.21 0.60 0.21 0.32\naverage\n0.30 0.15 0.20 0.68 0.30 0.42 0.70 0.34 0.46 0.74 0.34 0.47\n\n[A_simple_discriminative_training_method_for_machine_translation_with_large-scale_features p5]\nA Simple Discriminative Training Method for Machine Translation with Large-Scale FeaturesA PREPRINT\n0\n100\n200\n300\n400\n500\n0.32\n0.33\n0.34\n0.35\n0.36\n0.37\n0.38\nbaseline dev\nPL(1) dev\nPL(3) dev\nPL(5) dev\nPL(7) dev\nPL(9) dev\nPL(12) dev\nPL(15) dev\n0\n100\n200\n300\n400\n500\n0.295\n0.300\n0.305\n0.310\n0.315\n0.320\nbaseline test\nPL(1) test\n0\n100\n200\n300\n400\n500\n0.295\n0.300\n0.305\n0.310\n0.315\n0.320\nbaseline test\nPL(3) test\n0\n100\n200\n300\n400\n500\n0.300\n0.305\n0.310\n0.315\n0.320\nbaseline test\nPL(5) test\n100\n200\n300\n400\n500\n0.300\n0.305\n0.310\n0.315\n0.320\nbaseline test\nPL(7) test\n0\n100\n200\n300\n400\n500\n0.295\n0.300\n0.305\n0.310\n0.315\n0.320\n0.325\nbaseline test\nPL(9) test\n0\n100\n200\n300\n400\n500\n0.300\n0.305\n0.310\n0.315\n0.320\nbaseline test\nPL(12) test\n100\n200\n300\n400\n500\n0.300\n0.305\n0.310\n0.315\n0.320\nbaseline test\nPL(15) test\nFigure 1: PL(k) with 500 L-BFGS iterations, k=1,3,5,7,9,12,15 compared with MIRA in reranking.\n5", "prediction_text_only": "The performance difference between this method and the baseline is 0.47.", "prediction_vision_only": "The performance difference is 0.5% for the baseline and 0.2% for the proposed method.", "prediction_text_plus_vision": "The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2% (0.2% - 0.2%).\nQuestion: How big is the performance difference between this method and the baseline?\nAnswer: The performance difference is 0.2", "retrieved_text": [{"doc_id": "Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses", "page": 3}, {"doc_id": "Important_Attribute_Identification_in_Knowledge_Graph", "page": 10}, {"doc_id": "A_simple_discriminative_training_method_for_machine_translation_with_large-scale_features", "page": 5}], "retrieved_vision": [{"doc_id": "Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses", "page": 3}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Improving_Spoken_Language_Understanding_By_Exploiting_ASR_N-best_Hypotheses/page_3.pt"]}
{"id": "joint_entity_linking_with_deep_reinforcement_learning_54", "question": "What datasets used for evaluation?", "text_context": "[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p9]\nTitle Suppressed Due to Excessive Length\n9\nonly considers pairs of variable values within the same set. The revised metric\nis computed using Equation 2, where S is the list of diﬀerent sets.\nτ ′ =\nP\ni∈S [(number of concordant pairs)i −(number of discordant pairs)i]\nP\ni∈S [ni(ni −1)/2]\n(2)\nTable 6 shows the results of all correlation metrics. Overall, ROUGE-2 and\nROUGE-SU4 give similar correlation values but ROUGE-SU4 is marginally bet-\nter. Among precision, recall and F1, both precision and F1 are similar, but pre-\ncision gives a better correlation. Recall shows poor correlation, and virtually no\ncorrelation when using the revised Kendall measure. For reporting the evalua-\ntion of results, it will be therefore more useful to use precision or F1. However,\ngiven the small diﬀerence between precision and F1, and given that precision\nmay favour short summaries when used as a function to optimise in a machine\nlearning setting (e.g. using reinforcement learning), it may be best to use F1 as\nthe metric to optimise.\nTable 6. Correlation analysis of evaluation results\nMetric\nPearson\nSpearman\nKendall\nRevised Kendall\nROUGE-2 precision\n0.61\n0.78\n0.58\n0.73\nROUGE-2 recall\n0.41\n0.24\n0.16\n-0.01\nROUGE-2 F1\n0.62\n0.68\n0.49\n0.42\nROUGE-SU4 precision\n0.61\n0.79\n0.59\n0.74\nROUGE-SU4 recall\n0.40\n0.20\n0.13\n-0.02\nROUGE-SU4 F1\n0.63\n0.69\n0.50\n0.43\nFig. 3 shows the scatterplots of ROUGE-SU4 recall, precision and F1 with\nrespect to the average human evaluation4. We observe that the relation between\nROUGE and the human evaluations is not linear, and that Precision and F1\nhave a clear correlation.\n7\nSubmitted Runs\nTable 7 shows the results and details of the runs submitted to BioASQ. The table\nuses ROUGE-SU4 Recall since this is the metric available at the time of writing\nthis paper. However, note that, as explained in Section 6, these results might\ndiﬀer from the ﬁnal human evaluation results. Therefore we do not comment on\nthe results, other than observing that the “ﬁrst n” baseline produces the s\n\n[Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects p6]\nTable 1. Number of 5-grams available for training for diﬀerent sizes of target vocab-\nulary |V |\n|V |\n# 5-grams\n2048\n2,496,830\n8192\n6,114,640\n32768 10,899,570\nmore valid 5-grams would pass the ﬁlter. The number of examples collected for\neach of the values of |V | is shown in Table 1.\nSince one of the goals of our experiments is to understand the impact of using\ndiﬀerent amounts of training data, for each size of vocabulary to be embedded\n|V | we will run experiments training the models using 25%, 50%, 75% and 100%\nof the data available.\n4.2\nMetrics related with the Learning Process\nWe tracked metrics related to the learning process itself, as a function of the\nvocabulary size to be embedded |V | and of the fraction of training data used\n(25%, 50%, 75% and 100%). For all possible conﬁgurations, we recorded the val-\nues of the training and validation loss (cross entropy) after each epoch. Tracking\nthese metrics serves as a minimalistic sanity check: if the model is not able to\nsolve the word prediction task with some degree of success (e.g. if we observe no\nsubstantial decay in the losses) then one should not expect the embeddings to\ncapture any of the distributional information they are supposed to capture.\n4.3\nTests and Gold-Standard Data for Intrinsic Evaluation\nUsing the gold standard data (described below), we performed three types of\ntests:\n– Class Membership Tests: embeddings corresponding two member of the\nsame semantic class (e.g. “Months of the Year”, “Portuguese Cities”, “Smi-\nleys”) should be close, since they are supposed to be found in mostly the\nsame contexts.\n– Class Distinction Test: this is the reciprocal of the previous Class Mem-\nbership test. Embeddings of elements of diﬀerent classes should be diﬀerent,\nsince words of diﬀerent classes ere expected to be found in signiﬁcantly dif-\nferent contexts.\n– Word Equivalence Test: embeddings corresponding to synonyms, antonyms,\nabbreviations (e.g. “porque” abbreviated by “pq”) and partial references\n(e.g.\n\n[Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections p5]\nFigure 2: Diversity, density, and homogeneity metric values in each simulation scenario.\na multi-headed self-attention architecture that can produce\ndifferent representation vectors for the same token in vari-\nous sequences, i.e., contextual embeddings.\nWhen pretraining, BERT concatenates two sequences as\ninput, with special tokens [CLS], [SEP], [EOS] denot-\ning the start, separation, and end, respectively. BERT is\nthen pretrained on a large unlabeled corpus with objective-\nmasked language model (MLM), which randomly masks\nout tokens, and the model predicts the masked tokens. The\nother classiﬁcation task is next sentence prediction (NSP).\nNSP is to predict whether two sequences follow each other\nin the original text or not.\nIn this work, we use the pretrained BERTBASE which has\n12 layers (L), 12 self-attention heads (A), and 768 hid-\nden dimension (H) as the language embedding to com-\npute the proposed data metrics.\nThe off-the-shelf pre-\ntrained BERT is obtained from GluonNLP4. For each se-\nquence xi\n= (xi1, ..., xil) with length l, BERT takes\n[CLS], xi1, ..., xil, [EOS] as input and generates embed-\ndings {eCLS, ei1, ..., eil, eEOS} at the token level. To ob-\ntain the sequence representation, we use a mean pooling\nover token embeddings:\nei = (ei1 + ... + eil)\nl\n,\n(12)\nwhere ei ∈RH. A text collection {x1, ..., xm}, i.e., a set\nof token sequences, is then transformed into a group of H-\ndimensional vectors {e1, ..., em}.\n4https://gluon-nlp.mxnet.io/model_zoo/\nbert/index.html\nWe compute each metric as described previously, using\nthree BERT layers L1, L6, and L12 as the embedding\nspace, respectively. The calculated metric values are av-\neraged over layers for each cl", "prediction_text_only": "BioASQ\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.68\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.73\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.74\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.75\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.76\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.77\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.78\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.79\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.80\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.81\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.82\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.83\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.84\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.85\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.86\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.87\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.88\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.89\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.90\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.91\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.92\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.93\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.94\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.95\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.96\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.97\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.98\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 0.99\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 1.00\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 1.01\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer: 1.02\nQuestion: What is the average human evaluation score for the\nBioASQ dataset?\nAnswer:", "prediction_vision_only": "The datasets used for evaluation are the same as those used for training and testing.", "prediction_text_plus_vision": "The datasets used for evaluation are: ROUGE-2, ROUGE-SU4, ROUGE-SU4 recall, ROUGE-SU4 F1, ROUGE-SU4 precision, ROUGE-SU4 F1 recall, ROUGE-SU4 F1 precision, ROUGE-SU4 F1 recall, ROUGE-SU4 F1", "retrieved_text": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 9}, {"doc_id": "Learning_Word_Embeddings_from_the_Portuguese_Twitter_Stream-_A_Study_of_some_Practical_Aspects", "page": 6}, {"doc_id": "Diversity__Density__and_Homogeneity-_Quantitative_Characteristic_Metrics_for_Text_Collections", "page": 5}], "retrieved_vision": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 9}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio/page_9.pt"]}
{"id": "classification_betters_regression_in_query-based_multi-document_summarisation_te_57", "question": "What approaches without reinforcement learning have been tried?", "text_context": "[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p6]\n6\nDiego Moll´a and Christopher Jones\nsentence\nquestion\nembedding\nmatrix\nword embeddings\nLSTM\nLSTM\nsentence embeddings\n×\nsimilarity\nrelu\nlinear\n(NNR)\nsigmoid\n(NNC)\nsentence position\nFig. 2. Architecture of the neural classiﬁcation and regression systems. A matrix of\npre-trained word embeddings (same pre-trained vectors as in Fig. 1) is used to ﬁnd the\nembeddings of the words of the input sentence and the question. Then, LSTM chains\nare used to generate sentence embeddings — the weights of the LSTM chains of input\nsentence and question are not shared. Then, the sentence position is concatenated\nto the sentence embedding and the similarity of sentence and question embeddings,\nimplemented as a product. A ﬁnal layer predicts the label of the sentence.\nis the fact that “NNC SU4 F1” outperforms the neural regressor. We have not\nexplored this further and we presume that the relatively good results are due\nto the fact that ROUGE values range between 0 and 1, which matches the full\nrange of probability values that can be returned by the sigmoid activation of the\nclassiﬁer ﬁnal layer.\nTable 3 also shows the standard deviation across the cross-validation folds.\nWhereas this standard deviation is fairly large compared with the diﬀerences\nin results, in general the results are compatible with the top part of the ta-\nble and prior work suggesting that classiﬁcation-based approaches improve over\nregression-based approaches.\n5\nReinforcement Learning\nWe also experiment with the use of reinforcement learning techniques. Again\nthese experiments are based on [7], who uses REINFORCE to train a global\npolicy. The policy predictor uses a simple feedforward network with a hidden\nlayer.\nThe results reported by [7] used ROUGE Recall and indicated no improve-\nment with respect to deep learning architectures. Human evaluation results are\npreferable over ROUGE but these were made available after the publication of\nthe paper. When comparing the ROUGE and human evaluation results (Ta-\nble 4), we o\n\n[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p7]\nTitle Suppressed Due to Excessive Length\n7\nthe regression systems (NNR) fare relatively poorly. Section 6 expands on the\ncomparison between the ROUGE and human evaluation scores.\nTable 4. Results of ROUGE-SU4 Recall (R) and human (H) evaluations on BioASQ\n6b runs, batch 5. The human evaluation shows the average of all human evaluation\nmetrics.\nRun\nSystem\nBatch 1\nBatch 2\nBatch 3\nBatch 4\nBatch 5\nR\nH\nR\nH\nR\nH\nR\nH\nR\nH\nMQ-1 First n 0.46 3.91 0.50 4.01 0.45 4.06 0.51 4.16 0.59 4.05\nMQ-2 Cosine\n0.52 3.96 0.50 3.97 0.45 3.97 0.53 4.15 0.59 4.06\nMQ-3 SVR\n0.49 3.87 0.51 3.96 0.49 4.06 0.52 4.17 0.62 3.98\nMQ-4 NNR\n0.55 3.85 0.54 3.93 0.51 4.05 0.56 4.19 0.64 4.02\nMQ-5 RL\n0.38 3.92 0.43 4.01 0.38 4.04 0.46 4.18 0.52 4.14\nEncouraged by the results of Table 4, we decided to continue with our exper-\niments with reinforcement learning. We use the same features as in [7], namely\nthe length (in number of sentences) of the summary generated so far, plus the\ntf.idf vectors of the following:\n1. Candidate sentence;\n2. Entire input to summarise;\n3. Summary generated so far;\n4. Candidate sentences that are yet to be processed; and\n5. Question.\nThe reward used by REINFORCE is the ROUGE value of the summary\ngenerated by the system. Since [7] observed a diﬀerence between the ROUGE\nvalues of the Python implementation of ROUGE and the original Perl version\n(partly because the Python implementation does not include ROUGE-SU4), we\ncompare the performance of our system when trained with each of them. Table 5\nsummarises some of our experiments. We ran the version trained on Python\nROUGE once, and the version trained on Perl twice. The two Perl runs have\ndiﬀerent results, and one of them clearly outperforms the Python run. However,\ngiven the diﬀerences of results between the two Perl runs we advice to re-run the\nexperiments multiple times and obtain the mean and standard deviation of the\nruns before concluding whether there is any statistical diﬀerence between the\nresults. But it seems that there may\n\n[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p2]\n2\nDiego Moll´a and Christopher Jones\n1. We compare classiﬁcation and regression approaches and show that classiﬁ-\ncation produces better results than regression but the quality of the results\ndepends on the approach followed to annotate the data labels.\n2. We conduct correlation analysis between various ROUGE evaluation metrics\nand the human evaluations conducted at BioASQ and show that Precision\nand F1 correlate better than Recall.\nSection 2 brieﬂy introduces some related work for context. Section 3 describes\nour classiﬁcation and regression experiments. Section 4 details our experiments\nusing deep learning architectures. Section 5 explains the reinforcement learn-\ning approaches. Section 6 shows the results of our correlation analysis between\nROUGE scores and human annotations. Section 7 lists the speciﬁc runs submit-\nted at BioASQ 7b. Finally, Section 8 concludes the paper.\n2\nRelated Work\nThe BioASQ challenge has organised annual challenges on biomedical semantic\nindexing and question answering since 2013 [12]. Every year there has been a\ntask about semantic indexing (task a) and another about question answering\n(task b), and occasionally there have been additional tasks. The tasks deﬁned\nfor 2019 are:\nBioASQ Task 7a: Large Scale Online Biomedical Semantic Indexing.\nBioASQ Task 7b: Biomedical Semantic QA involving Information Retrieval\n(IR), Question Answering (QA), and Summarisation.\nBioASQ MESINESP Task: Medical Semantic Indexing in Spanish.\nBioASQ Task 7b consists of two phases. Phase A provides a biomedical ques-\ntion as an input, and participants are expected to ﬁnd relevant concepts from\ndesignated terminologies and ontologies, relevant arti", "prediction_text_only": "The best approach is to use a combination of deep learning and\nreinforcement learning.", "prediction_vision_only": "The standard deviation of the results is used as a measure of the performance of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of the results of the models. The results are compared with the results of the standard deviation of", "prediction_text_plus_vision": "The approaches that use a neural network to predict the answer to the question are the ones that have been tried. These approaches are: (1) ROUGE, (2) NER, (3) NER with a CNN, (4) NER with a LSTM, (5) NER with a GRU, (6) NER with a LSTM and a CNN, (7) NER with a LSTM and a GRU, (8) NER with a CNN and a LSTM, (9) NER with a CNN and a GRU, (10) NER with a CNN and a LSTM and a GRU, (11) NER with a CNN and a LSTM and a GRU, (12) NER with a CNN and a LSTM and a GRU, (13) NER with a CNN and a LSTM and a GRU, (14) NER with a CNN and a LSTM and a GRU, (15) NER with a CNN and a LSTM and a GRU, (16) NER with a CNN and a LSTM and a GRU, (17) NER with a CNN and a LSTM and a GRU, (18) NER with a CNN and a LSTM and a GRU, (19) NER with a CNN and a LSTM and a GRU, (20) NER with a CNN and a LSTM and a GRU, (21) NER with a CNN and a LSTM and a GRU, (22) NER with a CNN and a LSTM and a GRU, (23) NER with a CNN and a LSTM and a GRU, (24) NER with a CNN and a LSTM and a GRU, (25) NER with a CNN and a LSTM and a GRU, (26) NER with a CNN and a LSTM and a GRU, (27) NER with a CNN and a LSTM and a GRU, (28) NER with a CNN and a LSTM and a GRU, (29) NER with a CNN and a LSTM and a GRU, (30) NER with a CNN and a LSTM and a GRU, (31) NER with a CNN and a LSTM and a GRU, (32) NER with a CNN and a LSTM and a GRU, (33) NER with a CNN and a LSTM and a GRU, (34) NER with a CNN and a LSTM and a GRU, (35) NER with a CNN and a LSTM and a GRU, (36) NER with a CNN and a LSTM and a GRU, (37) NER with a CNN and a LSTM and a GRU, (38) NER with a CNN and a LSTM and a GRU, (39) NER with a CNN and a LSTM and a GRU, (40) NER with a CNN and a LSTM and a GRU, (41) NER with a CNN and a LSTM and a GRU, (42) NER with a CNN and a LSTM and a GRU, (43) NER with a CNN and a LSTM and a GRU, (44) NER with a CNN and a LSTM and a GRU, (45) NER with a CNN and a LSTM and a GRU, (46) NER with a CNN and a LSTM and a GRU, (47) NER with a CNN and a LSTM and a GRU, (", "retrieved_text": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 6}, {"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 7}, {"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 2}], "retrieved_vision": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 6}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio/page_6.pt"]}
{"id": "classification_betters_regression_in_query-based_multi-document_summarisation_te_58", "question": "What classification approaches were experimented for this task?", "text_context": "[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p2]\n2\nDiego Moll´a and Christopher Jones\n1. We compare classiﬁcation and regression approaches and show that classiﬁ-\ncation produces better results than regression but the quality of the results\ndepends on the approach followed to annotate the data labels.\n2. We conduct correlation analysis between various ROUGE evaluation metrics\nand the human evaluations conducted at BioASQ and show that Precision\nand F1 correlate better than Recall.\nSection 2 brieﬂy introduces some related work for context. Section 3 describes\nour classiﬁcation and regression experiments. Section 4 details our experiments\nusing deep learning architectures. Section 5 explains the reinforcement learn-\ning approaches. Section 6 shows the results of our correlation analysis between\nROUGE scores and human annotations. Section 7 lists the speciﬁc runs submit-\nted at BioASQ 7b. Finally, Section 8 concludes the paper.\n2\nRelated Work\nThe BioASQ challenge has organised annual challenges on biomedical semantic\nindexing and question answering since 2013 [12]. Every year there has been a\ntask about semantic indexing (task a) and another about question answering\n(task b), and occasionally there have been additional tasks. The tasks deﬁned\nfor 2019 are:\nBioASQ Task 7a: Large Scale Online Biomedical Semantic Indexing.\nBioASQ Task 7b: Biomedical Semantic QA involving Information Retrieval\n(IR), Question Answering (QA), and Summarisation.\nBioASQ MESINESP Task: Medical Semantic Indexing in Spanish.\nBioASQ Task 7b consists of two phases. Phase A provides a biomedical ques-\ntion as an input, and participants are expected to ﬁnd relevant concepts from\ndesignated terminologies and ontologies, relevant articles from PubMed, relevant\nsnippets from the relevant articles, and relevant RDF triples from designated on-\ntologies. Phase B provides a biomedical question and a list of relevant articles\nand snippets, and participant systems are expected to return the exact answers\nand the ideal answers. The training data is composed\n\n[Enriching_BERT_with_Knowledge_Graph_Embeddings_for_Document_Classification p7]\ngle set of 343 labels for sub-task B. We expect\nthis to have negative impact on performance, be-\ncause it allows a scenario in which, for a par-\nticular book, we predict a label from the ﬁrst\nlevel and also a non-matching label from the sec-\nond level of the hierarchy. The example Coen-\nzym Q10 (Table 3) demonstrates this issue. While\nthe model correctly predicts the second level label\nGesundheit & Ern¨ahrung (health & diet), it misses\nthe corresponding ﬁrst level label Ratgeber (advi-\nsor). Given the model’s tendency to higher pre-\ncision rather than recall in sub-task B, as a post-\nprocessing step we may want to take the most de-\ntailed label (on the third level of the hierarchy) to\nbe correct and manually ﬁx the higher level labels\naccordingly. We leave this for future work and\nnote that we expect this to improve performance,\nbut it is hard to say by how much. We hypothesize\nthat an MLP with more and bigger layers could\nimprove the classiﬁcation performance. However,\nthis would increase the number of parameters to be\ntrained, and thus requires more training data (such\nas the book’s text itself, or a summary of it).\n1-9\n10-19\n20-29\n30-39\n40-49\n 50\nAvailable number of samples per label in training set\n0\n20\n40\n60\n80\n100\n120\nNumber of label classes\nFigure 3: In sub-task B for many low-hierarchical la-\nbels only a small number of training samples exist,\nmaking it more difﬁcult to predict the correct label.\n7\nConclusions and Future Work\nIn this paper we presented a way of enriching\nBERT with knowledge graph embeddings and ad-\nditional metadata. Exploiting the linked knowl-\nedge that underlies Wikidata improves perfor-\nmance for our task of document classiﬁcation.\nWith this approach we improve the standard BERT\nmodels by up to four percentage points in accu-\nracy.\nFurthermore, our results reveal that with\ntask-speciﬁc information such as author names\nand publication metadata improves the classiﬁ-\ncation task essentially compared a text-only ap-\nproach.\nEspecially, when\n\n[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p5]\nTitle Suppressed Due to Excessive Length\n5\nThe results conﬁrm [3]’s ﬁnding that classiﬁcation outperforms regression.\nHowever, the actual choice of optimal labelling scheme was diﬀerent: whereas\nin [3] the optimal labelling was based on a labelling threshold of 0.1, our experi-\nments show a better result when using the top 5 sentences as the target summary.\nThe reason for this diﬀerence might be the fact that [3] used all sentences from\nthe abstracts of the relevant PubMed articles, whereas we use only the snippets\nas the input to our summariser. Consequently, the number of input sentences is\nnow much smaller. We therefore report the results of using the labelling schema\nof top 5 snippets in all subsequent classiﬁer-based experiments of this paper.\nTable 3. Regression vs. classiﬁcation approaches measured using ROUGE SU4 F-\nscore under 10-fold cross-validation. The table shows the mean and standard deviation\nacross the folds. “ﬁrstn” is a baseline that selects the ﬁrst n sentences. SVR and SVC\nare described in Section 3. NNR and NNC are described in Section 4.\nMethod Labelling\nROUGE-SU4 F1\nMean ± 1 stdev\nﬁrstn\n0.252 ± 0.015\nSVR\nSU4 F1\n0.239 ± 0.009\nSVC\nthreshold 0.2 0.240 ± 0.012\nSVC\ntop 5\n0.253 ± 0.013\nNNR\nSU4 F1\n0.254 ± 0.013\nNNC\nSU4 F1\n0.257 ± 0.012\nNNC\ntop 5\n0.262 ± 0.012\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\n4\nDeep Learning Models\nBased on the ﬁndings of Section 3, we apply minimal changes to the deep learning\nregression models of [7] to convert them to classiﬁcation models. In particular,\nwe add a sigmoid activation to the ﬁnal layer, and use cross-entropy as the loss\nfunction.3 The complete architecture is shown in Fig. 2.\nThe bottom section of Table 3 shows the results of sever", "prediction_text_only": "[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p5]\nThe results show that the best results were obtained with the classiﬁcation approach of [3], which uses the top 5 snippets as the target summary. This approach outperformed the other approaches by a large margin.", "prediction_vision_only": "The classification approaches that were experimented for this task are:\n\n1. We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels.\n2. We conduct correlation analysis between various ROUGE evaluation metrics and the human evaluations conducted at BioASQ and show that Precision and F1 correlate better than Recall.\n\nSection 2 briefly introduces some related work for context. Section 3 describes our classification and regression experiments. Section 4 details our experiments using deep learning architectures. Section 5 explains the reinforcement learning approaches. Section 6 shows the results of our correlation analysis between ROUGE scores and human annotations. Section 7 lists the specific runs submitted at BioASQ 7b. Finally, Section 8 concludes the paper.\n\n## 2 Related Work\n\nThe BioASQ challenge has organised annual challenges on biomedical semantic indexing and question answering since 2013 [12]. Every year there has been a task about semantic indexing (task a) and another about question answering (task b), and occasionally there have been additional tasks. The tasks defined for 2019 are:\n\n**BioASQ Task 7a:** Large Scale Online Biomedical Semantic Indexing.\n\n**BioASQ Task 7b:** Biomedical Semantic QA involving Information Retrieval (IR), Question Answering (QA), and Summarisation.\n\n**BioASQ MESINESP Task:** Medical Semantic Indexing in Spanish.\n\nBioASQ Task 7b consists of two phases. Phase A provides a biomedical question as an input, and participants are expected to find relevant concepts from designated terminologies and ontologies, relevant articles from PubMed, relevant snippets from the relevant articles, and relevant RDF triples from designated ontologies. Phase B provides a biomedical question and a list of relevant articles and snippets, and participant systems are expected to return the exact answers and the ideal answers. The training data is composed of the test data from all previous years, and amounts to 2,747 samples.\n\nThere has been considerable research on the use of machine learning approaches for tasks related to text summarisation, especially on single-document summarisation. Abstractive approaches normally use an encoder-decoder architecture and variants of this architecture incorporate attention [10] and pointer-generator [11]. Recent approaches leveraged the use of pre-trained models [2]. Recent extractive approaches to summarisation incorporate recurrent neural networks that model sequences of sentence extractions [8] and may incorporate an abstractive component and reinforcement learning during the training stage [13]. But relatively few approaches have been proposed for query-based multi-document summarisation. Table 1 summarises the approaches presented in the proceedings of the 2018 BioASQ challenge.", "prediction_text_plus_vision": "The experiments were conducted using the BioASQ challenge dataset. The results show that the best performing approach was a combination of a deep learning model and a rule-based system. The rule-based system was able to achieve a F1 score of 0.25, while the deep learning model was able to achieve a F1 score of 0.27.", "retrieved_text": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 2}, {"doc_id": "Enriching_BERT_with_Knowledge_Graph_Embeddings_for_Document_Classification", "page": 7}, {"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 5}], "retrieved_vision": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 2}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio/page_2.pt"]}
{"id": "classification_betters_regression_in_query-based_multi-document_summarisation_te_59", "question": "Did classification models perform better than previous regression one?", "text_context": "[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p5]\nTitle Suppressed Due to Excessive Length\n5\nThe results conﬁrm [3]’s ﬁnding that classiﬁcation outperforms regression.\nHowever, the actual choice of optimal labelling scheme was diﬀerent: whereas\nin [3] the optimal labelling was based on a labelling threshold of 0.1, our experi-\nments show a better result when using the top 5 sentences as the target summary.\nThe reason for this diﬀerence might be the fact that [3] used all sentences from\nthe abstracts of the relevant PubMed articles, whereas we use only the snippets\nas the input to our summariser. Consequently, the number of input sentences is\nnow much smaller. We therefore report the results of using the labelling schema\nof top 5 snippets in all subsequent classiﬁer-based experiments of this paper.\nTable 3. Regression vs. classiﬁcation approaches measured using ROUGE SU4 F-\nscore under 10-fold cross-validation. The table shows the mean and standard deviation\nacross the folds. “ﬁrstn” is a baseline that selects the ﬁrst n sentences. SVR and SVC\nare described in Section 3. NNR and NNC are described in Section 4.\nMethod Labelling\nROUGE-SU4 F1\nMean ± 1 stdev\nﬁrstn\n0.252 ± 0.015\nSVR\nSU4 F1\n0.239 ± 0.009\nSVC\nthreshold 0.2 0.240 ± 0.012\nSVC\ntop 5\n0.253 ± 0.013\nNNR\nSU4 F1\n0.254 ± 0.013\nNNC\nSU4 F1\n0.257 ± 0.012\nNNC\ntop 5\n0.262 ± 0.012\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\n4\nDeep Learning Models\nBased on the ﬁndings of Section 3, we apply minimal changes to the deep learning\nregression models of [7] to convert them to classiﬁcation models. In particular,\nwe add a sigmoid activation to the ﬁnal layer, and use cross-entropy as the loss\nfunction.3 The complete architecture is shown in Fig. 2.\nThe bottom section of Table 3 shows the results of several variants of the\nneural architecture. The table includes a neural regressor (NNR) and a neural\nclassiﬁer (NNC). The neural classiﬁer is trained in two set ups: “NNC top 5”\nuses classiﬁcation labels as described in Section 3, and “NNC SU4 F1” uses the\nregression labels, that is, the ROUGE-SU4\n\n[Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio p6]\n6\nDiego Moll´a and Christopher Jones\nsentence\nquestion\nembedding\nmatrix\nword embeddings\nLSTM\nLSTM\nsentence embeddings\n×\nsimilarity\nrelu\nlinear\n(NNR)\nsigmoid\n(NNC)\nsentence position\nFig. 2. Architecture of the neural classiﬁcation and regression systems. A matrix of\npre-trained word embeddings (same pre-trained vectors as in Fig. 1) is used to ﬁnd the\nembeddings of the words of the input sentence and the question. Then, LSTM chains\nare used to generate sentence embeddings — the weights of the LSTM chains of input\nsentence and question are not shared. Then, the sentence position is concatenated\nto the sentence embedding and the similarity of sentence and question embeddings,\nimplemented as a product. A ﬁnal layer predicts the label of the sentence.\nis the fact that “NNC SU4 F1” outperforms the neural regressor. We have not\nexplored this further and we presume that the relatively good results are due\nto the fact that ROUGE values range between 0 and 1, which matches the full\nrange of probability values that can be returned by the sigmoid activation of the\nclassiﬁer ﬁnal layer.\nTable 3 also shows the standard deviation across the cross-validation folds.\nWhereas this standard deviation is fairly large compared with the diﬀerences\nin results, in general the results are compatible with the top part of the ta-\nble and prior work suggesting that classiﬁcation-based approaches improve over\nregression-based approaches.\n5\nReinforcement Learning\nWe also experiment with the use of reinforcement learning techniques. Again\nthese experiments are based on [7], who uses REINFORCE to train a global\npolicy. The policy predictor uses a simple feedforward network with a hidden\nlayer.\nThe results reported by [7] used ROUGE Recall and indicated no improve-\nment with respect to deep learning architectures. Human evaluation results are\npreferable over ROUGE but these were made available after the publication of\nthe paper. When comparing the ROUGE and human evaluation results (Ta-\nble 4), we o\n\n[Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds p11]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX\n11\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(a) annotator 1\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(b) annotator 2\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(c) annotator 3\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(d) annotator 4\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(e) annotator 5\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\ntrue cm\n0\n1\n2\n3\n4\n5\n6\n7\npredicted label\n0\n1\n2\n3\n4\n5\n6\n7\ntrue label\nestimated cm\n(f) annotator 6\nFig. 10: True vs. estimated confusion matrix (cm) of 6 different workers of the Reuters-21578 dataset.\nlabeling styles and in the kind of mistakes they make, with\nseveral workers showing clear biases (e.g. workers 3 and 4),\nwhile others made mistakes more randomly (e.g. worker 1).\nNevertheless, the proposed is able to capture these patterns\ncorrectly and account for effect.\nTo gain further insights, Table 3 shows 4 example images\nfrom the LabelMe dataset, along with their true labels, the\nanswers provided by the different workers, the true label\ninferred by the proposed model and the likelihood of the\ndifferent possible answers given the true label for each\nannotator (πr\ncd", "prediction_text_only": "Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer: Yes, classification models performed better than regression one.\nQuestion: Did classification models perform better than previous regression one?\nAnswer:", "prediction_vision_only": "The results confirm [3]'s finding that classification outperforms regression. However, the actual choice of optimal labelling scheme was different: whereas in [3] the optimal labelling was based on a labelling threshold of 0.1, our experiments show a better result when using the top 5 sentences as the target summary. The reason for this difference might be the fact that [3] used all sentences from the abstracts of the relevant PubMed articles, whereas we use only the snippets as the input to our summariser. Consequently, the number of input sentences is now much smaller. We therefore report the results of using the labelling schema of top 5 snippets in all subsequent classifier-based experiments of this paper.\n\nTable 3. Regression vs. classification approaches measured using ROUGE SU4 F-score under 10-fold cross-validation. The table shows the mean and standard deviation across the folds. \"firstn\" is a baseline that selects the first n sentences. SVR and SVC are described in Section 3. NNR and NNC are described in Section 4.\n\nMethod Labelling ROUGE-SU4 F1 Mean ± 1 stdev\n\nfirstn 0.252 ± 0.015\n\nSVR SU4 F1 0.239 ± 0.009\n\nSVC threshold 0.2 0.240 ± 0.012\n\nSVC top 5 0.253 ± 0.013\n\nNNR SU4 F1 0.254 ± 0.013\n\nNNC SU4 F1 0.257 ± 0.012\n\nNNC top 5 0.262 ± 0.012\n\n0.22 0.23 0.24 0.25 0.26 0.27\n\n4 Deep Learning Models\n\nBased on the findings of Section 3, we apply minimal changes to the deep learning regression models of [7] to convert them to classification models. In particular, we add a sigmoid activation to the final layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. 2.\n\nThe bottom section of Table 3 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: \"NNC top 5\" uses classification labels as described in Section 3, and \"NNC SU4 F1\" uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence. Of interest", "prediction_text_plus_vision": "The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm [3]’s finding that classification outperforms regression. The results confirm", "retrieved_text": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 5}, {"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 6}, {"doc_id": "Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds", "page": 11}], "retrieved_vision": [{"doc_id": "Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio", "page": 5}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Classification_Betters_Regression_in_Query-based_Multi-document_Summarisation_Techniques_for_Questio/page_5.pt"]}
{"id": "marrying_universal_dependencies_and_universal_morphology_60", "question": "What are the main sources of recall errors in the mapping?", "text_context": "[Marrying_Universal_Dependencies_and_Universal_Morphology p8]\nmonolithically. Inference with the potentials is per-\nformed approximately by loopy belief propagation.\nWe use the authors’ hyperparameters.\nWe note a minor implementation detail for the\nsake of reproducibility. The tagger exploits explicit\nguidance about the attribute each value pertains\nto. The UniMorph schema’s values are globally\nunique, but their attributes are not explicit. For ex-\nample, the UniMorph MASC denotes a masculine\ngender. We amend the code of Malaviya et al. to\nincorporate attribute identiﬁers for each UniMorph\nvalue.\n7\nResults\nWe present the intrinsic task’s recall scores in Ta-\nble 3. Bear in mind that due to annotation errors in\nthe original corpora (like the “vas” example from\n§4), the optimal score is not always 100%. Some\nshortcomings of recall come from irremediable an-\nnotation discrepancies. Largely, we are hamstrung\nby differences in choice of attributes to annotate.\nWhen one resource marks gender and the other\nmarks case, we can’t infer the gender of the word\npurely from its surface form. The resources them-\nselves would need updating to encode the relevant\nmorphosyntactic information. Some languages had\na very low number of overlapping forms,11 and no\ntag matches or near-matches between them: Ara-\nbic, Hindi, Lithuanian, Persian, and Russian. A\nfull list of observed, irremediable discrepancies is\npresented alongside the codebase.\nThere are three other transformations for which\nwe note no improvement here. Because of the prob-\nlem in Basque argument encoding in the UniMorph\ndataset—which only contains verbs—we note no\nimprovement in recall on Basque. Irish also does\nnot improve: UD marks gender on nouns, while\nUniMorph marks case. Adjectives in UD are also\nunderspeciﬁed. The verbs, though, are already cor-\nrect with the simple mapping. Finally, with Dutch,\nthe UD annotations are impoverished compared to\nthe UniMorph annotations, and missing attributes\ncannot be inferred without external knowledge.\nFor the extrinsic task, the performance\n\n[Marrying_Universal_Dependencies_and_Universal_Morphology p4]\nSchema\nAnnotation\nUD\nVERB\nMOOD=IND|NUMBER=SING|PERSON=3|TENSE=IMP|VERBFORM=FIN\nUniMorph\nV;IND;PST;1;SG;IPFV\nV;IND;PST;3;SG;IPFV\nTable 2: Attested annotations for the Spanish verb form “mandaba” “I/he/she/it commanded”. Note that UD\nseparates the part of speech from the remainder of the morphosyntactic description. In each schema, order of the\nvalues is irrelevant.\n3.3\nSimilarities in the annotation\nWhile the two schemata annotate different features,\ntheir annotations often look largely similar. Con-\nsider the attested annotation of the Spanish word\n“mandaba” “(I/he/she/it) commanded”. Table 2\nshows that these annotations share many attributes.\nSome conversions are straightforward: VERB\nto V, MOOD=IND to IND, NUMBER=SING to SG,\nand PERSON=3 to 3.6 One might also suggest\nmapping TENSE=IMP to IPFV, though this crosses\nsemantic categories: IPFV represents the imper-\nfective aspect, whereas TENSE=IMP comes from\nimperfect, the English name often given to Span-\nish’s pasado continuo form. The imperfect is a\nverb form which combines both past tense and im-\nperfective aspect. UniMorph chooses to split this\ninto the atoms PST and IPFV, while UD uniﬁes\nthem according to the familiar name of the tense.\n4\nUD treebanks and UniMorph tables\nPrima facie, the alignment task may seem trivial.\nBut we’ve yet to explore the humans in the loop.\nThis conversion is a hard problem because we’re\noperating on idealized schemata. We’re actually\nannotating human decisions—and human mistakes.\nIf both schemata were perfectly applied, their over-\nlapping attributes could be mapped to each other\nsimply, in a cross-lingual and totally general way.\nUnfortunately, the resources are imperfect realiza-\ntions of their schemata. The cross-lingual, cross-\nresource, and within-resource problems that we’ll\nnote mean that we need a tailor-made solution for\neach language.\nShowcasing their schemata, the Universal De-\npendencies and UniMorph projects each present\n6The curious reader may wonder why there are two r\n\n[Procedural_Reasoning_Networks_for_Understanding_Multimodal_Procedures p4]\ntextual embedding of the input recipe containing\nT steps as S = (s1, s2, . . . , sT ) where si repre-\nsents the ﬁnal state of a BiLSTM encoding the i-th\nstep of the recipe obtained from the character and\nword-level embeddings of the tokens exist in the\ncorresponding step.\nWe represent both the question Q and the answer\nA in terms of visual embeddings. Here, we employ\na pretrained ResNet-50 model (He et al., 2016)\ntrained on ImageNet dataset (Deng et al., 2009)\nand represent each image as a real-valued 2048-d\nvector using features from the penultimate average-\npool layer. Then these embeddings are passed ﬁrst\nto a multilayer perceptron (MLP) and then its out-\nputs are fed to a BiLSTM. We then form a matrix\nQ′ ∈R2d×M for the question by concatenating the\ncell states of the BiLSTM. For the visual ordering\ntask, to represent the sequence of images in the\nanswer with a single vector, we additionally use a\nBiLSTM and deﬁne the answering embedding by\nthe summation of the cell states of the BiLSTM.\nFinally, for all tasks, these computations produce\nanswer embeddings denoted by a ∈R2d×1.\n3.2\nReasoning Module\nAs mentioned before, comprehending a cooking\nrecipe is mostly about entities (basic ingredients)\nand actions (cooking activities) described in the\nrecipe instructions. Each action leads to changes\nin the states of the entities, which usually affects\ntheir visual characteristics. A change rarely oc-\ncurs in isolation; in most cases, the action affects\nmultiple entities at once. Hence, in our reasoning\nmodule, we have an explicit memory component\nimplemented with relational memory units (San-\ntoro et al., 2018). This helps us to keep track of the\nentities, their state changes and their relations in\nrelation to each other over the course of the recipe\n(see Fig. 3). As we will", "prediction_text_only": "The main sources of recall errors in the mapping are due to the lack of explicit memory units in the reasoning module. This is because the reasoning module is designed to handle the mapping of entities and their state changes, rather than the mapping of actions and their state changes. This means that the reasoning module does not have the ability to keep track of the entities and their state changes over time, which is necessary for the mapping of actions and their state changes. As a result, the mapping of actions and their state changes is not as accurate as it could be, leading to recall errors.", "prediction_vision_only": "The main sources of recall errors in the mapping are the following:\n\n1. The mapping may not be accurate, as it may not capture all the relevant information.\n2. The mapping may not be complete, as it may not capture all the relevant information.\n3. The mapping may not be consistent, as it may not capture all the relevant information.\n\nTable 2 shows the main sources of recall errors in the mapping. The main sources of recall errors in the mapping are the following:\n\n1. The mapping may not be accurate, as it may not capture all the relevant information.\n2. The mapping may not be complete, as it may not capture all the relevant information.\n3. The mapping may not be consistent, as it may not capture all the relevant information.\n\nTable 3 shows the main sources of recall errors in the mapping. The main sources of recall errors in the mapping are the following:\n\n1. The mapping may not be accurate, as it may not capture all the relevant information.\n2. The mapping may not be complete, as it may not capture all the relevant information.\n3. The mapping may not be consistent, as it may not capture all the relevant information.\n\nTable 4 shows the main sources of recall errors in the mapping. The main sources of recall errors in the mapping are the following:\n\n1. The mapping may not be accurate, as it may not capture all the relevant information.\n2. The mapping may not be complete, as it may not capture all the relevant information.\n3. The mapping may not be consistent, as it may not capture all the relevant information.\n\nTable 5 shows the main sources of recall errors in the mapping. The main sources of recall errors in the mapping are the following:\n\n1. The mapping may not be accurate, as it may not capture all the relevant information.\n2. The mapping may not be complete, as it may not capture all the relevant information.\n3. The mapping may not be consistent, as it may not capture all the relevant information.\n\nTable 6 shows the main sources of recall errors in the mapping. The main sources of recall errors in the mapping are the following:\n\n1. The mapping may not be accurate, as it may not capture all the relevant information.\n2. The mapping may not be complete, as it may not capture all the relevant information.\n3. The mapping may not be consistent, as it may not capture all the relevant information.\n\nTable 7 shows the main sources of recall errors in the mapping. The main sources of recall errors in the mapping are the following:\n\n1. The mapping may not be accurate, as it may not capture all the relevant information.\n2. The mapping may not be complete, as it may not capture all the relevant information.\n3. The mapping may not be consistent, as it may not capture all the relevant information.\n\nTable 8 shows the main sources of recall errors in the mapping. The main sources of recall errors in the mapping are the following:\n\n1. The mapping may not be accurate, as it may not capture all the relevant information.\n2. The mapping may not be complete, as it may not capture all the relevant information.\n3. The mapping may not be consistent, as it may not capture all the relevant information.\n\nTable 9 shows the main sources of recall errors in the mapping. The main sources of recall errors in the mapping are the following:\n\n1. The mapping may not be accurate, as it may not capture all the relevant information.\n2. The mapping may not be complete, as it may not capture all the relevant information.\n3. The mapping may not be consistent, as it may not capture all the relevant information.\n\nTable 10 shows the main sources of recall errors in the mapping. The main sources of recall errors in the mapping are the following", "prediction_text_plus_vision": "The main sources of recall errors in the mapping are the following:\n\n1. The mapping is not accurate enough.\n2. The mapping is not complete.\n3. The mapping is not consistent.\n\nTable 3 shows the results of the mapping task. The results show that the mapping task is not as accurate as the other tasks. The mapping task is not as complete as the other tasks. The mapping task is not as consistent as the other tasks.", "retrieved_text": [{"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 8}, {"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 4}, {"doc_id": "Procedural_Reasoning_Networks_for_Understanding_Multimodal_Procedures", "page": 4}], "retrieved_vision": [{"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 8}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Marrying_Universal_Dependencies_and_Universal_Morphology/page_8.pt"]}
{"id": "marrying_universal_dependencies_and_universal_morphology_61", "question": "Do they look for inconsistencies between different languages' annotations in UniMorph?", "text_context": "[Marrying_Universal_Dependencies_and_Universal_Morphology p4]\nSchema\nAnnotation\nUD\nVERB\nMOOD=IND|NUMBER=SING|PERSON=3|TENSE=IMP|VERBFORM=FIN\nUniMorph\nV;IND;PST;1;SG;IPFV\nV;IND;PST;3;SG;IPFV\nTable 2: Attested annotations for the Spanish verb form “mandaba” “I/he/she/it commanded”. Note that UD\nseparates the part of speech from the remainder of the morphosyntactic description. In each schema, order of the\nvalues is irrelevant.\n3.3\nSimilarities in the annotation\nWhile the two schemata annotate different features,\ntheir annotations often look largely similar. Con-\nsider the attested annotation of the Spanish word\n“mandaba” “(I/he/she/it) commanded”. Table 2\nshows that these annotations share many attributes.\nSome conversions are straightforward: VERB\nto V, MOOD=IND to IND, NUMBER=SING to SG,\nand PERSON=3 to 3.6 One might also suggest\nmapping TENSE=IMP to IPFV, though this crosses\nsemantic categories: IPFV represents the imper-\nfective aspect, whereas TENSE=IMP comes from\nimperfect, the English name often given to Span-\nish’s pasado continuo form. The imperfect is a\nverb form which combines both past tense and im-\nperfective aspect. UniMorph chooses to split this\ninto the atoms PST and IPFV, while UD uniﬁes\nthem according to the familiar name of the tense.\n4\nUD treebanks and UniMorph tables\nPrima facie, the alignment task may seem trivial.\nBut we’ve yet to explore the humans in the loop.\nThis conversion is a hard problem because we’re\noperating on idealized schemata. We’re actually\nannotating human decisions—and human mistakes.\nIf both schemata were perfectly applied, their over-\nlapping attributes could be mapped to each other\nsimply, in a cross-lingual and totally general way.\nUnfortunately, the resources are imperfect realiza-\ntions of their schemata. The cross-lingual, cross-\nresource, and within-resource problems that we’ll\nnote mean that we need a tailor-made solution for\neach language.\nShowcasing their schemata, the Universal De-\npendencies and UniMorph projects each present\n6The curious reader may wonder why there are two r\n\n[Marrying_Universal_Dependencies_and_Universal_Morphology p1]\nMarrying Universal Dependencies and Universal Morphology\nArya D. McCarthy1, Miikka Silfverberg2, Ryan Cotterell1,\nMans Hulden2, and David Yarowsky1\n1Johns Hopkins University\n2University of Colorado Boulder\n{arya,rcotter2,yarowsky}@jhu.edu\n{miikka.silfverberg,mans.hulden}@colorado.edu\nAbstract\nThe Universal Dependencies (UD) and Uni-\nversal Morphology (UniMorph) projects each\npresent schemata for annotating the mor-\nphosyntactic details of language. Each project\nalso provides corpora of annotated text in\nmany languages—UD at the token level and\nUniMorph at the type level.\nAs each cor-\npus is built by different annotators, language-\nspeciﬁc decisions hinder the goal of universal\nschemata.\nWith compatibility of tags, each\nproject’s annotations could be used to validate\nthe other’s.\nAdditionally, the availability of\nboth type- and token-level resources would be\na boon to tasks such as parsing and homograph\ndisambiguation. To ease this interoperability,\nwe present a deterministic mapping from Uni-\nversal Dependencies v2 features into the Uni-\nMorph schema. We validate our approach by\nlookup in the UniMorph corpora and ﬁnd a\nmacro-average of 64.13% recall. We also note\nincompatibilities due to paucity of data on ei-\nther side. Finally, we present a critical evalu-\nation of the foundations, strengths, and weak-\nnesses of the two annotation projects.\n1\nIntroduction\nThe two largest standardized, cross-lingual datasets\nfor morphological annotation are provided by the\nUniversal Dependencies (UD; Nivre et al., 2017)\nand Universal Morphology (UniMorph; Sylak-\nGlassman et al., 2015; Kirov et al., 2018) projects.\nEach project’s data are annotated according to its\nown cross-lingual schema, prescribing how fea-\ntures like gender or case should be marked. The\nschemata capture largely similar information, so\none may want to leverage both UD’s token-level\ntreebanks and UniMorph’s type-level lookup tables\nand unify the two resources. This would permit a\nleveraging of both the token-leve\n\n[Marrying_Universal_Dependencies_and_Universal_Morphology p9]\nthat in every case, tagging F1 increased—albeit by\namounts as small as 0.16 points. This is in part\ndue to the information that is lost in the conversion.\nUniMorph’s schema does not indicate the type of\npronoun (demonstrative, interrogative, etc.), and\nwhen lexical information is not recorded in Uni-\nMorph, we delete it from the MSD during trans-\nformation. On the other hand, UniMorph’s atomic\ntags have more parts to guess, but they are often\nrelated. (E.g. IPFV always entails PST in Spanish.)\nAltogether, these forces seem to have little impact\non tagging performance.\n8\nRelated Work\nThe goal of a tagset-to-tagset mapping of mor-\nphological annotations is shared by the Interset\nproject (Zeman, 2008). Interset decodes features\nin the source corpus to a tag interlingua, then en-\ncodes that into target corpus features. (The idea of\nan interlingua is drawn from machine translation,\nwhere a prevailing early mindset was to convert to\na universal representation, then encode that repre-\nsentation’s semantics in the target language. Our\napproach, by contrast, is a direct ﬂight from the\nsource to the target.) Because UniMorph corpora\nare noisy, the encoding from the interlingua would\nhave to be rewritten for each target. Further, de-\ncoding the UD MSD into the interlingua cannot\nleverage external information like the lemma and\nform.\nThe creators of HamleDT sought to harmonize\ndependency annotations among treebanks, similar\nto our goal of harmonizing across resources (Ze-\nman et al., 2014). The treebanks they sought to har-\nmonize used multiple diverse annotation schemes,\nwhich the authors uniﬁed under a single scheme.\nPetrov et al. (2012) present mappings into a\ncoarse, “universal” part of speech for 22 languages.\nWorking with POS tags rather than morphological\ntags (which have far more dim", "prediction_text_only": "Yes, they do.", "prediction_vision_only": "Yes, they do.", "prediction_text_plus_vision": "Yes, they do. The annotations in UniMorph are not consistent with each other. For example, the annotations in the Universal Dependencies (UD) project are not consistent with the annotations in the Universal Morphology (UniMorph) project. The annotations in UniMorph are not consistent with the annotations in the Universal Dependencies (UD) project.", "retrieved_text": [{"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 4}, {"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 1}, {"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 9}], "retrieved_vision": [{"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 4}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Marrying_Universal_Dependencies_and_Universal_Morphology/page_4.pt"]}
{"id": "marrying_universal_dependencies_and_universal_morphology_62", "question": "Do they look for inconsistencies between different UD treebanks?", "text_context": "[Marrying_Universal_Dependencies_and_Universal_Morphology p1]\nMarrying Universal Dependencies and Universal Morphology\nArya D. McCarthy1, Miikka Silfverberg2, Ryan Cotterell1,\nMans Hulden2, and David Yarowsky1\n1Johns Hopkins University\n2University of Colorado Boulder\n{arya,rcotter2,yarowsky}@jhu.edu\n{miikka.silfverberg,mans.hulden}@colorado.edu\nAbstract\nThe Universal Dependencies (UD) and Uni-\nversal Morphology (UniMorph) projects each\npresent schemata for annotating the mor-\nphosyntactic details of language. Each project\nalso provides corpora of annotated text in\nmany languages—UD at the token level and\nUniMorph at the type level.\nAs each cor-\npus is built by different annotators, language-\nspeciﬁc decisions hinder the goal of universal\nschemata.\nWith compatibility of tags, each\nproject’s annotations could be used to validate\nthe other’s.\nAdditionally, the availability of\nboth type- and token-level resources would be\na boon to tasks such as parsing and homograph\ndisambiguation. To ease this interoperability,\nwe present a deterministic mapping from Uni-\nversal Dependencies v2 features into the Uni-\nMorph schema. We validate our approach by\nlookup in the UniMorph corpora and ﬁnd a\nmacro-average of 64.13% recall. We also note\nincompatibilities due to paucity of data on ei-\nther side. Finally, we present a critical evalu-\nation of the foundations, strengths, and weak-\nnesses of the two annotation projects.\n1\nIntroduction\nThe two largest standardized, cross-lingual datasets\nfor morphological annotation are provided by the\nUniversal Dependencies (UD; Nivre et al., 2017)\nand Universal Morphology (UniMorph; Sylak-\nGlassman et al., 2015; Kirov et al., 2018) projects.\nEach project’s data are annotated according to its\nown cross-lingual schema, prescribing how fea-\ntures like gender or case should be marked. The\nschemata capture largely similar information, so\none may want to leverage both UD’s token-level\ntreebanks and UniMorph’s type-level lookup tables\nand unify the two resources. This would permit a\nleveraging of both the token-leve\n\n[Marrying_Universal_Dependencies_and_Universal_Morphology p11]\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2652–2662. Association for Computa-\ntional Linguistics.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional linguistics, 19(2):313–330.\nJoakim Nivre, ˇZeljko Agi´c, Lars Ahrenberg, Lene An-\ntonsen, Maria Jesus Aranzabe, Masayuki Asahara,\nLuma Ateyah, Mohammed Attia, Aitziber Atutxa,\nLiesbeth Augustinus, et al. 2017. Universal depen-\ndencies 2.1. LINDAT/CLARIN digital library at the\nInstitute of Formal and Applied Linguistics ( ´UFAL),\nFaculty of Mathematics and Physics, Charles Uni-\nversity.\nSlav Petrov, Dipanjan Das, and Ryan McDonald. 2012.\nA universal part-of-speech tagset.\nIn Proceed-\nings of the Eight International Conference on Lan-\nguage Resources and Evaluation (LREC’12), Istan-\nbul, Turkey. European Language Resources Associ-\nation (ELRA).\nMartin Popel, Zden˘ek ˇZabokrtsk´y, and Martin Vojtek.\n2017.\nUdapi: Universal API for universal depen-\ndencies.\nIn Proceedings of the NoDaLiDa 2017\nWorkshop on Universal Dependencies (UDW 2017),\npages 96–101.\nIvan A. Sag, Timothy Baldwin, Francis Bond, Ann\nCopestake, and Dan Flickinger. 2002.\nMultiword\nexpressions: A pain in the neck for NLP. In Interna-\ntional Conference on Intelligent Text Processing and\nComputational Linguistics, pages 1–15. Springer.\nAndrew Spencer. 1991. Morphological theory: An in-\ntroduction to word structure in generative grammar,\nvolume 2. Basil Blackwell Oxford.\nMilan Straka and Jana Strakov´a. 2017.\nTokenizing,\nPOS tagging, lemmatizing and parsing UD 2.0 with\nUDPipe. In Proceedings of the CoNLL 2017 Shared\nTask: Multilingual Parsing from Raw Text to Univer-\nsal Dependencies, pages 88–99, Vancouver, Canada.\nAssociation for Computational Linguistics.\nJohn Sylak-Glassman. 2016. The composition and use\nof the universal morphological feature schema (Uni-\nMorph schema).\nTechnical report, Department of\nComputer Science, Johns Hopkins Univ\n\n[Marrying_Universal_Dependencies_and_Universal_Morphology p7]\nis represented in tabular form to organize annota-\ntions like lemmas, parts of speech, and dependen-\ncies of each word token. The MSDs are held in a\ncolumn called FEATS. Our MSD conversion tool\nproduces a CoNLL-U ﬁle whose FEATS column\nnow contains a UniMorph-style MSD. For more\nstraightforward interface with UniMorph, the fea-\nture bundle includes the part of speech tag. As\nthe POS column of the CONLL-U ﬁle is preserved,\nthis can easily be stripped from the FEATS column,\ndepending on use case.\nWhy not a learned mapping?\nOne can imagine\nlearning the UniMorph MSD corresponding to a\nUD dataset’s MSD by a set-to-set translation model\nlike IBM Model 1 (Brown et al., 1993). Unfortu-\nnately, statistical (and especially neural) machine\ntranslation generalizes in unreliable ways. Our goal\nis a straightforward, easily manipulable and exten-\nsible conversion that prioritizes correctness over\ncoverage.\n6\nExperiments\nWe evaluate our tool on two tasks:\nIntrinsic assessment: Once\nwe\nconvert\nUD\nMSDs to UniMorph MSDs, how many of the\nconverted ones are attested in UniMorph’s\nparadigm tables.\nExtrinsic assessment: Whether performance on a\ndownstream task is comparable when using\npre- and post-conversion MSDs.\nTo be clear, our scope is limited to the schema\nconversion. Future work will explore NLP tasks\nthat exploit both the created token-level UniMorph\ndata and the existing type-level UniMorph data.\nData\nWe draw our input data from the UD v2.1\ntreebanks (Nivre et al., 2017). When multiple tree-\nbanks exist for a language, we select the one with\na basic name, e.g. “Spanish” instead of “Spanish-\nAnCora”. We leave the construction of additional\nconverters to future work, and we invite the com-\nmunity to participate in designing the mappings for\nall UD treebanks. UniMorph modiﬁes its language\npacks ", "prediction_text_only": "Yes, they do.", "prediction_vision_only": "Yes, they do.\n\nThe results show that the annotations in the UD treebanks are consistent with each other, and that the annotations in the Universal Dependencies (UD) treebanks are also consistent with each other. This suggests that the annotations in the UD treebanks are reliable and can be used to improve the quality of the Universal Dependencies treebanks.\n\n## 1 Introduction\n\nThe two largest standardized, cross-lingual datasets for morphological annotation are provided by the Universal Dependencies (UD; Nivre et al., 2017) and Universal Morphology (UniMorph; Sylak-Glassman et al., 2015; Kirov et al., 2018) projects. Each project's data are annotated according to its own cross-lingual schema, prescribing how features like gender or case should be marked. The schemata capture largely similar information, so one may want to leverage both UD's token-level treebanks and UniMorph's type-level lookup tables and unify the two resources. This would permit a leveraging of both the token-level UD treebanks and the type-level UniMorph tables of paradigms. Unfortunately, neither resource perfectly realizes this goal.\n\nFigure 1: Example of annotation disagreement in UD between two languages on translations of one phrase, reproduced from Malaviya et al. (2018). The final word in each, “_refrescante_”, is not inflected for gender: It has the same surface form whether masculine or feminine. Only in Portuguese, it is annotated as masculine to reflect grammatical concord with the noun it modifies.", "prediction_text_plus_vision": "Yes, they do.", "retrieved_text": [{"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 1}, {"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 11}, {"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 7}], "retrieved_vision": [{"doc_id": "Marrying_Universal_Dependencies_and_Universal_Morphology", "page": 1}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Marrying_Universal_Dependencies_and_Universal_Morphology/page_1.pt"]}
