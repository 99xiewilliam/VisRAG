{"id": 1, "question": "On which datasets does LadaBERT achieve state-of-the-art?", "reference": "LadaBERT -1, -2 achieves state of art on all datasets namely, MNLI-m MNLI-mm, SST-2, QQP, and QNLI. \nLadaBERT-3 achieves SOTA on the first four dataset. \nLadaBERT-4 achieves SOTA on MNLI-m, MNLI-mm, and QNLI", "prediction_text_only": "The context does not specify which datasets LadaBERT achieves state-of-the-art on. It only mentions that LadaBERT is evaluated on the GLUE-benchmark evaluation platform and that it outperforms the strongest baselines under similar model sizes, but does not list the specific datasets. Therefore, the answer cannot be determined from the provided context.", "prediction_vision": "It achieves state-of-the-art accuracy on various public datasets.", "prediction_vision_only": "LadaBERT achieves state-of-the-art accuracy on various public datasets (including MNLI-m).", "retrieval_text_only": {"text_results": [{"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p7_c0", "metadata": {"text": "selection is conducted on the validation data for each dataset. After training, the prediction results are\nsubmitted to the GLUE-benchmark evaluation platform3 to get the evaluation performance on test data.\nFor a comprehensive evaluation, we experiment with four settings of LadaBERT, namely LadaBERT-\n1, -2, -3 and -4, which reduce the model parameters of BERT-Base by 2.5, 5.0, 7.5 and 10.0 times\n", "pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "page": 7, "chunk_id": 0, "text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_7"}}, {"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p9_c6", "metadata": {"chunk_id": 6, "text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_9", "pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "page": 9, "text": "on\npipeline that generates an adaptive BERT model efﬁciently based on a given task and speciﬁc constraint.\nIt is based on a hybrid solution, which conducts matrix factorization, weight pruning and knowledge\ndistillation in a collaborative fashion. The experimental results demonstrate that LadaBERT is able to\nachieve comparable performance with other state-of-the-art solutions using much less train"}}, {"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p7_c7", "metadata": {"text": " state-of-the-art approaches are listed in Table 3, where the\nmodels are ranked by parameter sizes for feasible comparison. As shown in the table, LadaBERT con-\nsistently outperforms the strongest baselines under similar model sizes. In addition, the performance\nof LadaBERT demonstrates the superiority of a combination of SVD-based matrix factorization, weight\npruning and knowledge distillation.\n3", "chunk_id": 7, "page": 7, "pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_7"}}]}, "retrieval_vision": {"image_results": [{"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p1", "metadata": {"pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "page": 1, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression/page_1.png", "text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1"}}, {"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p8", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression/page_8.png", "page": 8, "pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_8"}}, {"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p7", "metadata": {"text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_7", "pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "page": 7, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression/page_7.png"}}], "text_results": [{"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p1_c3", "metadata": {"pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "chunk_id": 3, "text": "eacher model. In this paper, we address this issue by proposing a hybrid solution\nnamed LadaBERT (Lightweight adaptation of BERT through hybrid model compression), which\ncombines the advantages of different model compression methods, including weight pruning,\nmatrix factorization and knowledge distillation. LadaBERT achieves state-of-the-art accuracy on\nvarious public datasets while the training o", "page": 1}}, {"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p1_c8", "metadata": {"chunk_id": 8, "text": " comparable accuracy using signiﬁcantly less time\nand resource consumption?\nIn this paper, we propose LadaBERT (Lightweight adaptation of BERT through hybrid model com-\npression) to tackle this problem. Speciﬁcally, LadaBERT is based on an iterative hybrid model com-\npression framework consisting of weighting pruning, matrix factorization and knowledge distillation.\nInitially, the architecture and", "page": 1, "pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression"}}, {"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p1_c0", "metadata": {"pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "page": 1, "chunk_id": 0, "text": "LadaBERT: Lightweight Adaptation of BERT\nthrough Hybrid Model Compression\nYihuan Mao1,∗, Yujing Wang2,3,†, Chufan Wu1, Chen Zhang2, Yang Wang2\nQuanlu Zhang2, Yaming Yang2, Yunhai Tong3, Jing Bai2\n1Tsinghua University\n2Microsoft Research Asia\n3Key Laboratory of Machine Perception, MOE, School of EECS, Peking University\nmaoyh20@mails.tsinghua.edu.cn, {yujwang,yhtong}@pku.edu.cn, chufanwu15@gmail.com"}}]}, "retrieval_vision_only": {"image_results": [{"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p1", "metadata": {"pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "page": 1, "text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_1", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression/page_1.png"}}, {"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p8", "metadata": {"text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_8", "page": 8, "pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression/page_8.png"}}, {"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p7", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression/page_7.png", "page": 7, "text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_7", "pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression"}}]}, "metrics_text_only": {"f1": 0.22988505747126434, "rouge_l": 0.20689655172413793, "bleu_1": 0.20833333333333334, "bleu_2": 0.13315591032282686, "bleu_3": 0.10496103543669949, "bleu_4": 0.08466919880918329, "meteor": 0.21372876636034532}, "metrics_vision": {"f1": 0.2448979591836735, "rouge_l": 0.2448979591836735, "bleu_1": 0.03301393203384434, "bleu_2": 0.024607132077411276, "bleu_3": 0.02027072758444257, "bleu_4": 0.01599609569388068, "meteor": 0.14158202523853494}, "metrics_vision_only": {"f1": 0.34615384615384615, "rouge_l": 0.34615384615384615, "bleu_1": 0.09369365762534726, "bleu_2": 0.06501296714810205, "bleu_3": 0.04702723452987603, "bleu_4": 0.03444409984096093, "meteor": 0.18908560575227243}}
{"id": 2, "question": "What inter-annotator agreement did they obtain?", "reference": "Raw agreement is around .90 for this dataset.", "prediction_text_only": ".93", "prediction_vision": "They obtained raw agreement of 0.92/0.95/0.90 (Phase 1) and 0.93/0.90/0.89 (Phase 2), with kappa of 0.90/0.93/0.88 (Phase 1) and 0.92/0.88/0.88 (Phase 2) for scene/function/construal respectively.", "prediction_vision_only": "They obtained inter-annotator agreement of 0.90–0.95 raw agreement and 0.88–0.93 Cohen’s kappa (across scene, function, and construal).", "retrieval_text_only": {"text_results": [{"id": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese_p3_c14", "metadata": {"text": "hases, and\ntherefore we present two inter-annotator agreement studies\nto demonstrate the reproducibility of SNACS and the relia-\nbility of the adapted scheme for Chinese.\nTable 1 shows raw agreement and Cohen’s kappa across\nthree annotators computed by averaging three pairwise com-\nparisons. Agreement levels on scene role, function, and full\nconstrual are high for both phases, attesting to the val", "chunk_id": 14, "page": 3, "pdf_name": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese", "text_collection": "pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_3"}}, {"id": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese_p4_c0", "metadata": {"text": "IAA SAMPLES\nPhase\nTime\nChapters\n# Adpositions\nPhase 1\nJuly 2018\n15–20\n111\nPhase 2\nSept 2019\n26–27\n124\nRAW AGREEMENT\nPhase\nScene\nFunction\nConstrual\nPhase 1\n.92\n.95\n.90\nPhase 2\n.93\n.90\n.89\nKAPPA\nPhase\nScene\nFunction\nConstrual\nPhase 1\n.90\n.93\n.88\nPhase 2\n.92\n.88\n.88\nTable 1: Inter-annotator agreement (IAA) results on two samples\nfrom different phases of the project.\nToks. Types\nChapters\n27\nNA\nSentenc", "page": 4, "pdf_name": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese", "chunk_id": 0, "text_collection": "pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4"}}, {"id": "cl-bench-_a_benchmark_for_context_learnin_p7_c9", "metadata": {"text": " 4.5 and Qwen-3-Max as verifiers. Results show that the raw agreement between\nGPT-5.1 and the other two verifiers exceeds 90%, indicating strong inter-verifier agreement and\nsuggesting that GPT-5.1 does not exhibit noticeable self-evaluation bias.\nSecond, we randomly sample 100 LM-generated solutions along with the GPT-5.1-generated ra-\ntionales and scores, and annotators assess whether GPT-5.1’s ", "page": 7, "pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "chunk_id": 9, "text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_7"}}]}, "retrieval_vision": {"image_results": [{"id": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese_p4", "metadata": {"pdf_name": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/a_corpus_of_adpositional_supersenses_for_mandarin_chinese/page_4.png", "text_collection": "pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4", "page": 4}}, {"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p3", "metadata": {"pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "page": 3, "text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_3", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study/page_3.png"}}, {"id": "political_speech_generation_p11", "metadata": {"page": 11, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/political_speech_generation/page_11.png", "text_collection": "pdf_political_speech_generation_11", "pdf_name": "political_speech_generation"}}], "text_results": [{"id": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese_p4_c0", "metadata": {"chunk_id": 0, "text": "IAA SAMPLES\nPhase\nTime\nChapters\n# Adpositions\nPhase 1\nJuly 2018\n15–20\n111\nPhase 2\nSept 2019\n26–27\n124\nRAW AGREEMENT\nPhase\nScene\nFunction\nConstrual\nPhase 1\n.92\n.95\n.90\nPhase 2\n.93\n.90\n.89\nKAPPA\nPhase\nScene\nFunction\nConstrual\nPhase 1\n.90\n.93\n.88\nPhase 2\n.92\n.88\n.88\nTable 1: Inter-annotator agreement (IAA) results on two samples\nfrom different phases of the project.\nToks. Types\nChapters\n27\nNA\nSentenc", "pdf_name": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese", "page": 4}}, {"id": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese_p4_c7", "metadata": {"text": "ions.\n5.2.\nSupersense & Construal Distributions in\nChinese versus English\nThe distribution of scene role and function types in Chinese\nand English reﬂects the differences and similarities of adpo-\nsition semantics in both languages. In table 4 we compare\nthis corpus with the largest English adposition supersense\ncorpus, STREUSLE version 4.1 (Schneider et al., 2018),\nwhich consists of web reviews. ", "page": 4, "pdf_name": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese", "chunk_id": 7}}, {"id": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese_p4_c11", "metadata": {"text": "Little Prince corpus for\nqualitative comparisons, whereas all quantitative results in\nEnglish refer to the larger STREUSLE corpus of English\nWeb Treebank reviews (Schneider et al., 2018).\nFewer Adpositions in Chinese\nAs shown in Table 4, the\npercentage of adposition targets over tokens in Chinese is\nonly half of that in English. This is due to the fact that\nChinese has a stronger preference to con", "pdf_name": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese", "page": 4, "chunk_id": 11}}]}, "retrieval_vision_only": {"image_results": [{"id": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese_p4", "metadata": {"pdf_name": "a_corpus_of_adpositional_supersenses_for_mandarin_chinese", "page": 4, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/a_corpus_of_adpositional_supersenses_for_mandarin_chinese/page_4.png", "text_collection": "pdf_a_corpus_of_adpositional_supersenses_for_mandarin_chinese_4"}}, {"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p3", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study/page_3.png", "pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_3", "page": 3}}, {"id": "political_speech_generation_p11", "metadata": {"pdf_name": "political_speech_generation", "page": 11, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/political_speech_generation/page_11.png", "text_collection": "pdf_political_speech_generation_11"}}]}, "metrics_text_only": {"f1": 0.0, "rouge_l": 0.0, "bleu_1": 9.11881965554517e-13, "bleu_2": 9.11881965554517e-13, "bleu_3": 9.11881965554517e-13, "bleu_4": 9.11881965554517e-13, "meteor": 0.0}, "metrics_vision": {"f1": 0.14545454545454545, "rouge_l": 0.14545454545454545, "bleu_1": 0.08510638297872339, "bleu_2": 0.04301323936193401, "bleu_3": 0.00012276317177523942, "bleu_4": 6.558447938493833e-06, "meteor": 0.2652310924369748}, "metrics_vision_only": {"f1": 0.18181818181818182, "rouge_l": 0.12121212121212122, "bleu_1": 0.12000000000000001, "bleu_2": 0.07071067811865474, "bleu_3": 0.0001709975946676699, "bleu_4": 8.408964152537154e-06, "meteor": 0.2634593356242841}}
{"id": 3, "question": "what is the size of the dataset?", "reference": "2771 speeches containing 50871 sentences", "prediction_text_only": "- The size of the dataset is not specified in the provided context.", "prediction_vision": "The dataset size is not specified in the provided context or image.", "prediction_vision_only": "The dataset size is not specified in the provided image.", "retrieval_text_only": {"text_results": [{"id": "cl-bench-_a_benchmark_for_context_learnin_p71_c3", "metadata": {"text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_71", "text": "ounter information that is uncertain or\nis missing information, explain this to the user. Do not attempt to access realtime information or\nbrowse the internet. If current data is needed, you should state your limitation. You should maintain a\nprofessional tone that avoids informal language. You should provide direct answers first always and\nprovide brief clarification when it would help understand", "page": 71, "chunk_id": 3, "pdf_name": "cl-bench-_a_benchmark_for_context_learnin"}}, {"id": "cl-bench-_a_benchmark_for_context_learnin_p42_c1", "metadata": {"chunk_id": 1, "text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_42", "page": 42, "pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "text": "34f93f5a44\n• Model: Doubao-1.6-Thinking\n• Context Category: Empirical Discovery & Simulation\n• Context Sub-Category: Observational Data\n• Task: What can be inferred from this dataset?\nSystem Prompt\nAudience: faculty, staff, and students without specialized STEM backgrounds. Tone/style: concise,\nplain language, define terms on first use, avoid jargon. Default format: respond in JSON unless the\nuser"}}, {"id": "cl-bench-_a_benchmark_for_context_learnin_p39_c4", "metadata": {"pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "chunk_id": 4, "page": 39, "text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_39", "text": "uestion to the user. - Your response must use bullet points. - You must bold the answer to the\nuser’s question. - Beneath the answer, provide a one bullet point summary about how you arrived at\nthe answer. - Explicitly state assumptions that you made in the first bullet point. - Explicitly state\nif the information is found in text provided to you or your internal memory. If information is not\nprov"}}]}, "retrieval_vision": {"image_results": [{"id": "cl-bench-_a_benchmark_for_context_learnin_p16", "metadata": {"pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/cl-bench-_a_benchmark_for_context_learnin/page_16.png", "page": 16, "text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_16"}}, {"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p6", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression/page_6.png", "page": 6, "text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6", "pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression"}}, {"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p9", "metadata": {"pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "page": 9, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/dynamic_fusion_networks_for_machine_reading_comprehension/page_9.png", "text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9"}}], "text_results": [{"id": "cl-bench-_a_benchmark_for_context_learnin_p16_c10", "metadata": {"page": 16, "pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "text": "ps://openreview.net/forum?\nid=d7KBjmI3GmQ.\n[25] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and\nBoris Ginsburg. RULER: What’s the real context size of your long-context language models?\nIn First Conference on Language Modeling, 2024. URL https://openreview.net/\nforum?id=kIoBbc76Sy.\n16", "chunk_id": 10}}, {"id": "cl-bench-_a_benchmark_for_context_learnin_p16_c7", "metadata": {"pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "text": "to. Length-controlled\nalpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475,\n2024.\n[21] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,\nMeng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A\nsurvey, 2024. URL https://arxiv.org/abs/2312.10997.\n[22] Yunfan Gao, Yun Xiong, Meng Wang, and Haof", "page": 16, "chunk_id": 7}}, {"id": "cl-bench-_a_benchmark_for_context_learnin_p16_c3", "metadata": {"text": "ng, et al. A survey on in-context learning. In Proceedings of the 2024\nconference on empirical methods in natural language processing, pp. 1107–1128, 2024.\n[17] Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. Bamboo: A com-\nprehensive benchmark for evaluating long text modeling capacities of large language models.\nIn Proceedings of the 2024 Joint International Conference on Com", "chunk_id": 3, "pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "page": 16}}]}, "retrieval_vision_only": {"image_results": [{"id": "cl-bench-_a_benchmark_for_context_learnin_p16", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/cl-bench-_a_benchmark_for_context_learnin/page_16.png", "page": 16, "text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_16", "pdf_name": "cl-bench-_a_benchmark_for_context_learnin"}}, {"id": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_p6", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression/page_6.png", "text_collection": "pdf_ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression_6", "pdf_name": "ladabert-_lightweight_adaptation_of_bert_through_hybrid_model_compression", "page": 6}}, {"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p9", "metadata": {"pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/dynamic_fusion_networks_for_machine_reading_comprehension/page_9.png", "page": 9}}]}, "metrics_text_only": {"f1": 0.0, "rouge_l": 0.0, "bleu_1": 1.0000000000000007e-09, "bleu_2": 1.0000000000000007e-09, "bleu_3": 1.0000000000000007e-09, "bleu_4": 1.0000000000000007e-09, "meteor": 0.0}, "metrics_vision": {"f1": 0.0, "rouge_l": 0.0, "bleu_1": 1.0000000000000007e-09, "bleu_2": 1.0000000000000007e-09, "bleu_3": 1.0000000000000007e-09, "bleu_4": 1.0000000000000007e-09, "meteor": 0.0}, "metrics_vision_only": {"f1": 0.0, "rouge_l": 0.0, "bleu_1": 1.0000000000000007e-09, "bleu_2": 1.0000000000000007e-09, "bleu_3": 1.0000000000000007e-09, "bleu_4": 1.0000000000000007e-09, "meteor": 0.0}}
{"id": 4, "question": "What 20 domains are available for selection of source domain?", "reference": "Amazon Instant Video\nAutomotive\nBaby\nBeauty\nBooks\nClothing Accessories\nElectronics\nHealth\nHome Kitchen\nMovies TV\nMusic\nOffice Products\nPatio\nPet Supplies\nShoes\nSoftware\nSports Outdoors\nTools Home Improvement\nToys Games\nVideo Games", "prediction_text_only": "20 domains", "prediction_vision": "Books, Electronics, DVD, Kitchen, Apparel, Camera & Photo, Health & Personal Care, Toys & Games, Video, Baby, Magazines, Software, Sports & Outdoors, IMDB, MR, Yelp, TripAdvisor, Restaurants, Hotels, and Airlines.", "prediction_vision_only": "Books, Electronics, DVD, Kitchen, Apparel, Camera, Health, Music, Toys, Video, Baby, Magazine, Sports, Software, Jewelry, Automotive, Office, Grocery, Movies, and Beauty.", "retrieval_text_only": {"text_results": [{"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p1_c2", "metadata": {"text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1", "pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "chunk_id": 2, "page": 1, "text": "e source domain) to\nleverage from is, at best, intuitive. In this paper, we investigate text similarity metrics to facilitate source domain selection for CDSA.\nWe report results on 20 domains (all possible pairs) using 11 similarity metrics. Speciﬁcally, we compare CDSA performance with these\nmetrics for different domain-pairs to enable the selection of a suitable source domain, given a target dom"}}, {"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p1_c10", "metadata": {"page": 1, "chunk_id": 10, "pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1", "text": "create a recommendation\nchart that prescribes domains that are the best as the\nsource or target domain, for each of the domains.\n3. In general, we show which similarity metrics are cru-\ncial indicators of the beneﬁt to a target domain, in\nterms of source domain selection for CDSA.\nWith rising business applications of sentiment analysis, the\nconvenience of cross-domain adaptation of sentiment clas-"}}, {"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p7_c16", "metadata": {"chunk_id": 16, "page": 7, "text": "ur work i.e.,\nto pre-determine best source domain for CDSA using non-\nintensive text similarity-based metrics.\n7.\nConclusion and Future Work\nIn this paper, we investigate how text similarity-based met-\nrics facilitate the selection of a suitable source domain for\nCDSA. Based on a dataset of reviews in 20 domains, our\nrecommendation chart that shows the best source and target", "pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_7"}}]}, "retrieval_vision": {"image_results": [{"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p1", "metadata": {"text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study/page_1.png", "page": 1, "pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study"}}, {"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p6", "metadata": {"pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "page": 6, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study/page_6.png", "text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_6"}}, {"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p8", "metadata": {"page": 8, "pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_8", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study/page_8.png"}}], "text_results": [{"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p1_c2", "metadata": {"text": "e source domain) to\nleverage from is, at best, intuitive. In this paper, we investigate text similarity metrics to facilitate source domain selection for CDSA.\nWe report results on 20 domains (all possible pairs) using 11 similarity metrics. Speciﬁcally, we compare CDSA performance with these\nmetrics for different domain-pairs to enable the selection of a suitable source domain, given a target dom", "page": 1, "chunk_id": 2, "pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study"}}, {"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p1_c10", "metadata": {"pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "page": 1, "text": "create a recommendation\nchart that prescribes domains that are the best as the\nsource or target domain, for each of the domains.\n3. In general, we show which similarity metrics are cru-\ncial indicators of the beneﬁt to a target domain, in\nterms of source domain selection for CDSA.\nWith rising business applications of sentiment analysis, the\nconvenience of cross-domain adaptation of sentiment clas-", "chunk_id": 10}}, {"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p1_c0", "metadata": {"page": 1, "chunk_id": 0, "pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "text": "Recommendation Chart of Domains for Cross-Domain Sentiment Analysis:\nFindings of A 20 Domain Study\nAkash Sheoran♦, Diptesh Kanojia†,♣,⋆, Aditya Joshi♥, and Pushpak Bhattacharyya†\n♦Samsung Research Institute Bangalore, India\n†Indian Institute of Technology Bombay, India\n♣IITB-Monash Research Academy, India\n⋆Monash University, Australia\n♥CSIRO Data61, Australia\n♦a.sheoran@samsung.com, †{diptesh, pb}"}}]}, "retrieval_vision_only": {"image_results": [{"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p1", "metadata": {"page": 1, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study/page_1.png", "pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_1"}}, {"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p6", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study/page_6.png", "page": 6, "text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_6", "pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study"}}, {"id": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_p8", "metadata": {"pdf_name": "recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study", "text_collection": "pdf_recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study_8", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/recommendation_chart_of_domains_for_cross-domain_sentiment_analysis-findings_of_a_20_domain_study/page_8.png", "page": 8}}]}, "metrics_text_only": {"f1": 0.0, "rouge_l": 0.0, "bleu_1": 3.05902320501826e-16, "bleu_2": 3.05902320501826e-16, "bleu_3": 3.05902320501826e-16, "bleu_4": 3.05902320501826e-16, "meteor": 0.0}, "metrics_vision": {"f1": 0.3793103448275862, "rouge_l": 0.20689655172413793, "bleu_1": 0.3358903552306717, "bleu_2": 0.20656077597071545, "bleu_3": 0.14132887038864425, "bleu_4": 0.0012235387126245366, "meteor": 0.2543822708848765}, "metrics_vision_only": {"f1": 0.5283018867924528, "rouge_l": 0.22641509433962265, "bleu_1": 0.3948400108639286, "bleu_2": 1.5292087864923523e-05, "bleu_3": 5.173867802804941e-07, "bleu_4": 9.516770569934874e-08, "meteor": 0.22653721682847897}}
{"id": 5, "question": "How much improvement is given on RACE by their introduced approach?", "reference": "7.3% on RACE-M and 1.5% on RACE-H", "prediction_text_only": "The improvement on RACE by the introduced approach is 14.3%.", "prediction_vision": "DFN improves accuracy on RACE by 3.3 percentage points (47.4% vs 44.1% for GA).", "prediction_vision_only": "7.8% improvement on RACE.", "retrieval_text_only": {"text_results": [{"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p3_c8", "metadata": {"page": 3, "pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "chunk_id": 8, "text": "nd\nhigh school exams, respectively.\nRACE-M has\n28,293 questions and RACE-H has 69,574. Each\nquestion is associated with 4 candidate answers,\none of which is correct. The data generation pro-\ncess of RACE differs from most MRC datasets\n- instead of generating questions and answers by\nheuristics or crowd-sourcing, questions in RACE\nare speciﬁcally designed for testing human read-\ning skills, and are", "text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_3"}}, {"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p9_c9", "metadata": {"chunk_id": 9, "page": 9, "text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9", "pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "text": "ty in the dataset. Human per-\nformance drops from 85.1 on RACE-M to 69.4 on\nRACE-H, which indicates RACE-H is very chal-\nlenging even for human.\nFigure 5 shows six randomly-selected questions"}}, {"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p3_c5", "metadata": {"text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_3", "page": 3, "chunk_id": 5, "text": " incorporate dynamic decisions, but\ninstead of using ﬁxed steps, we apply dynamic de-\ncisions to various attention strategies and ﬂexible\nreasoning steps.\n3\nRACE - The MRC Task\nIn this section, we ﬁrst give a brief introduction to\nthe RACE dataset, and then explain the rationale\nbehind choosing RACE as the testbed in our study.\n3.1\nThe Dataset\nRACE (Reading Comprehension Dataset From\nExaminations)", "pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension"}}]}, "retrieval_vision": {"image_results": [{"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p9", "metadata": {"pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "page": 9, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/dynamic_fusion_networks_for_machine_reading_comprehension/page_9.png", "text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9"}}, {"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p3", "metadata": {"pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/dynamic_fusion_networks_for_machine_reading_comprehension/page_3.png", "text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_3", "page": 3}}, {"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p2", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/dynamic_fusion_networks_for_machine_reading_comprehension/page_2.png", "page": 2, "pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_2"}}], "text_results": [{"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p9_c9", "metadata": {"chunk_id": 9, "page": 9, "pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "text": "ty in the dataset. Human per-\nformance drops from 85.1 on RACE-M to 69.4 on\nRACE-H, which indicates RACE-H is very chal-\nlenging even for human.\nFigure 5 shows six randomly-selected questions"}}, {"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p9_c8", "metadata": {"text": "%\nand 7.3% performance boost over GA and Stan-\nford AR, respectively. On RACE-H, the outper-\nformance is 1.5% and 2.7%. The ensemble models\nalso gained a performance boost of 4-5% compar-\ning to previous methods. We suspect that the lower\ngain on RACE-H might result from the higher\nlevel of difﬁculty in those questions in RACE-H,\nas well as ambiguity in the dataset. Human per-\nformance drops from ", "page": 9, "chunk_id": 8, "pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension"}}, {"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p9_c1", "metadata": {"pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "page": 9, "chunk_id": 1, "text": "tatistics on the RACE dataset from (Lai et al., 2017).\nModel\nRACE-M RACE-H RACE\nSliding Window\n37.3\n30.4\n32.2\nStanford AR\n44.2\n43.0\n43.3\nGA\n43.7\n44.2\n44.1\nElimiNet\nN/A\nN/A\n44.5\nDFN\n51.5\n45.7\n47.4\nGA∗\nN/A\nN/A\n45.9\nElimiNet∗\nN/A\nN/A\n46.5\nGA∗+ElimiNet∗\nN/A\nN/A\n47.2\nDFN∗\n55.6\n49.4\n51.2\nTable 3: Accuracy% of DFN compared to base-\nline methods on RACE test sets. Results of the\nbaseline models came from "}}]}, "retrieval_vision_only": {"image_results": [{"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p9", "metadata": {"pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_9", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/dynamic_fusion_networks_for_machine_reading_comprehension/page_9.png", "page": 9}}, {"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p3", "metadata": {"pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/dynamic_fusion_networks_for_machine_reading_comprehension/page_3.png", "text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_3", "page": 3}}, {"id": "dynamic_fusion_networks_for_machine_reading_comprehension_p2", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/dynamic_fusion_networks_for_machine_reading_comprehension/page_2.png", "pdf_name": "dynamic_fusion_networks_for_machine_reading_comprehension", "page": 2, "text_collection": "pdf_dynamic_fusion_networks_for_machine_reading_comprehension_2"}}]}, "metrics_text_only": {"f1": 0.3, "rouge_l": 0.19999999999999998, "bleu_1": 0.26691246763893595, "bleu_2": 0.1634498379256302, "bleu_3": 0.00027760015305330806, "bleu_4": 1.1440297143332518e-05, "meteor": 0.23662551440329216}, "metrics_vision": {"f1": 0.2857142857142857, "rouge_l": 0.21428571428571427, "bleu_1": 0.23529411764705885, "bleu_2": 0.12126781251816653, "bleu_3": 0.0002449986525148224, "bleu_4": 1.1012166567854238e-05, "meteor": 0.32068965517241377}, "metrics_vision_only": {"f1": 0.37499999999999994, "rouge_l": 0.37499999999999994, "bleu_1": 0.1807165271473212, "bleu_2": 0.11665201667100261, "bleu_3": 0.00016003330513835246, "bleu_4": 5.927470980881448e-06, "meteor": 0.24572649572649571}}
{"id": 6, "question": "What directions are suggested to improve language models?", "reference": "Improved architecture for ANN, use of linguistical properties of words or sentences as features.", "prediction_text_only": "some directions for improving neural network language modeling further is discussed.", "prediction_vision": "Pre-train, prompt, and predict.", "prediction_vision_only": "The references suggest improving language models via better prompting/context engineering, improved in-context learning, explicit read-write memory and system-like operating frameworks, and more robust evaluation/benchmarks (including verifiable instruction following and leaderboards).", "retrieval_text_only": {"text_results": [{"id": "a_study_on_neural_network_language_modeling_p15_c10", "metadata": {"chunk_id": 10, "page": 15, "pdf_name": "a_study_on_neural_network_language_modeling", "text_collection": "pdf_a_study_on_neural_network_language_modeling_15", "text": "language models are described and a number of\nimprovement techniques are evaluated in this paper, but there are still something more\nshould be included, like gate recurrent unit (GRU) RNNLM, dropout strategy for address-\n15"}}, {"id": "a_study_on_neural_network_language_modeling_p1_c3", "metadata": {"chunk_id": 3, "text": "eural network language models is\nthe approximate probabilistic distribution of word sequences from a certain training data\nset rather than the knowledge of a language itself or the information conveyed by word\nsequences in a natural language. Finally, some directions for improving neural network\nlanguage modeling further is discussed.\nKeywords:\nneural network language modeling, optimization techni", "pdf_name": "a_study_on_neural_network_language_modeling", "text_collection": "pdf_a_study_on_neural_network_language_modeling_1", "page": 1}}, {"id": "cl-bench-_a_benchmark_for_context_learnin_p18_c3", "metadata": {"text": "Christoforos Nalmpantis, Ramakanth Pasunuru,\nRoberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard\nGrave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. Transactions\non Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.\nnet/forum?id=jh7wH2AzKK. Survey Certification.\n[45] Grégoire Mialon, Clémentine Fourrier, Thomas W", "pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "page": 18, "text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_18", "chunk_id": 3}}]}, "retrieval_vision": {"image_results": [{"id": "cl-bench-_a_benchmark_for_context_learnin_p18", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/cl-bench-_a_benchmark_for_context_learnin/page_18.png", "text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_18", "pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "page": 18}}, {"id": "cl-bench-_a_benchmark_for_context_learnin_p19", "metadata": {"page": 19, "text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_19", "pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/cl-bench-_a_benchmark_for_context_learnin/page_19.png"}}, {"id": "a_study_on_neural_network_language_modeling_p17", "metadata": {"text_collection": "pdf_a_study_on_neural_network_language_modeling_17", "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/a_study_on_neural_network_language_modeling/page_17.png", "pdf_name": "a_study_on_neural_network_language_modeling", "page": 17}}], "text_results": [{"id": "cl-bench-_a_benchmark_for_context_learnin_p18_c3", "metadata": {"chunk_id": 3, "page": 18, "pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "text": "Christoforos Nalmpantis, Ramakanth Pasunuru,\nRoberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard\nGrave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. Transactions\non Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.\nnet/forum?id=jh7wH2AzKK. Survey Certification.\n[45] Grégoire Mialon, Clémentine Fourrier, Thomas W"}}, {"id": "cl-bench-_a_benchmark_for_context_learnin_p18_c1", "metadata": {"text": "s and where to find them: Overcoming few-shot prompt order sensitivity. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pp.\n8086–8098, 2022.\n[42] Ggaliwango Marvin, Nakayiza Hellen, Daudi Jjingo, and Joyce Nakatumba-Nabende. Prompt\nengineering in large language models. In International conference on data intelligence and\ncognitive informatics, pp. 387–40", "chunk_id": 1, "pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "page": 18}}, {"id": "cl-bench-_a_benchmark_for_context_learnin_p18_c0", "metadata": {"page": 18, "text": "[40] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language\nprocessing. ACM computing surveys, 55(9):1–35, 2023.\n[41] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically\nordered prompts and where to find them: Overcoming few-shot prom", "chunk_id": 0, "pdf_name": "cl-bench-_a_benchmark_for_context_learnin"}}]}, "retrieval_vision_only": {"image_results": [{"id": "cl-bench-_a_benchmark_for_context_learnin_p18", "metadata": {"image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/cl-bench-_a_benchmark_for_context_learnin/page_18.png", "page": 18, "text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_18", "pdf_name": "cl-bench-_a_benchmark_for_context_learnin"}}, {"id": "cl-bench-_a_benchmark_for_context_learnin_p19", "metadata": {"pdf_name": "cl-bench-_a_benchmark_for_context_learnin", "text_collection": "pdf_cl-bench-_a_benchmark_for_context_learnin_19", "page": 19, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/cl-bench-_a_benchmark_for_context_learnin/page_19.png"}}, {"id": "a_study_on_neural_network_language_modeling_p17", "metadata": {"page": 17, "image_path": "/home/xwh/VisRAG/output/api_assets/pdf_images/a_study_on_neural_network_language_modeling/page_17.png", "pdf_name": "a_study_on_neural_network_language_modeling", "text_collection": "pdf_a_study_on_neural_network_language_modeling_17"}}]}, "metrics_text_only": {"f1": 0.08, "rouge_l": 0.08, "bleu_1": 0.06920912606335215, "bleu_2": 7.258714378936715e-06, "bleu_3": 3.423143893801628e-07, "bleu_4": 7.433748760623203e-08, "meteor": 0.072992700729927}, "metrics_vision": {"f1": 0.0, "rouge_l": 0.0, "bleu_1": 1.6529888822158669e-10, "bleu_2": 1.6529888822158669e-10, "bleu_3": 1.6529888822158669e-10, "bleu_4": 1.6529888822158669e-10, "meteor": 0.0}, "metrics_vision_only": {"f1": 0.041666666666666664, "rouge_l": 0.041666666666666664, "bleu_1": 0.02941176470588235, "bleu_2": 5.4232614454664035e-06, "bleu_3": 3.0867895949930455e-07, "bleu_4": 7.364279629037994e-08, "meteor": 0.03125}}
