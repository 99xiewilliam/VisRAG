{"title": "Enriching BERT with Knowledge Graph Embeddings for Document Classification", "question": "By how much do they outperform standard BERT?", "answer": "With this approach we improve the standard BERT models by up to four percentage points in accuracy."}
{"title": "Enriching BERT with Knowledge Graph Embeddings for Document Classification", "question": "What dataset do they use?", "answer": "hierarchical\n\nIn this paper, we work with the dataset of the 2019 GermEval shared task on hierarchical text classification BIBREF0 and use the predefined set of labels to evaluate our approach to this classification task.\n\nOur experiments are modelled on the GermEval 2019 shared task and deal with the classification of books. The dataset contains 20,784 German books."}
{"title": "Enriching BERT with Knowledge Graph Embeddings for Document Classification", "question": "How do they combine text representations with the knowledge graph embeddings?", "answer": "To derive contextualized representations from textual features, the book title and blurb are concatenated and then fed through BERT\n\nThe non-text features are generated in a separate preprocessing step. The metadata features are represented as a ten-dimensional vector (two dimensions for gender, see Section SECREF10). Author embedding vectors have a length of 200 (see Section SECREF22). In the next step, all three representations are concatenated and passed into a MLP with two layers, 1024 units each and ReLu activation function."}
{"title": "Diachronic Topics in New High German Poetry", "question": "What is the algorithm used for the classification tasks?", "answer": "To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers."}
{"title": "Diachronic Topics in New High German Poetry", "question": "Is the outcome of the LDA analysis evaluated in any way?", "answer": "The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%."}
{"title": "Diachronic Topics in New High German Poetry", "question": "What is the corpus used in the study?", "answer": "The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3.\n\nThe Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3. It was mined from http://zeno.org and covers a time period from the mid 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete (e.g. it contains only half of Rilke’s work)."}
{"title": "Important Attribute Identification in Knowledge Graph", "question": "What are the traditional methods to identifying important attributes?", "answer": "In BIBREF0 , BIBREF1 , BIBREF2 , Pasca et al. firstly extract potential class-attribute pairs using linguistically motivated patterns from unstructured text including query logs and query sessions, and then score the attributes using the Bayes model. In BIBREF3 , Rahul Rai proposed to identify product attributes from customer online reviews using part-of-speech(POS) tagging patterns, and to evaluate their importance with several different frequency metrics. In BIBREF4 , Lee et al. developed a system to extract concept-attribute pairs from multiple data sources, such as Probase, general web documents, query logs and external knowledge base, and aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model.\n\nIn BIBREF5 , Li et al. introduced the OntoRank algorithm for ranking the importance of semantic web objects at three levels of granularity: document, terms and RDF graphs. The algorithm is based on the rational surfer model, successfully used in the Swoogle semantic web search engine.\n\nThe existing co-occurrence methods do not suit our application scenario at all, since exact string matching is too strong a requirement and initial trial has shown its incompetency. In stead we implemented an improved version of their method based on TextRank as our baseline. In addition, we also tested multiple semantic matching algorithms for comparison with our chosen method.\n\nTextRank: TextRank is a graph-based ranking model for text processing. BIBREF18 It is an unsupervised algorithm for keyword extraction. Since product attributes are usually the keywords in enquiries, we can compare these keywords with the category attributes and find the most important attributes. This method consists of three steps. The first step is to merge all enquiries under one category as an article. The second step is to extract the top 50 keywords for each category. The third step is to find the most important attributes by comparing top keywords with category attributes.\n\nWord2vec BIBREF19 : We use the word vector trained by BIBREF19 as the distributed representation of words. Then we get the enquiry sentence representation and category attribute representation. Finally we collect the statistics about the matched attributes of each category, and select the most frequent attributes under the same category.\n\nGloVe BIBREF20 : GloVe is a global log-bilinear regression model for the unsupervised learning of word representations, which utilizes the ratios of word-word co-occurrence probabilities. We use the GloVe method to train the distributed representation of words. And attribute selection procedure is the same as word2vec."}
{"title": "Important Attribute Identification in Knowledge Graph", "question": "What do you use to calculate word/sub-word embeddings", "answer": "Evaluating FastText, GloVe and word2vec, we show that compared to other word representation learning algorithms, the FastText performs best."}
{"title": "Important Attribute Identification in Knowledge Graph", "question": "What user generated text data do you use?", "answer": ""}
{"title": "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections", "question": "Did they propose other metrics?", "answer": "We introduce our proposed diversity, density, and homogeneity metrics with their detailed formulations and key intuitions."}
{"title": "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections", "question": "Which real-world datasets did they use?", "answer": "In the first task, we use the SST-2 (Stanford Sentiment Treebank, version 2) dataset BIBREF25 to conduct sentiment analysis experiments. SST-2 is a sentence binary classification dataset with train/dev/test splits provided and two types of sentence labels, i.e., positive and negative.\n\nHere we experiment with the Snips dataset BIBREF26, which is widely used in SLU research. This dataset contains test spoken utterances (text) classified into one of 7 intents.\n\nIn the first task, we use the SST-2 (Stanford Sentiment Treebank, version 2) dataset BIBREF25 to conduct sentiment analysis experiments.\n\nHere we experiment with the Snips dataset BIBREF26, which is widely used in SLU research."}
{"title": "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections", "question": "How did they obtain human intuitions?", "answer": ""}
{"title": "What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016", "question": "What are the country-specific drivers of international development rhetoric?", "answer": "More generally, we know little about the types of development issues that different countries prioritise, or whether country-specific factors such as wealth or democracy make countries more likely to push for specific development issues to be put on the global political agenda.\n\nWe find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional effects)."}
{"title": "What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016", "question": "Is the dataset multilingual?", "answer": "We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 .\n\nFLOAT SELECTED: Fig. 2. Topic quality. 20 highest probability words for the 16-topic model."}
{"title": "What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016", "question": "How are the main international development topics that states raise identified?", "answer": "We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures.\n\nHighly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive.\n\nFollowing BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a “better” exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 ."}
{"title": "QnAMaker: Data to Bot in 2 Minutes", "question": "What experiments do the authors present to validate their system?", "answer": "To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19)."}
{"title": "QnAMaker: Data to Bot in 2 Minutes", "question": "How does the conversation layer work?", "answer": ""}
{"title": "QnAMaker: Data to Bot in 2 Minutes", "question": "What components is the QnAMaker composed of?", "answer": "System description ::: Architecture\n\nThe components involved in the process are:\n\nQnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker.\n\nQnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content.\n\nAzure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.\n\nQnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index.\n\nBot: Calls the WebApp with the User's query to get results.\n\nThe components involved in the process are:\n\nQnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.\n\nQnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.\n\nAzure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.\n\nQnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.\n\nBot: Calls the WebApp with the User's query to get results."}
{"title": "A simple discriminative training method for machine translation with large-scale features", "question": "How they measure robustness in experiments?", "answer": "The log-likelihood of a Plackett-Luce model is not a strict upper bound of the BLEU score, however, it correlates with BLEU well in the case of rich features. The concept of “rich” is actually qualitative, and obscure to define in different applications. We empirically provide a formula to measure the richness in the scenario of machine translation.\n\nThe greater, the richer. In practice, we find a rough threshold of r is 5\n\nFirst, the Plackett-Luce models boost the training BLEU very greatly, even up to 2.5 points higher than MIRA. This verifies our assumption, richer features benefit BLEU, though they are optimized towards a different objective.\n\nSecond, the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$. In PL(1), the over-fitting is quite obvious, the portion in which the curve overpasses MIRA is the smallest compared to other $k$, and its convergent performance is below the baseline. When $k$ is not smaller than 5, the curves are almost above the MIRA line."}
{"title": "A simple discriminative training method for machine translation with large-scale features", "question": "Is new method inferior in terms of robustness to MIRAs in experiments?", "answer": ""}
{"title": "A simple discriminative training method for machine translation with large-scale features", "question": "What experiments with large-scale features are performed?", "answer": "Plackett-Luce Model for SMT Reranking\nAfter being de-duplicated, the N-best list has an average size of around 300, and with 7491 features.\n\nThis experiment displays, in large-scale features, the Plackett-Luce model correlates with BLEU score very well, and alleviates overfitting in some degree."}
{"title": "Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses", "question": "Which ASR system(s) is used in this work?", "answer": "For a given transcribed utterance, it is firstly encoded with Byte Pair Encoding (BPE) BIBREF14, a compression algorithm splitting words to fundamental subword units (pairs of bytes or BPs) and reducing the embedded vocabulary size. Then we use a BiLSTM BIBREF15 encoder and the output state of the BiLSTM is regarded as a vector representation for this utterance. Finally, a fully connected Feed-forward Neural Network (FNN) followed by a softmax layer, labeled as a multilayer perceptron (MLP) module, is used to perform the domain/intent classification task based on the vector.\n\nWe name it Oracle simply because we assume that hypotheses are noisy versions of transcription."}
{"title": "Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses", "question": "What are the series of simple models?", "answer": "Besides the Baseline and Oracle, where only ASR 1-best hypothesis is considered, we also perform experiments to utilize ASR $n$-best hypotheses during evaluation."}
{"title": "Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses", "question": "Over which datasets/corpora is this work evaluated?", "answer": "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains."}
{"title": "DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German", "question": "Is the semantic hierarchy representation used for any task?", "answer": "An extrinsic evaluation was carried out on the task of Open IE BIBREF7.\n\nAs illustrated in Figure FIGREF9, with the help of the semantic hierarchy generated by our discourse-aware sentence splitting approach the output of Open IE systems can be easily enriched with contextual information that allows to restore the semantic relationship between a set of propositions and, hence, preserve their interpretability in downstream tasks."}
{"title": "DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German", "question": "What are the corpora used for the task?", "answer": "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains\n\nThe evaluation of the German version is in progress."}
{"title": "DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German", "question": "Is the model evaluated?", "answer": "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains in order to assess the performance of our framework with regard to the sentence splitting subtask. The results show that our proposed sentence splitting approach outperforms the state of the art in structural TS, returning fine-grained simplified sentences that achieve a high level of grammaticality and preserve the meaning of the input. The full evaluation methodology and detailed results are reported in niklaus-etal-2019-transforming. In addition, a comparative analysis with the annotations contained in the RST Discourse Treebank BIBREF6 demonstrates that we are able to capture the contextual hierarchy between the split sentences with a precision of almost 90% and reach an average precision of approximately 70% for the classification of the rhetorical relations that hold between them. The evaluation of the German version is in progress."}
{"title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "question": "What new metrics are suggested to track progress?", "answer": "We are in line with recent work BIBREF16 , proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores. For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine."}
{"title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "question": "What intrinsic evaluation metrics are used?", "answer": "Tests and Gold-Standard Data for Intrinsic Evaluation\nUsing the gold standard data (described below), we performed three types of tests:\n\nClass Membership Tests: embeddings corresponding two member of the same semantic class (e.g. “Months of the Year\", “Portuguese Cities\", “Smileys\") should be close, since they are supposed to be found in mostly the same contexts.\n\nClass Distinction Test: this is the reciprocal of the previous Class Membership test. Embeddings of elements of different classes should be different, since words of different classes ere expected to be found in significantly different contexts.\n\nWord Equivalence Test: embeddings corresponding to synonyms, antonyms, abbreviations (e.g. “porque\" abbreviated by “pq\") and partial references (e.g. “slb and benfica\") should be almost equal, since both alternatives are supposed to be used be interchangeable in all contexts (either maintaining or inverting the meaning).\n\nTherefore, in our tests, two words are considered:\n\ndistinct if the cosine of the corresponding embeddings is lower than 0.70 (or 0.80).\n\nto belong to the same class if the cosine of their embeddings is higher than 0.70 (or 0.80).\n\nequivalent if the cosine of the embeddings is higher that 0.85 (or 0.95).\n\nFor all these tests we computed a coverage metric. Our embeddings do not necessarily contain information for all the words contained in each of these tests. So, for all tests, we compute a coverage metric that measures the fraction of the gold-standard pairs that could actually be tested using the different embeddings produced.\n\nThen, for all the test pairs actually covered, we obtain the success metrics for each of the 3 tests by computing the ratio of pairs we were able to correctly classified as i) being distinct (cosine INLINEFORM0 0.7 or 0.8), ii) belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), and iii) being equivalent (cosine INLINEFORM2 0.85 or 0.95)."}
{"title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "question": "What experimental results suggest that using less than 50% of the available training examples might result in overfitting?", "answer": "The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 )."}
{"title": "Procedural Reasoning Networks for Understanding Multimodal Procedures", "question": "What multimodality is available in the dataset?", "answer": "Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images.\n\nIn particular, we take advantage of recently proposed RecipeQA dataset BIBREF2, a dataset for multimodal comprehension of cooking recipes, and ask whether it is possible to have a model which employs dynamic representations of entities in answering questions that require multimodal understanding of procedures.\n\nWe report that our proposed approach significantly improves upon previously published results on visual reasoning tasks in RecipeQA, which test understanding causal and temporal relations from images and text."}
{"title": "Procedural Reasoning Networks for Understanding Multimodal Procedures", "question": "What are previously reported models?", "answer": "We compare our model with several baseline models as described below. We note that the results of the first two are previously reported in BIBREF2.\n\nHasty Student BIBREF2 is a heuristics-based simple model which ignores the recipe and gives an answer by examining only the question and the answer set using distances in the visual feature space.\n\nImpatient Reader BIBREF19 is a simple neural model that takes its name from the fact that it repeatedly computes attention over the recipe after observing each image in the query.\n\nBiDAF BIBREF14 is a strong reading comprehension model that employs a bi-directional attention flow mechanism to obtain a question-aware representation and bases its predictions on this representation. Originally, it is a span-selection model from the input context.\n\nBiDAF w/ static memory is an extended version of the BiDAF model which resembles our proposed PRN model in that it includes a memory unit for the entities."}
{"title": "Procedural Reasoning Networks for Understanding Multimodal Procedures", "question": "How better is accuracy of new model compared to previously reported models?", "answer": "Table TABREF29 presents the quantitative results for the visual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models.\n\nIn multi-task training setting where a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF.\n\nFLOAT SELECTED: Table 1: Quantitative comparison of the proposed PRN model against the baselines."}
{"title": "Active Learning for Chinese Word Segmentation in Medical Text", "question": "How does the scoring model work?", "answer": "To select the most appropriate sentences in a large number of unlabeled corpora, we propose a scoring model based on information entropy and neural network as the sampling strategy of active learning, which is inspired by Cai and Zhao BIBREF32 . The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. Fig. FIGREF10 illustrates the entire scoring model. A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model.\n\nThe score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history."}
{"title": "Active Learning for Chinese Word Segmentation in Medical Text", "question": "How does the active learning model work?", "answer": "Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively."}
{"title": "Active Learning for Chinese Word Segmentation in Medical Text", "question": "Which neural network architectures are employed?", "answer": "A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model."}
{"title": "InScript: Narrative texts annotated with script information", "question": "What are the key points in the role of script knowledge that can be studied?", "answer": ""}
{"title": "InScript: Narrative texts annotated with script information", "question": "Did the annotators agreed and how much?", "answer": "FLOAT SELECTED: Figure 4: Inter-annotator agreement statistics.\n\nThe results are shown in Figure 4 and indicate moderate to substantial agreement\n\nFor coreference chain annotation, we calculated the percentage of pairs which were annotated by at least 3 annotators (qualified majority vote) compared to the set of those pairs annotated by at least one person (see Figure 4 ). We take the result of 90.5% between annotators to be a good agreement.\n\nThe results are shown in Figure 4 and indicate moderate to substantial agreement BIBREF5 .\n\nWe take the result of 90.5% between annotators to be a good agreement."}
{"title": "InScript: Narrative texts annotated with script information", "question": "How many subjects have been used to create the annotations?", "answer": "The stories from each scenario were distributed among four different annotators."}
{"title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "question": "What datasets are used to evaluate this approach?", "answer": "FLOAT SELECTED: Table 2: Data Statistics of the benchmarks.\n\nWN18 and YAGO3-10\n\nSecond, we show that our additive attacks can effectively reduce the performance of state of the art models BIBREF2 , BIBREF10 up to $27.3\\%$ and $50.7\\%$ in Hits@1 for two large KGs: WN18 and YAGO3-10."}
{"title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "question": "How is this approach used to detect incorrect facts?", "answer": "if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data.\n\nHere, we demonstrate another potential use of adversarial modifications: finding erroneous triples in the knowledge graph. Intuitively, if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. Formally, to find the incorrect triple $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ in the neighborhood of the train triple $\\langle s, r, o\\rangle $ , we need to find the triple $\\langle s^{\\prime },r^{\\prime },o\\rangle $ that results in the least change $\\Delta _{(s^{\\prime },r^{\\prime })}(s,r,o)$ when removed from the graph."}
{"title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "question": "Can this adversarial approach be used to directly improve model accuracy?", "answer": "When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors."}
{"title": "Learning Supervised Topic Models for Classification and Regression from Crowds", "question": "what are the advantages of the proposed model?", "answer": "The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best.\n\nIn order to assess the computational advantages of the stochastic variational inference (svi) over the batch algorithm, the log marginal likelihood (or log evidence) was plotted against the number of iterations. Fig. FIGREF88 shows this comparison. Not surprisingly, the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm."}
{"title": "Learning Supervised Topic Models for Classification and Regression from Crowds", "question": "what are the state of the art approaches?", "answer": "With the purpose of comparing the proposed model with a popular state-of-the-art approach for image classification, for the LabelMe dataset, the following baseline was introduced:\n\nBosch 2006 (mv): This baseline is similar to one in BIBREF33 . The authors propose the use of pLSA to extract the latent topics, and the use of k-nearest neighbor (kNN) classifier using the documents' topics distributions. For this baseline, unsupervised LDA is used instead of pLSA, and the labels from the different annotators for kNN (with INLINEFORM0 ) are aggregated using majority voting (mv).\n\nThe results obtained by the different approaches for the LabelMe data are shown in Fig. FIGREF94 , where the svi version is using mini-batches of 200 documents.\n\nAnalyzing the results for the Reuters-21578 and LabelMe data, we can observe that MA-sLDAc outperforms all the baselines, with slightly better accuracies for the batch version, especially in the Reuters data. Interestingly, the second best results are consistently obtained by the multi-annotator approaches, which highlights the need for accounting for the noise and biases of the answers of the different annotators.\n\nBoth the batch and the stochastic variational inference (svi) versions of the proposed model (MA-sLDAc) are compared with the following baselines:\n\n[itemsep=0.02cm]\n\nLDA + LogReg (mv): This baseline corresponds to applying unsupervised LDA to the data, and learning a logistic regression classifier on the inferred topics distributions of the documents. The labels from the different annotators were aggregated using majority voting (mv). Notice that, when there is a single annotator label per instance, majority voting is equivalent to using that label for training. This is the case of the 20-Newsgroups' simulated annotators, but the same does not apply for the experiments in Section UID89 .\n\nLDA + Raykar: For this baseline, the model of BIBREF21 was applied using the documents' topic distributions inferred by LDA as features.\n\nLDA + Rodrigues: This baseline is similar to the previous one, but uses the model of BIBREF9 instead.\n\nBlei 2003 (mv): The idea of this baseline is to replicate a popular state-of-the-art approach for document classification. Hence, the approach of BIBREF0 was used. It consists of applying LDA to extract the documents' topics distributions, which are then used to train a SVM. Similarly to the previous approach, the labels from the different annotators were aggregated using majority voting (mv).\n\nsLDA (mv): This corresponds to using the classification version of sLDA BIBREF2 with the labels obtained by performing majority voting (mv) on the annotators' answers."}
{"title": "Learning Supervised Topic Models for Classification and Regression from Crowds", "question": "what datasets were used?", "answer": "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 .\n\nIn order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise."}
{"title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset", "question": "How was the dataset collected?", "answer": "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database.\n\nGoal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context.\n\nDialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals.\n\nDialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories.\n\nThe data collection process is summarized as below:\n\nDatabase Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary.\n\nGoal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal.\n\nDialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.\n\nDialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances."}
{"title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset", "question": "What are the benchmark models?", "answer": "We adapted BERTNLU from ConvLab-2.\n\nWe implemented a rule-based model (RuleDST) and adapted TRADE (Transferable Dialogue State Generator) BIBREF19 in this experiment.\n\nWe adapted a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy)."}
{"title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset", "question": "How was the corpus annotated?", "answer": "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.\n\nDialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances."}
{"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "question": "What models other than standalone BERT is new model compared to?", "answer": "Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin."}
{"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "question": "How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?", "answer": "Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking."}
{"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "question": "What are three downstream task datasets?", "answer": "To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23."}
{"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "question": "What is dataset for word probing task?", "answer": "We evalute Bertram on the WNLaMPro dataset of BIBREF0."}
{"title": "Joint Entity Linking with Deep Reinforcement Learning", "question": "How fast is the model compared to baselines?", "answer": ""}
{"title": "Joint Entity Linking with Deep Reinforcement Learning", "question": "How big is the performance difference between this method and the baseline?", "answer": "FLOAT SELECTED: Table 3: Compare our model with other baseline methods on different types of datasets. The evaluation metric is micro F1."}
{"title": "Joint Entity Linking with Deep Reinforcement Learning", "question": "What datasets used for evaluation?", "answer": "In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets.\n\nAIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.\n\nWe conduct experiments on several different types of public datasets including news and encyclopedia corpus.\n\nAIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.\n\nOURSELF-WIKI is crawled by ourselves from Wikipedia pages."}
{"title": "Joint Entity Linking with Deep Reinforcement Learning", "question": "what are the mentioned cues?", "answer": "FLOAT SELECTED: Figure 2: The overall structure of our RLEL model. It contains three parts: Local Encoder, Global Encoder and Entity Selector. In this framework, (Vmt ,Vekt ) denotes the concatenation of the mention context vector Vmt and one candidate entity vector Vekt . The policy network selects one entity from the candidate set, and Vat denotes the concatenation of the mention context vector Vmt and the selected entity vector Ve∗t . ht represents the hidden status of Vat , and it will be fed into St+1.\n\nAs mentioned in global encoder module, $V_{m_i}^t$4 is the output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7 ."}
{"title": "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "question": "How did the author's work rank among other submissions on the challenge?", "answer": ""}
{"title": "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "question": "What approaches without reinforcement learning have been tried?", "answer": "The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively.\n\nThe bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC).\n\nBased on the findings of Section SECREF3, we apply minimal changes to the deep learning regression models of BIBREF2 to convert them to classification models."}
{"title": "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "question": "What classification approaches were experimented for this task?", "answer": "The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively.\n\nThe table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: “NNC top 5” uses classification labels as described in Section SECREF3, and “NNC SU4 F1” uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence."}
{"title": "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "question": "Did classification models perform better than previous regression one?", "answer": "We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels."}
{"title": "Marrying Universal Dependencies and Universal Morphology", "question": "What are the main sources of recall errors in the mapping?", "answer": "irremediable annotation discrepancies\n\nSome shortcomings of recall come from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase.\n\nWe were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources."}
{"title": "Marrying Universal Dependencies and Universal Morphology", "question": "Do they look for inconsistencies between different languages' annotations in UniMorph?", "answer": "Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources."}
{"title": "Marrying Universal Dependencies and Universal Morphology", "question": "Do they look for inconsistencies between different UD treebanks?", "answer": "The contributions of this work are:"}
{"title": "Marrying Universal Dependencies and Universal Morphology", "question": "Which languages do they validate on?", "answer": "FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method.\n\nFLOAT SELECTED: Table 4: Tagging F1 using UD sentences annotated with either original UD MSDs or UniMorph-converted MSDs\n\nWe apply this conversion to the 31 languages\n\nFinally, with Dutch, the UD annotations are impoverished compared to the UniMorph annotations, and missing attributes cannot be inferred without external knowledge.\n\nSome languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase.\n\nUniMorph's atomic tags have more parts to guess, but they are often related. (E.g. Ipfv always entails Pst in Spanish.) Altogether, these forces seem to have little impact on tagging performance."}
