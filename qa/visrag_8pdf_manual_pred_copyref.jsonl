{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_0", "prediction_text": "DeepSeek-OCR consists of two components: DeepEncoder\nand DeepSeek3B-MoE-A570M as the decoder."}
{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_1", "prediction_text": "Experiments\nshow that when the number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10×), the model can achieve decoding (OCR) precision of 97%."}
{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_2", "prediction_text": "Even at a\ncompression ratio of 20×, the OCR accuracy still remains at about 60%."}
{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_3", "prediction_text": "Figure 1 | Figure (a) shows the compression ratio (number of text tokens in ground truth/number of vision tokens model used) testing on Fox [21] benchmark; Figure (b) shows performance comparisons on OmniDocBench [27]. DeepSeek-OCR can achieve state-of-the-art performance among end-to-end models enjoying the fewest vision tokens."}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_0", "prediction_text": "Here, we\npropose RAGen, a scalable and modular frame-\nwork for generating domain-grounded ques-\ntion–answer–context (QAC) triples tailored\nto diverse RAG adaptation approaches."}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_1", "prediction_text": "Gen produces these QAC triples by identify-\ning key concepts in documents, generating di-\nverse questions guided by Bloom’s Taxonomy-\ninspired principles, and pairing them with\nprecise answers extracted from relevant con-\ntexts."}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_2", "prediction_text": "including the optimization of\nkey components such as the LLM, retriever, and\nembedding model, etc."}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p3_3", "prediction_text": "Figure 1: Overview of RAGen framework, a three-stage process that first extract document concepts and then construct question stems, and finally create Question-Answer-Context datasets. resentation learning, while generation modules typ- ically leverage pre-trained encoder–decoder mod-"}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_0", "prediction_text": "We introduce\nContext Expansion with Parallel Encoding\n(CEPE\n), a framework that can be applied\nto any existing decoder-only LLMs to extend\ntheir context window."}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_1", "prediction_text": "it extends the con-\ntext window of LLAMA-2 to 128K tokens,\noffering 10× the throughput with only 1/6 of\nthe memory."}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_2", "prediction_text": "offering 10× the throughput with only 1/6 of\nthe memory."}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_3", "prediction_text": "Figure 1: A comparison between CEPE and other tech- niques of extending LLMs’ context window, including YARN (Peng et al., 2024), STREAMINGLLM (Xiao et al., 2024b), and REPLUG (Shi et al., 2024). CEPE"}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p1_0", "prediction_text": "This survey introduces neural network reprogramma-\nbility as a unifying framework that bridges mainstream model adaptation techniques–model\nreprogramming, prompt tuning, and prompt instruction–previously fragmented research areas\nyet converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen."}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p1_1", "prediction_text": "converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen."}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p1_2", "prediction_text": "across four key dimensions: manipulation format (fixed or learnable),\nlocation (interfaces where manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream tasks)."}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p2_3", "prediction_text": "Figure 1 | Paradigm shift from conventional parameter-centric adaptation (i.e., modifying model parameters) to reprogrammability-centric adaptation (i.e., modifying input data and model output). This represents a shift in thought from modifying the model to align with the task to modifying the task to align with the model. fine-tuning, where a substantial portion, if not all, of the model’s parameters are updated using task-specific"}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_0", "prediction_text": "it encounters significant challenges in practi-\ncal scenarios where data is inherently discrete\nand fragmented."}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_1", "prediction_text": "Our\ncore insight is simple yet effective: we do\nnot need heavy algorithms to organize this\ndata."}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_2", "prediction_text": "25.2% relative improvement over\nstrong baselines."}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p2_3", "prediction_text": "Figure 1: System overview of Orion-RAG. (1) The Path-Annotation Data Augmentation subsystem (left) employs dual-layer labeling agents to construct hierarchical navigation paths from fragmented text, enabling real-time incremental indexing. (2) The Multi-Layer Hybrid Retrieval subsystem (right) utilizes these paths as explicit logical signposts, integrating sparse and dense search to guide the generator towards accurate and interpretable answers."}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_0", "prediction_text": "However,\nmany existing RAG toolkits lack support for\nknowledge adaptation tailored to specific ap-\nplication scenarios."}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_1", "prediction_text": "we propose UltraRAG, a RAG toolkit that auto-\nmates knowledge adaptation throughout the en-\ntire workflow, from data construction and train-\ning to evaluation, while ensuring ease of use."}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_2", "prediction_text": "UltraRAG features a user-friendly WebUI that\nstreamlines the RAG process, allowing users\nto build and optimize systems without coding\nexpertise."}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p2_3", "prediction_text": "Table 1: Comparison of UltraRAG Features with Other RAG Frameworks. User-Friendly WebUI. UltraRAG provides an intuitive WebUI that allows users to easily deploy RAG systems and efficiently process knowledge"}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p1_0", "prediction_text": "This dual expansion send compute and memory costs skyrocket-\ning, making token compression indispensable."}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p1_1", "prediction_text": "We introduce VISION CENTRIC\nTOKEN COMPRESSION (VIST), a slow–fast compression framework that mir-\nrors human reading: the fast path renders distant tokens into images, letting\na frozen, lightweight vision encoder skim the low-salience context; the slow\npath feeds the proximal window into the LLM for fine-grained reasoning."}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p1_2", "prediction_text": "On eleven in-context\nlearning benchmarks, VIST achieves the same accuracy with 2."}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p2_3", "prediction_text": "Figure 1: Our method VIST adopts a lightweight vision encoder to process loosely relevant long contexts, offering a more cost-efficient alternative to full LLM processing. However, the inherent redundancy in long text leads to redundant visual tokens. Motivated by Selective Reading Strategy where low-frequency (content) words receive longer fixations while high-frequency function words are often skipped, we design Probability-informed"}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p1_0", "prediction_text": "This paper introduces xRAG, a novel context compression method designed specif-\nically for retrieval-augmented generation."}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p1_1", "prediction_text": "In xRAG, the modality bridge is the only\ntrainable component, while the retriever and language model remain frozen."}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p1_2", "prediction_text": "average improvement of over 10% across six\nknowledge-intensive tasks, compatible with various language model backbones,\nranging from a dense 7B model to an 8x7B Mixture of Experts configuration."}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p2_3", "prediction_text": "Figure 1: xRAG enables efficient retrieval augmentation by adding one document token [X]. compression is applied on the surface form. These approaches, however, either require significant memory for storing LLM activations (e.g., 1.05 MB per token as reported by [58]) or suffer from relatively low compression rates. More critically, these methods overlook a crucial characteristic"}
