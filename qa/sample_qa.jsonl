{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_0", "doc": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf", "page": 1, "question": "RAGen框架如何生成领域特定的QAC三元组？", "answer": "RAGen通过识别文档中的关键概念，生成基于Bloom’s Taxonomy原则的多样化问题，并从相关上下文中提取精确答案，从而生成领域特定的QAC三元组。", "context": "Domain-Specific Data Generation Framework for RAG Adaptation\nChris Xing Tian1*\nWeihao Xie2*\nZhen Chen2\nZhengyuan Yi2,\nHui Liu2\nHaoliang Li2\nShiqi Wang2\nSiwei Ma3\n1Peng Cheng Laboratory, Shenzhen, China\n2City University of Hong Kong, Hong Kong SAR\n3Peking University, Beijing, China\ntxsing@live.com, swma@pku.edu.cn\n{weihaxie-c, zchen979-c, liuhui3-c}@my.cityu.edu.hk\n{zhengyyi, haoliang.li, shiqwang}@cityu.edu.hk\nAbstract\nRetrieval-Augmented Generation (RAG) com-\nbines the language understanding and reason-\ning power of large language models (LLMs)\nwith external retrieval to enable domain-\ngrounded responses. Effectively adapting RAG\nsystems to domain-specific settings requires\nspecialized, context-rich training data beyond\ngeneral-purpose question-answering. Here, we\npropose RAGen, a scalable and modular frame-\nwork for generating domain-grounded ques-\ntion–answer–context (QAC) triples tailored\nto diverse RAG adaptation approaches. RA-\nGen produces these QAC triples by identify-\ning key concepts in documents, generating di-\nverse questions guided by Bloom’s Taxonomy-\ninspired principles, and pairing them with\nprecise answers extracted from relevant con-\ntexts. RAGen supports multiple RAG adap-\ntation strategies, including the optimization of\nkey components such as the LLM, retriever, and\nembedding model, etc. Its modular pipeline fea-\ntures semantic chunking, hierarchical concept\nextraction, and multi-chunk retrieval, along\nwith the introduction of curated distractor con-\ntexts to promote robust reasoning. Designed for\nscalability, RAGen efficiently handles large and\nevolving document corpora without redundant\nprocessing, making it especially suitable for\ndynamic evolving domains such as scientific\nresearch and enterprise knowledge bases.\n1\nIntroduction\nWith the growing adoption of large language mod-\nels (LLMs) in enterprise and organizational set-\ntings, there is increasing demand for integrat-\ning these models into domain-specific workflows\n(Chiarello et al., 2024; Qian et al., 2024). However,\nconcerns over data privacy, regulatory compliance,\nand the high cost of commercial API usage often\nprevent organizations from deploying proprietary,\n*Equal contribution.\ncloud-hosted LLMs. As a result, many turn to open-\nsource, locally deployed small- and medium-scale\nLLMs for internal use.\nDespite their accessibility, smaller models in-\nherently suffer from limited language understand-\ning and reasoning capabilities compared to fron-\ntier LLMs (Chen et al., 2024c; Mallen et al.,\n2022). This performance gap motivates the use\nof Retrieval-Augmented Generation (RAG) (Lewis\net al., 2020), which supplements an LLM with a\nretriever to provide external, context-specific in-\nformation. RAG offers a practical and modular\nsolution for grounding LLM outputs in proprietary\nknowledge bases without requiring massive model\nsizes.\nHowever, simply applying off-the-shelf RAG\npipelines to new domains often yields subopti-\nmal results (Barnett et al., 2024). This is because\ngeneral-purpose RAG systems are not tailored to\ndomain-specific data distributions or terminology.\nRAG adaptation, therefore, becomes essential. We\ndefine RAG adaptation as the process of refining\nand optimizing individual components of the RAG\npipeline, including the LLM, retriever, or embed-\nding model, to better align with domain-specific\nrequirements and improve end-to-end performance\n(Siriwardhana et al., 2023; Liu et al., 2025).\nRecent methods such as RAFT (Zhang et al.,\n2024c) introduce distractor-aware fine-tuning to\nimprove the robustness of LLMs in noisy RAG\ncontexts. Meanwhile, inference-time techniques\nlike Self-RAG and Open-RAG (Asai et al., 2023;\nIslam et al., 2024) aim to teach LLMs when and\nhow to retrieve.\nWhile these approaches provide valuable in-\nsights, they are often narrow in scope, each tar-\ngeting only one component of the RAG pipeline.\nHowever, RAG is a multi-stage architecture, and\noptimizing a single module (e.g., just the retriever\nor just the LLM) is insu"}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_0", "doc": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf", "page": 1, "question": "Orion-RAG如何解决传统检索系统在处理碎片化数据时的挑战？", "answer": "Orion-RAG通过使用低复杂度策略提取轻量级‘路径’来链接相关概念，避免需要预构建的知识图谱或复杂全局建模。它直接从碎片化文档中提取自然连接的文本关键词路径，将分散的文件转化为半结构化数据，从而有效跨文件关联信息。", "context": "Orion-RAG: Path-Aligned Hybrid Retrieval for Graphless Data\nZhen Chen1*\nWeihao Xie1*\nPeilin Chen1\nShiqi Wang1\nJianping Wang1\n1City University of Hong Kong, Hong Kong SAR\n{zchen979-c@my., weihaxie-c@my., plchen3@, shiqwang@, jianwang@}cityu.edu.hk\nAbstract\nRetrieval-Augmented Generation (RAG) has\nproven effective for knowledge synthesis, yet\nit encounters significant challenges in practi-\ncal scenarios where data is inherently discrete\nand fragmented.\nIn most environments, in-\nformation is distributed across isolated files\nlike reports and logs that lack explicit links.\nStandard search engines process files indepen-\ndently, ignoring the connections between them.\nFurthermore, manually building Knowledge\nGraphs is impractical for such vast data. To\nbridge this gap, we present Orion-RAG. Our\ncore insight is simple yet effective: we do\nnot need heavy algorithms to organize this\ndata. Instead, we use a low-complexity strat-\negy to extract lightweight “paths” that natu-\nrally link related concepts. We demonstrate that\nthis streamlined approach suffices to transform\nfragmented documents into semi-structured\ndata, enabling the system to link information\nacross different files effectively. Extensive ex-\nperiments demonstrate that Orion-RAG con-\nsistently outperforms mainstream frameworks\nacross diverse domains, supporting real-time\nupdates and explicit Human-in-the-Loop verifi-\ncation with high cost-efficiency. Experiments\non FinanceBench demonstrate superior preci-\nsion with a 25.2% relative improvement over\nstrong baselines.\n1\nIntroduction\nRetrieval-Augmented Generation (RAG) (Lewis\net al., 2021) integrates retrieval mechanisms\n(Salton and McGill, 1983) with large language\nmodels (LLMs) to enhance generation using ex-\nternal data. By combining parametric knowledge\nwith external evidence, RAG has become essential\nin knowledge-intensive domains, such as health-\ncare (Singhal et al., 2023), legal compliance (Cui\net al., 2024), finance (Islam et al., 2023), enterprise\n*Equal contribution.\nsupport (Jiang et al., 2023), and scientific work-\nflows (Taylor et al., 2022). RAG systems generally\noffer better accuracy and document understanding\ncompared to standalone LLMs.\nHowever, deploying RAG in real-world scenar-\nios entails challenges extending beyond accuracy,\nspecifically the enterprise demands for rapid de-\nployment and operational controllability. Funda-\nmentally, real-world data is fragmented: it consists\nof discrete text units with no explicit links con-\nnecting them. For example, when a user queries\na specific company’s revenue, standard retrievers\nmay fail to locate the relevant financial statement.\nThis occurs because the statement contains only nu-\nmerical data without explicitly repeating the com-\npany name, resulting in zero lexical overlap with\nthe query. Beyond fragmentation, a critical bar-\nrier is the absence of pre-constructed graphs in\nreal-world data, which forces heavy Knowledge\nGraph (KG) approaches (Edge et al., 2024; Ya-\nsunaga et al., 2021; Pan et al., 2024) to under-\ntake computationally expensive global construction.\nFurthermore, frequent updates from multiple users\nmake maintaining these rigid structures imprac-\ntical, as re-indexing limits real-time concurrency.\nLastly, “black-box” retrieval lacks the transparency\nrequired for Human-in-the-Loop (HITL) verifica-\ntion, which is critical for industrial adoption.\nTo address these challenges, we propose Orion-\nRAG, a hybrid retrieval framework designed for\nagile and lightweight implementation. It uncovers\nlatent structures without the overhead of pre-built\nKGs or complex global modeling. Our system\nconsists of two main modules:\nPath-Annotation Data Augmentation (see Fig. 1\nleft): This module generates a Path, which is a hi-\nerarchical list of textual keyword tags created by\nour dual-layer labeling system. This path acts as\na navigational “map” for retrieval, where each tag\nserves as a “signpost”. By embedding the path into\na single averaged vector, a query matching any"}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_0", "doc": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf", "page": 1, "question": "UltraRAG如何解决现有RAG工具包在知识适应性方面的不足？", "answer": "UltraRAG通过自动化知识适应在整个工作流中实现，从数据构建、训练到评估，确保知识适应性贯穿始终，同时保持易用性。", "context": "UltraRAG: A Modular and Automated Toolkit for Adaptive\nRetrieval-Augmented Generation\nYuxuan Chen1*, Dewen Guo1*, Sen Mei2*, Xinze Li2*, Hao Chen3, Yishan Li1,\nYixuan Wang3, Chaoyue Tang1, Ruobing Wang4, Dingjun Wu1, Yukun Yan3†\nZhenghao Liu2†, Shi Yu3, Zhiyuan Liu3, Maosong Sun3\n1ModelBest Inc., 2Northeastern University, 3Tsinghua University\n4University of Chinese Academy of Sciences\nAbstract\nRetrieval-Augmented Generation (RAG) sig-\nnificantly enhances the performance of large\nlanguage models (LLMs) in downstream tasks\nby integrating external knowledge. To facilitate\nresearchers in deploying RAG systems, various\nRAG toolkits have been introduced. However,\nmany existing RAG toolkits lack support for\nknowledge adaptation tailored to specific ap-\nplication scenarios. To address this limitation,\nwe propose UltraRAG, a RAG toolkit that auto-\nmates knowledge adaptation throughout the en-\ntire workflow, from data construction and train-\ning to evaluation, while ensuring ease of use.\nUltraRAG features a user-friendly WebUI that\nstreamlines the RAG process, allowing users\nto build and optimize systems without coding\nexpertise. It supports multimodal input and\nprovides comprehensive tools for managing the\nknowledge base. With its highly modular archi-\ntecture, UltraRAG delivers an end-to-end de-\nvelopment solution, enabling seamless knowl-\nedge adaptation across diverse user scenarios.\nThe code, demonstration videos, and installable\npackage for UltraRAG are publicly available at\nhttps://github.com/OpenBMB/UltraRAG.\n1\nIntroduction\nLarge language models (LLMs) (Achiam et al.,\n2023; Touvron et al., 2023; Guo et al., 2025) have\ndemonstrated impressive capabilities in understand-\ning and reasoning. However, due to the limita-\ntions of their parameterized knowledge and hal-\nlucinations, LLMs usually generate incorrect re-\nsponses (Guu et al., 2020; Ji et al., 2023; Xu et al.,\n2024). To address this, retrieval-augmented gener-\nation (RAG) (Lewis et al., 2020; Guu et al., 2020)\nhas emerged as an effective approach that inte-\ngrates external knowledge sources, enhancing the\naccuracy and reliability of responses generated by\nLLMs. Despite its promising potential, RAG still\n* Equal Contribution.\n† Corresponding Authors.\nfaces significant challenges in real-world applica-\ntions. These include the diversity of knowledge\ncorpus formats and modalities (Yu et al., 2024a),\nthe complexity of coordinating multiple compo-\nnents (Li et al., 2024), and the rapid development\nof algorithms and models. These challenges cre-\nate significant obstacles for researchers trying to\ndevelop RAG systems.\nFor these reasons, a variety of RAG toolkits\nhave been developed to offer technical support\nfor researchers (Liu, 2022; Chase, 2022; Jin et al.,\n2024). These tools typically modularize the RAG\nsystem (Jin et al., 2024), enabling users to flexi-\nbly select and configure different modules, which\nstreamlines both deployment and execution. How-\never, existing RAG toolkits are often overly com-\nplex (Chase, 2022) and lack knowledge adaptation\ndesigns tailored to real-world requirements (Jin\net al., 2024), making it difficult for users to cus-\ntomize and optimize RAG systems for specific sce-\nnarios, such as finance and law.\nIn this paper, we propose UltraRAG, a modu-\nlar and automated toolkit for adaptive retrieval-\naugmented generation, enabling users not only to\neasily deploy and execute RAG systems but also\nto enhance the RAG pipeline through knowledge\nadaptation for different scenarios. UltraRAG con-\nsists of two global setting modules (Model Manage-\nment and Knowledge Management) and three core\nfunctional modules (Data construction, Training,\nand Evaluation & Inference), covering all essential\ncomponents of the RAG pipeline. From knowl-\nedge base preparation to automated data genera-\ntion, model fine-tuning, and comprehensive eval-\nuation, UltraRAG streamlines the full lifecycle of\nRAG system development. In summary, UltraRAG\nprovides an end-to-end development platfor"}
