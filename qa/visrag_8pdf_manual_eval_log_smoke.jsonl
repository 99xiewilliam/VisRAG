{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_0", "question": "What are the two components of DeepSeek-OCR?", "reference": "DeepSeek-OCR consists of two components: DeepEncoder\nand DeepSeek3B-MoE-A570M as the decoder.", "prediction": "DeepSeek-OCR consists of two components: DeepEncoder\nand DeepSeek3B-MoE-A570M as the decoder.", "f1": 1.0, "em": 1.0, "meteor": 0.9997724169321802, "rouge_l": 1.0}
{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_1", "question": "What OCR precision is achieved when the compression ratio is < 10×?", "reference": "Experiments\nshow that when the number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10×), the model can achieve decoding (OCR) precision of 97%.", "prediction": "Experiments\nshow that when the number of text tokens is within 10 times that of vision tokens (i.e., a\ncompression ratio < 10×), the model can achieve decoding (OCR) precision of 97%.", "f1": 1.0, "em": 1.0, "meteor": 0.9999794989544467, "rouge_l": 1.0}
{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_2", "question": "What OCR accuracy is reported at a compression ratio of 20×?", "reference": "Even at a\ncompression ratio of 20×, the OCR accuracy still remains at about 60%.", "prediction": "Even at a\ncompression ratio of 20×, the OCR accuracy still remains at about 60%.", "f1": 1.0, "em": 1.0, "meteor": 0.9997724169321802, "rouge_l": 1.0}
{"id": "DeepSeek-OCR- Contexts Optical Compression.pdf_p1_3", "question": "According to Figure 1, what does it describe?", "reference": "Figure 1 | Figure (a) shows the compression ratio (number of text tokens in ground truth/number of vision tokens model used) testing on Fox [21] benchmark; Figure (b) shows performance comparisons on OmniDocBench [27]. DeepSeek-OCR can achieve state-of-the-art performance among end-to-end models enjoying the fewest vision tokens.", "prediction": "Figure 1 | Figure (a) shows the compression ratio (number of text tokens in ground truth/number of vision tokens model used) testing on Fox [21] benchmark; Figure (b) shows performance comparisons on OmniDocBench [27]. DeepSeek-OCR can achieve state-of-the-art performance among end-to-end models enjoying the fewest vision tokens.", "f1": 1.0, "em": 1.0, "meteor": 0.9999957500701239, "rouge_l": 1.0}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_0", "question": "What is RAGen proposed for in this paper?", "reference": "Here, we\npropose RAGen, a scalable and modular frame-\nwork for generating domain-grounded ques-\ntion–answer–context (QAC) triples tailored\nto diverse RAG adaptation approaches.", "prediction": "Here, we\npropose RAGen, a scalable and modular frame-\nwork for generating domain-grounded ques-\ntion–answer–context (QAC) triples tailored\nto diverse RAG adaptation approaches.", "f1": 1.0, "em": 1.0, "meteor": 0.999968, "rouge_l": 1.0}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_1", "question": "How does RAGen produce domain-grounded QAC triples?", "reference": "Gen produces these QAC triples by identify-\ning key concepts in documents, generating di-\nverse questions guided by Bloom’s Taxonomy-\ninspired principles, and pairing them with\nprecise answers extracted from relevant con-\ntexts.", "prediction": "Gen produces these QAC triples by identify-\ning key concepts in documents, generating di-\nverse questions guided by Bloom’s Taxonomy-\ninspired principles, and pairing them with\nprecise answers extracted from relevant con-\ntexts.", "f1": 1.0, "em": 1.0, "meteor": 0.9999872786484836, "rouge_l": 1.0}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_2", "question": "Which key components can RAGen optimize for RAG adaptation?", "reference": "including the optimization of\nkey components such as the LLM, retriever, and\nembedding model, etc.", "prediction": "including the optimization of\nkey components such as the LLM, retriever, and\nembedding model, etc.", "f1": 1.0, "em": 1.0, "meteor": 0.9997724169321802, "rouge_l": 1.0}
{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p3_3", "question": "According to Figure 1, what does it describe?", "reference": "Figure 1: Overview of RAGen framework, a three-stage process that first extract document concepts and then construct question stems, and finally create Question-Answer-Context datasets. resentation learning, while generation modules typ- ically leverage pre-trained encoder–decoder mod-", "prediction": "Figure 1: Overview of RAGen framework, a three-stage process that first extract document concepts and then construct question stems, and finally create Question-Answer-Context datasets. resentation learning, while generation modules typ- ically leverage pre-trained encoder–decoder mod-", "f1": 1.0, "em": 1.0, "meteor": 0.9999915709974881, "rouge_l": 1.0}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_0", "question": "What is CEPE introduced for?", "reference": "We introduce\nContext Expansion with Parallel Encoding\n(CEPE\n), a framework that can be applied\nto any existing decoder-only LLMs to extend\ntheir context window.", "prediction": "We introduce\nContext Expansion with Parallel Encoding\n(CEPE\n), a framework that can be applied\nto any existing decoder-only LLMs to extend\ntheir context window.", "f1": 1.0, "em": 1.0, "meteor": 0.9999638310185185, "rouge_l": 1.0}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_1", "question": "To how many tokens does CEPE extend LLAMA-2's context window?", "reference": "it extends the con-\ntext window of LLAMA-2 to 128K tokens,\noffering 10× the throughput with only 1/6 of\nthe memory.", "prediction": "it extends the con-\ntext window of LLAMA-2 to 128K tokens,\noffering 10× the throughput with only 1/6 of\nthe memory.", "f1": 1.0, "em": 1.0, "meteor": 0.9999375, "rouge_l": 1.0}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_2", "question": "What throughput and memory improvements are claimed for CEPE?", "reference": "offering 10× the throughput with only 1/6 of\nthe memory.", "prediction": "offering 10× the throughput with only 1/6 of\nthe memory.", "f1": 1.0, "em": 1.0, "meteor": 0.9993141289437586, "rouge_l": 1.0}
{"id": "Long-Context Language Modeling with Parallel Context Encoding.pdf_p1_3", "question": "According to Figure 1, what does it describe?", "reference": "Figure 1: A comparison between CEPE and other tech- niques of extending LLMs’ context window, including YARN (Peng et al., 2024), STREAMINGLLM (Xiao et al., 2024b), and REPLUG (Shi et al., 2024). CEPE", "prediction": "Figure 1: A comparison between CEPE and other tech- niques of extending LLMs’ context window, including YARN (Peng et al., 2024), STREAMINGLLM (Xiao et al., 2024b), and REPLUG (Shi et al., 2024). CEPE", "f1": 1.0, "em": 1.0, "meteor": 0.9999847412109375, "rouge_l": 1.0}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p1_0", "question": "What does the survey introduce as a unifying framework?", "reference": "This survey introduces neural network reprogramma-\nbility as a unifying framework that bridges mainstream model adaptation techniques–model\nreprogramming, prompt tuning, and prompt instruction–previously fragmented research areas\nyet converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen.", "prediction": "This survey introduces neural network reprogramma-\nbility as a unifying framework that bridges mainstream model adaptation techniques–model\nreprogramming, prompt tuning, and prompt instruction–previously fragmented research areas\nyet converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen.", "f1": 1.0, "em": 1.0, "meteor": 0.9999951841114204, "rouge_l": 1.0}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p1_1", "question": "What shared principle do these methods converge on?", "reference": "converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen.", "prediction": "converges on a shared principle: repurposing a pre-trained model by manipulating infor-\nmation at the interfaces while keeping the model parameters frozen.", "f1": 1.0, "em": 1.0, "meteor": 0.9999271030762502, "rouge_l": 1.0}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p1_2", "question": "What are the four key dimensions in the taxonomy?", "reference": "across four key dimensions: manipulation format (fixed or learnable),\nlocation (interfaces where manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream tasks).", "prediction": "across four key dimensions: manipulation format (fixed or learnable),\nlocation (interfaces where manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream tasks).", "f1": 1.0, "em": 1.0, "meteor": 0.9999847412109375, "rouge_l": 1.0}
{"id": "Neural Network Reprogrammability- A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction.pdf_p2_3", "question": "According to Figure 1, what does it describe?", "reference": "Figure 1 | Paradigm shift from conventional parameter-centric adaptation (i.e., modifying model parameters) to reprogrammability-centric adaptation (i.e., modifying input data and model output). This represents a shift in thought from modifying the model to align with the task to modifying the task to align with the model. fine-tuning, where a substantial portion, if not all, of the model’s parameters are updated using task-specific", "prediction": "Figure 1 | Paradigm shift from conventional parameter-centric adaptation (i.e., modifying model parameters) to reprogrammability-centric adaptation (i.e., modifying input data and model output). This represents a shift in thought from modifying the model to align with the task to modifying the task to align with the model. fine-tuning, where a substantial portion, if not all, of the model’s parameters are updated using task-specific", "f1": 1.0, "em": 1.0, "meteor": 0.999997902050955, "rouge_l": 1.0}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_0", "question": "What challenge does RAG face in fragmented data scenarios?", "reference": "it encounters significant challenges in practi-\ncal scenarios where data is inherently discrete\nand fragmented.", "prediction": "it encounters significant challenges in practi-\ncal scenarios where data is inherently discrete\nand fragmented.", "f1": 1.0, "em": 1.0, "meteor": 0.9998518518518519, "rouge_l": 1.0}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_1", "question": "What is the core insight of Orion-RAG?", "reference": "Our\ncore insight is simple yet effective: we do\nnot need heavy algorithms to organize this\ndata.", "prediction": "Our\ncore insight is simple yet effective: we do\nnot need heavy algorithms to organize this\ndata.", "f1": 1.0, "em": 1.0, "meteor": 0.999898229187869, "rouge_l": 1.0}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_2", "question": "What relative improvement is reported on FinanceBench?", "reference": "25.2% relative improvement over\nstrong baselines.", "prediction": "25.2% relative improvement over\nstrong baselines.", "f1": 1.0, "em": 1.0, "meteor": 0.9985422740524781, "rouge_l": 1.0}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p2_3", "question": "According to Figure 1, what does it describe?", "reference": "Figure 1: System overview of Orion-RAG. (1) The Path-Annotation Data Augmentation subsystem (left) employs dual-layer labeling agents to construct hierarchical navigation paths from fragmented text, enabling real-time incremental indexing. (2) The Multi-Layer Hybrid Retrieval subsystem (right) utilizes these paths as explicit logical signposts, integrating sparse and dense search to guide the generator towards accurate and interpretable answers.", "prediction": "Figure 1: System overview of Orion-RAG. (1) The Path-Annotation Data Augmentation subsystem (left) employs dual-layer labeling agents to construct hierarchical navigation paths from fragmented text, enabling real-time incremental indexing. (2) The Multi-Layer Hybrid Retrieval subsystem (right) utilizes these paths as explicit logical signposts, integrating sparse and dense search to guide the generator towards accurate and interpretable answers.", "f1": 1.0, "em": 1.0, "meteor": 0.9999975654765093, "rouge_l": 1.0}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_0", "question": "What limitation of existing RAG toolkits does UltraRAG address?", "reference": "However,\nmany existing RAG toolkits lack support for\nknowledge adaptation tailored to specific ap-\nplication scenarios.", "prediction": "However,\nmany existing RAG toolkits lack support for\nknowledge adaptation tailored to specific ap-\nplication scenarios.", "f1": 1.0, "em": 1.0, "meteor": 0.9998779296875, "rouge_l": 1.0}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_1", "question": "What does UltraRAG automate throughout the entire workflow?", "reference": "we propose UltraRAG, a RAG toolkit that auto-\nmates knowledge adaptation throughout the en-\ntire workflow, from data construction and train-\ning to evaluation, while ensuring ease of use.", "prediction": "we propose UltraRAG, a RAG toolkit that auto-\nmates knowledge adaptation throughout the en-\ntire workflow, from data construction and train-\ning to evaluation, while ensuring ease of use.", "f1": 1.0, "em": 1.0, "meteor": 0.9999745973682873, "rouge_l": 1.0}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_2", "question": "What does UltraRAG's WebUI enable according to the abstract?", "reference": "UltraRAG features a user-friendly WebUI that\nstreamlines the RAG process, allowing users\nto build and optimize systems without coding\nexpertise.", "prediction": "UltraRAG features a user-friendly WebUI that\nstreamlines the RAG process, allowing users\nto build and optimize systems without coding\nexpertise.", "f1": 1.0, "em": 1.0, "meteor": 0.9999271030762502, "rouge_l": 1.0}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p2_3", "question": "According to Table 1, what does it describe?", "reference": "Table 1: Comparison of UltraRAG Features with Other RAG Frameworks. User-Friendly WebUI. UltraRAG provides an intuitive WebUI that allows users to easily deploy RAG systems and efficiently process knowledge", "prediction": "Table 1: Comparison of UltraRAG Features with Other RAG Frameworks. User-Friendly WebUI. UltraRAG provides an intuitive WebUI that allows users to easily deploy RAG systems and efficiently process knowledge", "f1": 1.0, "em": 1.0, "meteor": 0.9999794989544467, "rouge_l": 1.0}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p1_0", "question": "Why is token compression considered indispensable in this paper?", "reference": "This dual expansion send compute and memory costs skyrocket-\ning, making token compression indispensable.", "prediction": "This dual expansion send compute and memory costs skyrocket-\ning, making token compression indispensable.", "f1": 1.0, "em": 1.0, "meteor": 0.9998177842565598, "rouge_l": 1.0}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p1_1", "question": "What is VIST's slow–fast compression framework?", "reference": "We introduce VISION CENTRIC\nTOKEN COMPRESSION (VIST), a slow–fast compression framework that mir-\nrors human reading: the fast path renders distant tokens into images, letting\na frozen, lightweight vision encoder skim the low-salience context; the slow\npath feeds the proximal window into the LLM for fine-grained reasoning.", "prediction": "We introduce VISION CENTRIC\nTOKEN COMPRESSION (VIST), a slow–fast compression framework that mir-\nrors human reading: the fast path renders distant tokens into images, letting\na frozen, lightweight vision encoder skim the low-salience context; the slow\npath feeds the proximal window into the LLM for fine-grained reasoning.", "f1": 1.0, "em": 1.0, "meteor": 0.9999937112455507, "rouge_l": 1.0}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p1_2", "question": "How many in-context learning benchmarks are used to evaluate VIST?", "reference": "On eleven in-context\nlearning benchmarks, VIST achieves the same accuracy with 2.", "prediction": "On eleven in-context\nlearning benchmarks, VIST achieves the same accuracy with 2.", "f1": 1.0, "em": 1.0, "meteor": 0.9997106481481481, "rouge_l": 1.0}
{"id": "Vision-centric Token Compression in Large Language Model.pdf_p2_3", "question": "According to Figure 1, what does it describe?", "reference": "Figure 1: Our method VIST adopts a lightweight vision encoder to process loosely relevant long contexts, offering a more cost-efficient alternative to full LLM processing. However, the inherent redundancy in long text leads to redundant visual tokens. Motivated by Selective Reading Strategy where low-frequency (content) words receive longer fixations while high-frequency function words are often skipped, we design Probability-informed", "prediction": "Figure 1: Our method VIST adopts a lightweight vision encoder to process loosely relevant long contexts, offering a more cost-efficient alternative to full LLM processing. However, the inherent redundancy in long text leads to redundant visual tokens. Motivated by Selective Reading Strategy where low-frequency (content) words receive longer fixations while high-frequency function words are often skipped, we design Probability-informed", "f1": 1.0, "em": 1.0, "meteor": 0.9999976851851852, "rouge_l": 1.0}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p1_0", "question": "What is xRAG introduced for?", "reference": "This paper introduces xRAG, a novel context compression method designed specif-\nically for retrieval-augmented generation.", "prediction": "This paper introduces xRAG, a novel context compression method designed specif-\nically for retrieval-augmented generation.", "f1": 1.0, "em": 1.0, "meteor": 0.9998518518518519, "rouge_l": 1.0}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p1_1", "question": "In xRAG, what is the only trainable component?", "reference": "In xRAG, the modality bridge is the only\ntrainable component, while the retriever and language model remain frozen.", "prediction": "In xRAG, the modality bridge is the only\ntrainable component, while the retriever and language model remain frozen.", "f1": 1.0, "em": 1.0, "meteor": 0.9998518518518519, "rouge_l": 1.0}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p1_2", "question": "What average improvement is reported and across how many tasks?", "reference": "average improvement of over 10% across six\nknowledge-intensive tasks, compatible with various language model backbones,\nranging from a dense 7B model to an 8x7B Mixture of Experts configuration.", "prediction": "average improvement of over 10% across six\nknowledge-intensive tasks, compatible with various language model backbones,\nranging from a dense 7B model to an 8x7B Mixture of Experts configuration.", "f1": 1.0, "em": 1.0, "meteor": 0.9999745973682873, "rouge_l": 1.0}
{"id": "xRAG- Extreme Context Compression for Retrieval-augmented Generation with One Token.pdf_p2_3", "question": "According to Figure 1, what does it describe?", "reference": "Figure 1: xRAG enables efficient retrieval augmentation by adding one document token [X]. compression is applied on the surface form. These approaches, however, either require significant memory for storing LLM activations (e.g., 1.05 MB per token as reported by [58]) or suffer from relatively low compression rates. More critically, these methods overlook a crucial characteristic", "prediction": "Figure 1: xRAG enables efficient retrieval augmentation by adding one document token [X]. compression is applied on the surface form. These approaches, however, either require significant memory for storing LLM activations (e.g., 1.05 MB per token as reported by [58]) or suffer from relatively low compression rates. More critically, these methods overlook a crucial characteristic", "f1": 1.0, "em": 1.0, "meteor": 0.9999969947407964, "rouge_l": 1.0}
