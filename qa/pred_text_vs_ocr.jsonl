{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_0", "question": "RAGen框架如何生成领域特定的QAC三元组？", "text_context": "[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p3]\nDocument Concepts Extraction\nConcept–centered Evidence Assembly\n2. Concept Extraction \n…\nChunks\n3. Concepts fusion\nClustering\n…\n4. Retrieval\nChunk level \nConcepts\n…\nExtract\nEvidence\n5. Evidence Extraction\nChunking\nExtract\n1. Semantic Chunking \nfusion \nConcepts\nRetrieval \nChunks\nStems\n6. Generate Question Stems\nCombine\nQAC Generation\n7. Bloom Level Guidance\nHard type\nMedium type\nEasy type\nRemembering | understanding\nApplying | Analyzing\nCreating | Evaluating\n8. Context Variant Builder\nFully\nsupportive\nPartially\nsupportive\nMisleading\nIrrelevant\n9. Multi-LLM Stem Analysis\nQA-pairs\nLLM\nFigure 1: Overview of RAGen framework, a three-stage process that first extract document concepts and then\nconstruct question stems, and finally create Question-Answer-Context datasets.\nresentation learning, while generation modules typ-\nically leverage pre-trained encoder–decoder mod-\nels like BART (Lewis et al., 2019) or T5 (Raf-\nfel et al., 2020). For improving the embedding\nspace, MAFIN (Zhang et al., 2024a) proposes a\nmethod to fine-tune black-box embedding models\nby augmenting them with trainable, open-sourced\nembeddings on domain-specific tasks. To capture\ninter-passage relationships, GraphRAG (Edge et al.,\n2024) models retrieved content as a graph and per-\nforms graph-based traversal during retrieval and\ndecoding. Although effective for structured data,\nGraphRAG relies on predefined graph schemas and\nlacks flexibility in adapting to new domains or dy-\nnamically constructing training data.\nIn terms of LLM optimization, RAFT (Zhang\net al., 2024c) introduces distractor-aware super-\nvision to improve the model’s robustness against\nnoisy or irrelevant contexts. More recent work has\nfocused on inference-time retrieval control, where\nthe LLM actively guides what and when to retrieve.\nRepresentative approaches include Self-RAG (Asai\net al., 2023), OpenRAG (Islam et al., 2024), and\nR1Searcher (Song et al., 2025) which adopt end-to-\nend training paradigms to align retrieval behavior\nwith ge\n\n[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p5]\nthe associated evidences. To guide this process,\nwe adopt Revised Bloom’s Taxonomy (Krathwohl,\n2002), a widely used pedagogical framework that\ncategorizes cognitive learning objectives in ascend-\ning order of complexity:\n• Remembering: Recognizing or recalling infor-\nmation,\n• Understanding: Constructing meaning from\ninformation.\n• Applying: Using knowledge in new situations,\n• Analyzing: Breaking down information into\nparts and finding evidence,\n• Evaluating: Making judgments based on cri-\nteria,\n• Creating: Putting elements together to form a\ncoherent whole.\nBy aligning question types with Bloom’s Taxon-\nomy, we simulate the cognitive learning trajectory\nof humans and enable the generation of questions\nthat span from factual recall to complex synthesis\nand reasoning. This approach allows us to explic-\nitly control the difficulty distribution of the gen-\nerated dataset, ensuring a balanced mix of lower-\norder and higher-order cognitive questions. In addi-\ntion, the flexible combination of stems—especially\nat higher ℓlevels—naturally promotes diversity\nin both content and reasoning depth, enabling the\ndataset to cover a wider range of topics and infer-\nential patterns.\nNotably, for combinations where ℓ≥2, it is\npossible that no meaningful question can be in-\nferred—particularly when the concepts in the stems\nare semantically unrelated. In such cases, we dis-\ncard the current combination and move on to the\nnext.\nBy combining chunk-level concept fusion with\nmulti-stem aggregation, our framework supports\nboth cross-chunk and cross-concept reasoning.\nThis layered design promotes the generation of\nhigh-quality, pedagogically diverse, and cogni-\ntively rich question–answer–context samples suit-\nable for domain-specific RAG adaptation.\nQuestion Generation.\nConditioned on the se-\nlected stem combination and Bloom’s Taxonomy\nlevels, we prompt ChatGPT-4o to generate the ques-\ntion, its reference answer, a concise reasoning trace,\nand the supporting evidences.\nTo enhance retri\n\n[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p6]\nDomain\nCorpus No.\nQuestions No.\nPPFS\n15 /3\n2726 /2502 /2084\nTradePolicy\n20 /5\n1977 /1820 /1500\nBusinessAI\n17 /3\n2228 /2118 /2072\nTable 1: Corpus size (training/evaluation) and number of\ngenerated questions (RAGen / LlamaIndex / AutoRAG)\nfor each domain.\nthat searches for optimal RAG pipeline configu-\nrations on user-provided data, including a built-in\ndataset generation module. 2. LlamaIndex Dataset\nGenerator(LlamaIndex, 2025) – an open-source\nQA data generator for RAG evaluation. We refer\nto it as LlamaIndex in this paper.\nBoth baselines follow a single-chunk question\ngeneration paradigm: AutoRAG uses a simplified\nBloom-style taxonomy (factual/conceptual), while\nLlamaIndex applies intra-chunk retrieval similar to\nour evidence extraction step. We exclude RAGEval\ndue to its reliance on structured schemas, which\nare incompatible with our unstructured corpora.\nEach dataset is constructed from self-contained\ndocuments, enabling standalone QA generation\nwithout cross-document reasoning.\nEvaluation\nsplits are shown in Table 1. We apply the same doc-\nument partitions and maintain comparable question\nvolumes across RAGen, AutoRAG, and LlamaIn-\ndex to ensure fairness.\nTo assess the impact of RAGen data, we conduct\nexperiments on both embedding model customiza-\ntion and LLMs fine-tuning using 4×NVIDIA RTX\n3090 GPUs. Results consistently show that RAGen-\ngenerated datasets lead to improved performance\nacross multiple adaptation settings.\nHyperparameter discussion\nDuring question\ngeneration, all methods segment documents into\n1024-token chunks with a 200-token overlap. For\nsingle-chunk baselines (AutoRAG, LlamaIndex),\nquestion generation is controlled by a single hy-\nperparameter: the number of questions per chunk.\nHowever, this approach is inherently constrained\nby the limited s", "ocr_context": "[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p3]\n0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.\n\n[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p5]\n0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.\n\n[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p6]\n0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.", "combined_context": "[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p3]\nDocument Concepts Extraction\nConcept–centered Evidence Assembly\n2. Concept Extraction \n…\nChunks\n3. Concepts fusion\nClustering\n…\n4. Retrieval\nChunk level \nConcepts\n…\nExtract\nEvidence\n5. Evidence Extraction\nChunking\nExtract\n1. Semantic Chunking \nfusion \nConcepts\nRetrieval \nChunks\nStems\n6. Generate Question Stems\nCombine\nQAC Generation\n7. Bloom Level Guidance\nHard type\nMedium type\nEasy type\nRemembering | understanding\nApplying | Analyzing\nCreating | Evaluating\n8. Context Variant Builder\nFully\nsupportive\nPartially\nsupportive\nMisleading\nIrrelevant\n9. Multi-LLM Stem Analysis\nQA-pairs\nLLM\nFigure 1: Overview of RAGen framework, a three-stage process that first extract document concepts and then\nconstruct question stems, and finally create Question-Answer-Context datasets.\nresentation learning, while generation modules typ-\nically leverage pre-trained encoder–decoder mod-\nels like BART (Lewis et al., 2019) or T5 (Raf-\nfel et al., 2020). For improving the embedding\nspace, MAFIN (Zhang et al., 2024a) proposes a\nmethod to fine-tune black-box embedding models\nby augmenting them with trainable, open-sourced\nembeddings on domain-specific tasks. To capture\ninter-passage relationships, GraphRAG (Edge et al.,\n2024) models retrieved content as a graph and per-\nforms graph-based traversal during retrieval and\ndecoding. Although effective for structured data,\nGraphRAG relies on predefined graph schemas and\nlacks flexibility in adapting to new domains or dy-\nnamically constructing training data.\nIn terms of LLM optimization, RAFT (Zhang\net al., 2024c) introduces distractor-aware super-\nvision to improve the model’s robustness against\nnoisy or irrelevant contexts. More recent work has\nfocused on inference-time retrieval control, where\nthe LLM actively guides what and when to retrieve.\nRepresentative approaches include Self-RAG (Asai\net al., 2023), OpenRAG (Islam et al., 2024), and\nR1Searcher (Song et al., 2025) which adopt end-to-\nend training paradigms to align retrieval behavior\nwith ge\n\n[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p5]\nthe associated evidences. To guide this process,\nwe adopt Revised Bloom’s Taxonomy (Krathwohl,\n2002), a widely used pedagogical framework that\ncategorizes cognitive learning objectives in ascend-\ning order of complexity:\n• Remembering: Recognizing or recalling infor-\nmation,\n• Understanding: Constructing meaning from\ninformation.\n• Applying: Using knowledge in new situations,\n• Analyzing: Breaking down information into\nparts and finding evidence,\n• Evaluating: Making judgments based on cri-\nteria,\n• Creating: Putting elements together to form a\ncoherent whole.\nBy aligning question types with Bloom’s Taxon-\nomy, we simulate the cognitive learning trajectory\nof humans and enable the generation of questions\nthat span from factual recall to complex synthesis\nand reasoning. This approach allows us to explic-\nitly control the difficulty distribution of the gen-\nerated dataset, ensuring a balanced mix of lower-\norder and higher-order cognitive questions. In addi-\ntion, the flexible combination of stems—especially\nat higher ℓlevels—naturally promotes diversity\nin both content and reasoning depth, enabling the\ndataset to cover a wider range of topics and infer-\nential patterns.\nNotably, for combinations where ℓ≥2, it is\npossible that no meaningful question can be in-\nferred—particularly when the concepts in the stems\nare semantically unrelated. In such cases, we dis-\ncard the current combination and move on to the\nnext.\nBy combining chunk-level concept fusion with\nmulti-stem aggregation, our framework supports\nboth cross-chunk and cross-concept reasoning.\nThis layered design promotes the generation of\nhigh-quality, pedagogically diverse, and cogni-\ntively rich question–answer–context samples suit-\nable for domain-specific RAG adaptation.\nQuestion Generation.\nConditioned on the se-\nlected stem combination and Bloom’s Taxonomy\nlevels, we prompt ChatGPT-4o to generate the ques-\ntion, its reference answer, a concise reasoning trace,\nand the supporting evidences.\nTo enhance retri\n\n[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p6]\nDomain\nCorpus No.\nQuestions No.\nPPFS\n15 /3\n2726 /2502 /2084\nTradePolicy\n20 /5\n1977 /1820 /1500\nBusinessAI\n17 /3\n2228 /2118 /2072\nTable 1: Corpus size (training/evaluation) and number of\ngenerated questions (RAGen / LlamaIndex / AutoRAG)\nfor each domain.\nthat searches for optimal RAG pipeline configu-\nrations on user-provided data, including a built-in\ndataset generation module. 2. LlamaIndex Dataset\nGenerator(LlamaIndex, 2025) – an open-source\nQA data generator for RAG evaluation. We refer\nto it as LlamaIndex in this paper.\nBoth baselines follow a single-chunk question\ngeneration paradigm: AutoRAG uses a simplified\nBloom-style taxonomy (factual/conceptual), while\nLlamaIndex applies intra-chunk retrieval similar to\nour evidence extraction step. We exclude RAGEval\ndue to its reliance on structured schemas, which\nare incompatible with our unstructured corpora.\nEach dataset is constructed from self-contained\ndocuments, enabling standalone QA generation\nwithout cross-document reasoning.\nEvaluation\nsplits are shown in Table 1. We apply the same doc-\nument partitions and maintain comparable question\nvolumes across RAGen, AutoRAG, and LlamaIn-\ndex to ensure fairness.\nTo assess the impact of RAGen data, we conduct\nexperiments on both embedding model customiza-\ntion and LLMs fine-tuning using 4×NVIDIA RTX\n3090 GPUs. Results consistently show that RAGen-\ngenerated datasets lead to improved performance\nacross multiple adaptation settings.\nHyperparameter discussion\nDuring question\ngeneration, all methods segment documents into\n1024-token chunks with a 200-token overlap. For\nsingle-chunk baselines (AutoRAG, LlamaIndex),\nquestion generation is controlled by a single hy-\nperparameter: the number of questions per chunk.\nHowever, this approach is inherently constrained\nby the limited s", "prediction_text": "RAGen框架通过以下步骤生成领域特定的QAC三元组：\n\n1. **概念提取**：从文档中提取概念并进行语义块划分。\n2. **概念融合和检索**：将提取的概念进行聚类和融合，并根据块级别进行相关证据的检索。\n3. **问题干扰生成**：结合所选的干扰组合和布鲁姆分类法的不同层次，引导ChatGPT-4o生成问题、参考答案、简洁的推理过程和支持证据。\n\n这种方法确保生成的问题涵盖从事实回忆到复杂推理的各种类型，并能够促进多样性，从而创建适合领域特定RAG适应的高质量问题-答案-上下文样本。", "prediction_with_ocr": "RAGen框架生成领域特定的QAC三元组的过程包含多个步骤：\n\n1. 概念提取：首先从文档中提取概念，以便进行后续的推理和问题生成。\n2. 概念融合和检索：对提取的概念进行融合，寻找相关的证据。\n3. 生成问题框架：根据选定的概念和布loom的分类法生成问题框架。\n4. 问题生成：在选定的问题框架和蓝姆分类法的级别上，用ChatGPT-4o生成问题及其参考答案、简洁的推理轨迹和支撑证据。\n\n这个多层次的设计促进了高质量、教学多样并具认知丰富性的问题-答案-上下文样本的生成，适合领域特定的RAG适应。", "prediction_with_vision_tokens": null, "retrieved_text": [{"doc_id": "Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation", "page": 3}, {"doc_id": "Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation", "page": 5}, {"doc_id": "Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation", "page": 6}], "retrieved_vision": [{"doc_id": "Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation", "page": 3}, {"doc_id": "Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation", "page": 5}, {"doc_id": "Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation", "page": 6}]}
