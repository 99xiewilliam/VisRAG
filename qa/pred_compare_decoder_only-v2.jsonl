{"id": "Domain-Specific Data Generation Framework for RAG Adaptation.pdf_p1_0", "question": "RAGen框架如何生成领域特定的QAC三元组？", "text_context": "[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p3]\nDocument Concepts Extraction\nConcept–centered Evidence Assembly\n2. Concept Extraction \n…\nChunks\n3. Concepts fusion\nClustering\n…\n4. Retrieval\nChunk level \nConcepts\n…\nExtract\nEvidence\n5. Evidence Extraction\nChunking\nExtract\n1. Semantic Chunking \nfusion \nConcepts\nRetrieval \nChunks\nStems\n6. Generate Question Stems\nCombine\nQAC Generation\n7. Bloom Level Guidance\nHard type\nMedium type\nEasy type\nRemembering | understanding\nApplying | Analyzing\nCreating | Evaluating\n8. Context Variant Builder\nFully\nsupportive\nPartially\nsupportive\nMisleading\nIrrelevant\n9. Multi-LLM Stem Analysis\nQA-pairs\nLLM\nFigure 1: Overview of RAGen framework, a three-stage process that first extract document concepts and then\nconstruct question stems, and finally create Question-Answer-Context datasets.\nresentation learning, while generation modules typ-\nically leverage pre-trained encoder–decoder mod-\nels like BART (Lewis et al., 2019) or T5 (Raf-\nfel et al., 2020). For improving the embedding\nspace, MAFIN (Zhang et al., 2024a) proposes a\nmethod to fine-tune black-box embedding models\nby augmenting them with trainable, open-sourced\nembeddings on domain-specific tasks. To capture\ninter-passage relationships, GraphRAG (Edge et al.,\n2024) models retrieved content as a graph and per-\nforms graph-based traversal during retrieval and\ndecoding. Although effective for structured data,\nGraphRAG relies on predefined graph schemas and\nlacks flexibility in adapting to new domains or dy-\nnamically constructing training data.\nIn terms of LLM optimization, RAFT (Zhang\net al., 2024c) introduces distractor-aware super-\nvision to improve the model’s robustness against\nnoisy or irrelevant contexts. More recent work has\nfocused on inference-time retrieval control, where\nthe LLM actively guides what and when to retrieve.\nRepresentative approaches include Self-RAG (Asai\net al., 2023), OpenRAG (Islam et al., 2024), and\nR1Searcher (Song et al., 2025) which adopt end-to-\nend training paradigms to align retrieval behavior\nwith ge\n\n[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p5]\nthe associated evidences. To guide this process,\nwe adopt Revised Bloom’s Taxonomy (Krathwohl,\n2002), a widely used pedagogical framework that\ncategorizes cognitive learning objectives in ascend-\ning order of complexity:\n• Remembering: Recognizing or recalling infor-\nmation,\n• Understanding: Constructing meaning from\ninformation.\n• Applying: Using knowledge in new situations,\n• Analyzing: Breaking down information into\nparts and finding evidence,\n• Evaluating: Making judgments based on cri-\nteria,\n• Creating: Putting elements together to form a\ncoherent whole.\nBy aligning question types with Bloom’s Taxon-\nomy, we simulate the cognitive learning trajectory\nof humans and enable the generation of questions\nthat span from factual recall to complex synthesis\nand reasoning. This approach allows us to explic-\nitly control the difficulty distribution of the gen-\nerated dataset, ensuring a balanced mix of lower-\norder and higher-order cognitive questions. In addi-\ntion, the flexible combination of stems—especially\nat higher ℓlevels—naturally promotes diversity\nin both content and reasoning depth, enabling the\ndataset to cover a wider range of topics and infer-\nential patterns.\nNotably, for combinations where ℓ≥2, it is\npossible that no meaningful question can be in-\nferred—particularly when the concepts in the stems\nare semantically unrelated. In such cases, we dis-\ncard the current combination and move on to the\nnext.\nBy combining chunk-level concept fusion with\nmulti-stem aggregation, our framework supports\nboth cross-chunk and cross-concept reasoning.\nThis layered design promotes the generation of\nhigh-quality, pedagogically diverse, and cogni-\ntively rich question–answer–context samples suit-\nable for domain-specific RAG adaptation.\nQuestion Generation.\nConditioned on the se-\nlected stem combination and Bloom’s Taxonomy\nlevels, we prompt ChatGPT-4o to generate the ques-\ntion, its reference answer, a concise reasoning trace,\nand the supporting evidences.\nTo enhance retri\n\n[Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation p6]\nDomain\nCorpus No.\nQuestions No.\nPPFS\n15 /3\n2726 /2502 /2084\nTradePolicy\n20 /5\n1977 /1820 /1500\nBusinessAI\n17 /3\n2228 /2118 /2072\nTable 1: Corpus size (training/evaluation) and number of\ngenerated questions (RAGen / LlamaIndex / AutoRAG)\nfor each domain.\nthat searches for optimal RAG pipeline configu-\nrations on user-provided data, including a built-in\ndataset generation module. 2. LlamaIndex Dataset\nGenerator(LlamaIndex, 2025) – an open-source\nQA data generator for RAG evaluation. We refer\nto it as LlamaIndex in this paper.\nBoth baselines follow a single-chunk question\ngeneration paradigm: AutoRAG uses a simplified\nBloom-style taxonomy (factual/conceptual), while\nLlamaIndex applies intra-chunk retrieval similar to\nour evidence extraction step. We exclude RAGEval\ndue to its reliance on structured schemas, which\nare incompatible with our unstructured corpora.\nEach dataset is constructed from self-contained\ndocuments, enabling standalone QA generation\nwithout cross-document reasoning.\nEvaluation\nsplits are shown in Table 1. We apply the same doc-\nument partitions and maintain comparable question\nvolumes across RAGen, AutoRAG, and LlamaIn-\ndex to ensure fairness.\nTo assess the impact of RAGen data, we conduct\nexperiments on both embedding model customiza-\ntion and LLMs fine-tuning using 4×NVIDIA RTX\n3090 GPUs. Results consistently show that RAGen-\ngenerated datasets lead to improved performance\nacross multiple adaptation settings.\nHyperparameter discussion\nDuring question\ngeneration, all methods segment documents into\n1024-token chunks with a 200-token overlap. For\nsingle-chunk baselines (AutoRAG, LlamaIndex),\nquestion generation is controlled by a single hy-\nperparameter: the number of questions per chunk.\nHowever, this approach is inherently constrained\nby the limited s", "prediction_text_only": "RAGen框架通过提取和融合文档中的概念和证据，生成领域特定的QAC三元组。", "prediction_with_vision": "RAGen框架通过构建跨领域的概念和事实知识图谱，将文档中的概念和事实知识进行对齐，从而生成领域特定的QAC三元组。", "retrieved_text": [{"doc_id": "Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation", "page": 3}, {"doc_id": "Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation", "page": 5}, {"doc_id": "Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation", "page": 6}], "retrieved_vision": [{"doc_id": "Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation", "page": 3}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Domain-Specific_Data_Generation_Framework_for_RAG_Adaptation/page_3.pt"]}
{"id": "Orion-RAG- Path-Aligned Hybrid Retrieval for Graphless Data.pdf_p1_0", "question": "Orion-RAG如何解决传统检索系统在处理碎片化数据时的挑战？", "text_context": "[Orion-RAG-_Path-Aligned_Hybrid_Retrieval_for_Graphless_Data p1]\nOrion-RAG: Path-Aligned Hybrid Retrieval for Graphless Data\nZhen Chen1*\nWeihao Xie1*\nPeilin Chen1\nShiqi Wang1\nJianping Wang1\n1City University of Hong Kong, Hong Kong SAR\n{zchen979-c@my., weihaxie-c@my., plchen3@, shiqwang@, jianwang@}cityu.edu.hk\nAbstract\nRetrieval-Augmented Generation (RAG) has\nproven effective for knowledge synthesis, yet\nit encounters significant challenges in practi-\ncal scenarios where data is inherently discrete\nand fragmented.\nIn most environments, in-\nformation is distributed across isolated files\nlike reports and logs that lack explicit links.\nStandard search engines process files indepen-\ndently, ignoring the connections between them.\nFurthermore, manually building Knowledge\nGraphs is impractical for such vast data. To\nbridge this gap, we present Orion-RAG. Our\ncore insight is simple yet effective: we do\nnot need heavy algorithms to organize this\ndata. Instead, we use a low-complexity strat-\negy to extract lightweight “paths” that natu-\nrally link related concepts. We demonstrate that\nthis streamlined approach suffices to transform\nfragmented documents into semi-structured\ndata, enabling the system to link information\nacross different files effectively. Extensive ex-\nperiments demonstrate that Orion-RAG con-\nsistently outperforms mainstream frameworks\nacross diverse domains, supporting real-time\nupdates and explicit Human-in-the-Loop verifi-\ncation with high cost-efficiency. Experiments\non FinanceBench demonstrate superior preci-\nsion with a 25.2% relative improvement over\nstrong baselines.\n1\nIntroduction\nRetrieval-Augmented Generation (RAG) (Lewis\net al., 2021) integrates retrieval mechanisms\n(Salton and McGill, 1983) with large language\nmodels (LLMs) to enhance generation using ex-\nternal data. By combining parametric knowledge\nwith external evidence, RAG has become essential\nin knowledge-intensive domains, such as health-\ncare (Singhal et al., 2023), legal compliance (Cui\net al., 2024), finance (Islam et al., 2023), enterprise\n*Equal co\n\n[Orion-RAG-_Path-Aligned_Hybrid_Retrieval_for_Graphless_Data p2]\nsegmentation\nMaster Tag  \nAgent\nParagraph Tag \nAgent\nMaster \nTags \nParagraph  \nTags\nStep1: Construct Path\nDocuments\nConstructed \nPath\nTag Index\nSparse Index\nFull Text Index\nGenerate Master Tags\nGenerate Paragraph Tags\nStep2: Retrieval Augmentation\nFusion Path\nHuman-in- \nthe-loop\nUser Query\nModule 1: Data Augmentation System\nModule 2: Retrieval & Generation System\nRewrite\nFusion Path\nRetrieve Relative Documents\nTarget \nDocuments\nUser Query\nFusion Path\nLLM  \nGenerator\nAnswer\nUser\nGenerate Answer with Logical Consistency\nHits Signpost\nUser\nFigure 1: System overview of Orion-RAG. (1) The Path-Annotation Data Augmentation subsystem (left) employs\ndual-layer labeling agents to construct hierarchical navigation paths from fragmented text, enabling real-time\nincremental indexing. (2) The Multi-Layer Hybrid Retrieval subsystem (right) utilizes these paths as explicit logical\nsignposts, integrating sparse and dense search to guide the generator towards accurate and interpretable answers.\ngle signpost significantly boosts the recall of the\ntarget document. Beyond retrieval, these paths can\nbe injected as structural prompts to the generator,\nenhancing the logical consistency of the answer.\nCrucially, this process operates locally on data seg-\nments, allowing for real-time incremental updates\nas new documents are uploaded by multiple users.\nThe generated “ephemeral knowledge graph” pro-\nvides explicit reasoning paths, making the system\ninterpretable and supporting human-in-the-loop au-\nditing.\nMulti-Layer Hybrid Retrieval (see Fig. 1 right):\nThis module integrates sparse retrieval and dense\nsemantic search with path-based indexing. It uti-\nlizes the paths as explicit logical chains of evi-\ndence. A sophisticated search algorithm is then\nemployed to rigorously optimize retrieval accuracy\nwhile maintaining high computational efficiency.\nOrion-RAG is explicitly engineered for indus-\ntrial scalability. Unlike methods that require com-\nplex recursive processing or global graph cluste\n\n[Orion-RAG-_Path-Aligned_Hybrid_Retrieval_for_Graphless_Data p8]\nTable 2: Generation Performance Comparison across FinanceBench, Mini-Wiki, and SeaCompany (k = 5). Best\nresults are bolded, and best baseline results are underlined.\nFinanceBench\nMini-Wiki\nSeaCompany\nModel\nBERT (F1)\nROUGE-L\nBERT (F1)\nROUGE-L\nBERT (F1)\nROUGE-L\nVSS\n0.8724\n0.1689\n0.8987\n0.4704\n0.8858\n0.2760\nVSS w/ Re-ranker\n0.8707\n0.1739\n0.9053\n0.4874\n0.8867\n0.2846\nSparse\n0.8513\n0.1586\n0.8991\n0.4818\n0.8895\n0.2860\nHybrid\n0.8690\n0.1627\n0.8944\n0.4790\n0.8934\n0.3087\nReAct\n0.7466\n0.0195\n0.9008\n0.5311\n0.8860\n0.2799\nDeepSieve\n0.8380\n0.1081\n0.9005\n0.4973\n0.8522\n0.2211\nRAPTOR\n0.8741\n0.1919\n0.8979\n0.5100\n0.8847\n0.2688\nOrion-RAG (Ours)\n0.8822\n0.2156\n0.9119\n0.5871\n0.8955\n0.3212\nRel. Improv.\n+0.93%\n+12.35%\n+0.73%\n+10.54%\n+0.24%\n+4.05%\nTable 3: Runtime on SeaCompany (8 concurrent).\nMethod\nIndex Const. (s)\nRetrieval (s)\nTotal (s)\nOrion-RAG\n129.57\n99.68\n229.25\nRAPTOR\n164.71\n232.48\n397.19\nReAct-RAG\n≈5.00\n858.14\n863.14\nDeepSieve\n≈5.00\n1182.17\n1187.17\ninvests 129.57s in offline tagging but achieves a\nblazing fast online retrieval of 99.68s—an 8.6×\nspeedup over ReAct. Even compared to RAPTOR,\nwhich also uses pre-computed structures, Orion-\nRAG is significantly faster in both construction and\nretrieval, proving its viability for high-concurrency\nproduction environments.\n4.4\nHuman-in-the-Loop Optimization\nA unique advantage of Orion-RAG is its inter-\npretability: unlike opaque dense vectors, our ex-\nplicit Tag Index allow for precise human interven-\ntion. To validate this, we analyzed a failure case\nwhere the automated tagger missed a query-specific\nconcept.\nAs detailed in Figure 4, the query specifically\ninquired about BDO Unibank’s “diversified busi-\nness model.” The initial automated tags correctly\nidentified general banking concepts (e.g., univer-\nsal banking, financial firm) but missed the s", "prediction_text_only": "Orion-RAG通过构建路径索引和路径推理，将碎片化数据转化为可理解的结构，从而支持高效且可解释的检索。", "prediction_with_vision": "Orion-RAG通过构建路径索引和知识图谱，能够高效地处理碎片化数据，并生成高质量的回答。\n\nFigure 1: Orion-RAG: Path-Aligned Hybrid Retrieval for Graphless Data. (1) Path-Annotation Data Augmentation subsystem (left) constructs hierarchical navigation paths from fragmented text, enabling the retrieval of explicit logical chains of evidence. (2) The Multi-Layer Hybrid Retrieval (see Fig. 1) integrates sparse retrieval and dense semantic search with path-based indexing. (3) Orion-RAG achieves significant speedup over RAPTOR, which uses pre-computed structures. (4) Orion-RAG is significantly faster in both construction and retrieval, proving its viability for high-concurrency production environments.", "retrieved_text": [{"doc_id": "Orion-RAG-_Path-Aligned_Hybrid_Retrieval_for_Graphless_Data", "page": 1}, {"doc_id": "Orion-RAG-_Path-Aligned_Hybrid_Retrieval_for_Graphless_Data", "page": 2}, {"doc_id": "Orion-RAG-_Path-Aligned_Hybrid_Retrieval_for_Graphless_Data", "page": 8}], "retrieved_vision": [{"doc_id": "Orion-RAG-_Path-Aligned_Hybrid_Retrieval_for_Graphless_Data", "page": 1}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/Orion-RAG-_Path-Aligned_Hybrid_Retrieval_for_Graphless_Data/page_1.pt"]}
{"id": "UltraRAG- A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation.pdf_p1_0", "question": "UltraRAG如何解决现有RAG工具包在知识适应性方面的不足？", "text_context": "[UltraRAG-_A_Modular_and_Automated_Toolkit_for_Adaptive_Retrieval-Augmented_Generation p1]\nUltraRAG: A Modular and Automated Toolkit for Adaptive\nRetrieval-Augmented Generation\nYuxuan Chen1*, Dewen Guo1*, Sen Mei2*, Xinze Li2*, Hao Chen3, Yishan Li1,\nYixuan Wang3, Chaoyue Tang1, Ruobing Wang4, Dingjun Wu1, Yukun Yan3†\nZhenghao Liu2†, Shi Yu3, Zhiyuan Liu3, Maosong Sun3\n1ModelBest Inc., 2Northeastern University, 3Tsinghua University\n4University of Chinese Academy of Sciences\nAbstract\nRetrieval-Augmented Generation (RAG) sig-\nnificantly enhances the performance of large\nlanguage models (LLMs) in downstream tasks\nby integrating external knowledge. To facilitate\nresearchers in deploying RAG systems, various\nRAG toolkits have been introduced. However,\nmany existing RAG toolkits lack support for\nknowledge adaptation tailored to specific ap-\nplication scenarios. To address this limitation,\nwe propose UltraRAG, a RAG toolkit that auto-\nmates knowledge adaptation throughout the en-\ntire workflow, from data construction and train-\ning to evaluation, while ensuring ease of use.\nUltraRAG features a user-friendly WebUI that\nstreamlines the RAG process, allowing users\nto build and optimize systems without coding\nexpertise. It supports multimodal input and\nprovides comprehensive tools for managing the\nknowledge base. With its highly modular archi-\ntecture, UltraRAG delivers an end-to-end de-\nvelopment solution, enabling seamless knowl-\nedge adaptation across diverse user scenarios.\nThe code, demonstration videos, and installable\npackage for UltraRAG are publicly available at\nhttps://github.com/OpenBMB/UltraRAG.\n1\nIntroduction\nLarge language models (LLMs) (Achiam et al.,\n2023; Touvron et al., 2023; Guo et al., 2025) have\ndemonstrated impressive capabilities in understand-\ning and reasoning. However, due to the limita-\ntions of their parameterized knowledge and hal-\nlucinations, LLMs usually generate incorrect re-\nsponses (Guu et al., 2020; Ji et al., 2023; Xu et al.,\n2024). To address this, retrieval-augmented gener-\nation (RAG) (Lewis et al., 2020; Guu et al., 2020)\nhas\n\n[UltraRAG-_A_Modular_and_Automated_Toolkit_for_Adaptive_Retrieval-Augmented_Generation p2]\nToolkit\nWebUI\nMultimodal\nKnowledge\nManagement\nEnd-to-End\nDevelopment\nKnowledge\nAdaptation\nLangChain (Chase, 2022)\n×\n✓\n✓\n×\n×\nLlamaIndex (Liu, 2022)\n×\n✓\n✓\n×\n×\nXRAG (Mao et al., 2024)\n✓\n×\n×\n×\n×\nFastRAG (Abane et al., 2024)\n×\n×\n×\n×\n×\nRAGLab (Zhang et al., 2024)\n×\n×\n✓\n×\n×\nLocalRQA (Yu et al., 2024b)\n✓\n×\n×\n✓\n×\nFlashRAG (Jin et al., 2024)\n✓\n✓\n×\n✓\n×\nUltraRAG (Ours)\n✓\n✓\n✓\n✓\n✓\nTable 1: Comparison of UltraRAG Features with Other RAG Frameworks.\nUser-Friendly WebUI. UltraRAG provides an\nintuitive WebUI that allows users to easily deploy\nRAG systems and efficiently process knowledge\nbases, including encoding and indexing documents\nin various formats such as TXT, PDF, and Mark-\ndown. This user-friendly interface significantly\nlowers the barrier to usage, allowing individuals\nwith limited technical expertise to quickly build\nand deploy RAG applications, while reducing both\nthe learning curve and operational complexity.\nMultimodal. UltraRAG supports multimodal\nRAG research and deployment by integrating\nMLLMs like MiniCPM-V (Yao et al., 2024) and\nmultimodal retrievers (Radford et al., 2021; Zhou\net al., 2024). It also incorporates VisRAG (Yu et al.,\n2024a), a model tailored for domain-specific multi-\nmodal scenarios, offering comprehensive technical\nsupport.\nKnowledge Management. UltraRAG enables\nparameterized knowledge base management, trans-\nforming complex processing into simple configura-\ntions. Unlike previous methods (Liu, 2022; Chase,\n2022) that impose format and specification con-\nstraints, UltraRAG supports diverse document for-\nmats, simplifying knowledge base processing.\nEnd-to-End Development. UltraRAG offers\nan end-to-end RAG solution that covers the entire\npipeline, from data construction, model fine-tuning\nto inference and evaluation. It integrates advanced\nRAG algorithms (Li et al., 2024; Zeng et al., 2024;\nYu et al., 2024a), allowing users to freely combine\nvarious techniques and explore numerous configu-\nration possibilities. In addition, UltraRAG includes\nover 40\n\n[UltraRAG-_A_Modular_and_Automated_Toolkit_for_Adaptive_Retrieval-Augmented_Generation p3]\nutilize a rollout method to obtain rewards from\nthe entire RAG system for each module and opti-\nmize them based on the reward. The success of\nthese approaches highlights the growing need for a\ngeneral-purpose RAG toolkit, which can streamline\ndevelopment and evaluation across diverse RAG\nframeworks.\nRAG Toolkits. Various RAG toolkits have been\ndeveloped to assist users in building customized\nRAG systems, such as LangChain (Chase, 2022)\nand LlamaIndex (Liu, 2022). These frameworks\nmodularize RAG pipelines and offer seamless in-\ntegration with knowledge databases, embedding\nmodels, and LLM APIs, thereby streamlining de-\nvelopment workflows and broadening their range\nof applications (deepset.ai, 2023). However, most\nexisting toolkits lack user-friendly WebUIs, do not\noffer free access to commonly used retrieval cor-\npora, and tend to be overly encapsulated. These\nlimitations significantly hinder their usability and\nscalability, making them less suitable for both re-\nsearch and practical deployment scenarios (Mao\net al., 2024; Jin et al., 2024).\nTo address these limitations, recent work has\nintroduced more transparent and adaptable RAG\ntoolkits.\nFor example, FastRAG (Abane et al.,\n2024) is built upon Haystack’s API, allowing users\nto freely assemble different modules within RAG\npipelines. RAGLAB (Zhang et al., 2024) focuses\non training RAG systems, offering training strate-\ngies tailored for different components. However,\nthese toolkits do not adequately support users in\nend-to-end deployment and development and are\nnot applicable to multimodal tasks. FlashRAG (Jin\net al., 2024) not only addresses several of these\nchallenges but also integrates multiple algorithms,\nallowing users to efficiently repr", "prediction_text_only": "UltraRAG通过提供用户友好的WebUI和模块化设计，解决了现有RAG工具包在知识适应性方面的不足。", "prediction_with_vision": "UltraRAG通过引入外部知识库，如WebUI和Knowledge Graph, 增强了RAG系统的知识适应性。此外，UltraRAG还支持多模态知识库，如文本、图像和表格，从而支持跨模态任务。", "retrieved_text": [{"doc_id": "UltraRAG-_A_Modular_and_Automated_Toolkit_for_Adaptive_Retrieval-Augmented_Generation", "page": 1}, {"doc_id": "UltraRAG-_A_Modular_and_Automated_Toolkit_for_Adaptive_Retrieval-Augmented_Generation", "page": 2}, {"doc_id": "UltraRAG-_A_Modular_and_Automated_Toolkit_for_Adaptive_Retrieval-Augmented_Generation", "page": 3}], "retrieved_vision": [{"doc_id": "UltraRAG-_A_Modular_and_Automated_Toolkit_for_Adaptive_Retrieval-Augmented_Generation", "page": 1}], "vision_tokens_paths": ["/home/xwh/VisRAG/output_reindex_1536/vision_tokens/UltraRAG-_A_Modular_and_Automated_Toolkit_for_Adaptive_Retrieval-Augmented_Generation/page_1.pt"]}
