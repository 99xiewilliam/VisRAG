{"title": "Enriching BERT with Knowledge Graph Embeddings for Document Classification", "question": "By how much do they outperform standard BERT?", "answer": "With this approach we improve the standard BERT models by up to four percentage points in accuracy.", "id": "enriching_bert_with_knowledge_graph_embeddings_for_document_classification_0"}
{"title": "Enriching BERT with Knowledge Graph Embeddings for Document Classification", "question": "How do they combine text representations with the knowledge graph embeddings?", "answer": "To derive contextualized representations from textual features, the book title and blurb are concatenated and then fed through BERT\n\nThe non-text features are generated in a separate preprocessing step. The metadata features are represented as a ten-dimensional vector (two dimensions for gender, see Section SECREF10). Author embedding vectors have a length of 200 (see Section SECREF22). In the next step, all three representations are concatenated and passed into a MLP with two layers, 1024 units each and ReLu activation function.", "id": "enriching_bert_with_knowledge_graph_embeddings_for_document_classification_2"}
{"title": "Diachronic Topics in New High German Poetry", "question": "What is the algorithm used for the classification tasks?", "answer": "To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers.", "id": "diachronic_topics_in_new_high_german_poetry_3"}
{"title": "Diachronic Topics in New High German Poetry", "question": "Is the outcome of the LDA analysis evaluated in any way?", "answer": "The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%.", "id": "diachronic_topics_in_new_high_german_poetry_4"}
{"title": "Important Attribute Identification in Knowledge Graph", "question": "What are the traditional methods to identifying important attributes?", "answer": "In BIBREF0 , BIBREF1 , BIBREF2 , Pasca et al. firstly extract potential class-attribute pairs using linguistically motivated patterns from unstructured text including query logs and query sessions, and then score the attributes using the Bayes model. In BIBREF3 , Rahul Rai proposed to identify product attributes from customer online reviews using part-of-speech(POS) tagging patterns, and to evaluate their importance with several different frequency metrics. In BIBREF4 , Lee et al. developed a system to extract concept-attribute pairs from multiple data sources, such as Probase, general web documents, query logs and external knowledge base, and aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model.\n\nIn BIBREF5 , Li et al. introduced the OntoRank algorithm for ranking the importance of semantic web objects at three levels of granularity: document, terms and RDF graphs. The algorithm is based on the rational surfer model, successfully used in the Swoogle semantic web search engine.\n\nThe existing co-occurrence methods do not suit our application scenario at all, since exact string matching is too strong a requirement and initial trial has shown its incompetency. In stead we implemented an improved version of their method based on TextRank as our baseline. In addition, we also tested multiple semantic matching algorithms for comparison with our chosen method.\n\nTextRank: TextRank is a graph-based ranking model for text processing. BIBREF18 It is an unsupervised algorithm for keyword extraction. Since product attributes are usually the keywords in enquiries, we can compare these keywords with the category attributes and find the most important attributes. This method consists of three steps. The first step is to merge all enquiries under one category as an article. The second step is to extract the top 50 keywords for each category. The third step is to find the most important attributes by comparing top keywords with category attributes.\n\nWord2vec BIBREF19 : We use the word vector trained by BIBREF19 as the distributed representation of words. Then we get the enquiry sentence representation and category attribute representation. Finally we collect the statistics about the matched attributes of each category, and select the most frequent attributes under the same category.\n\nGloVe BIBREF20 : GloVe is a global log-bilinear regression model for the unsupervised learning of word representations, which utilizes the ratios of word-word co-occurrence probabilities. We use the GloVe method to train the distributed representation of words. And attribute selection procedure is the same as word2vec.", "id": "important_attribute_identification_in_knowledge_graph_6"}
{"title": "Important Attribute Identification in Knowledge Graph", "question": "What do you use to calculate word/sub-word embeddings", "answer": "Evaluating FastText, GloVe and word2vec, we show that compared to other word representation learning algorithms, the FastText performs best.", "id": "important_attribute_identification_in_knowledge_graph_7"}
{"title": "What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016", "question": "What are the country-specific drivers of international development rhetoric?", "answer": "More generally, we know little about the types of development issues that different countries prioritise, or whether country-specific factors such as wealth or democracy make countries more likely to push for specific development issues to be put on the global political agenda.\n\nWe find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional effects).", "id": "what_drives_the_international_development_agenda_an_nlp_analysis_of_the_united_n_12"}
{"title": "What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016", "question": "How are the main international development topics that states raise identified?", "answer": "We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures.\n\nHighly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive.\n\nFollowing BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a “better” exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 .", "id": "what_drives_the_international_development_agenda_an_nlp_analysis_of_the_united_n_14"}
{"title": "QnAMaker: Data to Bot in 2 Minutes", "question": "What experiments do the authors present to validate their system?", "answer": "To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19).", "id": "qnamaker_data_to_bot_in_2_minutes_15"}
{"title": "QnAMaker: Data to Bot in 2 Minutes", "question": "What components is the QnAMaker composed of?", "answer": "System description ::: Architecture\n\nThe components involved in the process are:\n\nQnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker.\n\nQnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content.\n\nAzure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.\n\nQnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index.\n\nBot: Calls the WebApp with the User's query to get results.\n\nThe components involved in the process are:\n\nQnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.\n\nQnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.\n\nAzure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.\n\nQnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.\n\nBot: Calls the WebApp with the User's query to get results.", "id": "qnamaker_data_to_bot_in_2_minutes_17"}
{"title": "A simple discriminative training method for machine translation with large-scale features", "question": "What experiments with large-scale features are performed?", "answer": "Plackett-Luce Model for SMT Reranking\nAfter being de-duplicated, the N-best list has an average size of around 300, and with 7491 features.\n\nThis experiment displays, in large-scale features, the Plackett-Luce model correlates with BLEU score very well, and alleviates overfitting in some degree.", "id": "a_simple_discriminative_training_method_for_machine_translation_with_large-scale_20"}
{"title": "Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses", "question": "Which ASR system(s) is used in this work?", "answer": "For a given transcribed utterance, it is firstly encoded with Byte Pair Encoding (BPE) BIBREF14, a compression algorithm splitting words to fundamental subword units (pairs of bytes or BPs) and reducing the embedded vocabulary size. Then we use a BiLSTM BIBREF15 encoder and the output state of the BiLSTM is regarded as a vector representation for this utterance. Finally, a fully connected Feed-forward Neural Network (FNN) followed by a softmax layer, labeled as a multilayer perceptron (MLP) module, is used to perform the domain/intent classification task based on the vector.\n\nWe name it Oracle simply because we assume that hypotheses are noisy versions of transcription.", "id": "improving_spoken_language_understanding_by_exploiting_asr_n-best_hypotheses_21"}
{"title": "DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German", "question": "Is the semantic hierarchy representation used for any task?", "answer": "An extrinsic evaluation was carried out on the task of Open IE BIBREF7.\n\nAs illustrated in Figure FIGREF9, with the help of the semantic hierarchy generated by our discourse-aware sentence splitting approach the output of Open IE systems can be easily enriched with contextual information that allows to restore the semantic relationship between a set of propositions and, hence, preserve their interpretability in downstream tasks.", "id": "dissim_a_discourse-aware_syntactic_text_simplification_frameworkfor_english_and__24"}
{"title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "question": "What experimental results suggest that using less than 50% of the available training examples might result in overfitting?", "answer": "The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ).", "id": "learning_word_embeddings_from_the_portuguese_twitter_stream_a_study_of_some_prac_29"}
{"title": "Procedural Reasoning Networks for Understanding Multimodal Procedures", "question": "What multimodality is available in the dataset?", "answer": "Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images.\n\nIn particular, we take advantage of recently proposed RecipeQA dataset BIBREF2, a dataset for multimodal comprehension of cooking recipes, and ask whether it is possible to have a model which employs dynamic representations of entities in answering questions that require multimodal understanding of procedures.\n\nWe report that our proposed approach significantly improves upon previously published results on visual reasoning tasks in RecipeQA, which test understanding causal and temporal relations from images and text.", "id": "procedural_reasoning_networks_for_understanding_multimodal_procedures_30"}
{"title": "Procedural Reasoning Networks for Understanding Multimodal Procedures", "question": "How better is accuracy of new model compared to previously reported models?", "answer": "Table TABREF29 presents the quantitative results for the visual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models.\n\nIn multi-task training setting where a single model is trained to solve all the tasks at once, PRN and BIDAF w/ static memory perform comparably and give much better results than BIDAF.\n\nFLOAT SELECTED: Table 1: Quantitative comparison of the proposed PRN model against the baselines.", "id": "procedural_reasoning_networks_for_understanding_multimodal_procedures_32"}
{"title": "Active Learning for Chinese Word Segmentation in Medical Text", "question": "How does the scoring model work?", "answer": "To select the most appropriate sentences in a large number of unlabeled corpora, we propose a scoring model based on information entropy and neural network as the sampling strategy of active learning, which is inspired by Cai and Zhao BIBREF32 . The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. Fig. FIGREF10 illustrates the entire scoring model. A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model.\n\nThe score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history.", "id": "active_learning_for_chinese_word_segmentation_in_medical_text_33"}
{"title": "Active Learning for Chinese Word Segmentation in Medical Text", "question": "How does the active learning model work?", "answer": "Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.", "id": "active_learning_for_chinese_word_segmentation_in_medical_text_34"}
{"title": "Active Learning for Chinese Word Segmentation in Medical Text", "question": "Which neural network architectures are employed?", "answer": "A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model.", "id": "active_learning_for_chinese_word_segmentation_in_medical_text_35"}
{"title": "InScript: Narrative texts annotated with script information", "question": "How many subjects have been used to create the annotations?", "answer": "The stories from each scenario were distributed among four different annotators.", "id": "inscript_narrative_texts_annotated_with_script_information_38"}
{"title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "question": "How is this approach used to detect incorrect facts?", "answer": "if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data.\n\nHere, we demonstrate another potential use of adversarial modifications: finding erroneous triples in the knowledge graph. Intuitively, if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. Formally, to find the incorrect triple $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ in the neighborhood of the train triple $\\langle s, r, o\\rangle $ , we need to find the triple $\\langle s^{\\prime },r^{\\prime },o\\rangle $ that results in the least change $\\Delta _{(s^{\\prime },r^{\\prime })}(s,r,o)$ when removed from the graph.", "id": "investigating_robustness_and_interpretability_of_link_prediction_via_adversarial_40"}
{"title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "question": "Can this adversarial approach be used to directly improve model accuracy?", "answer": "When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors.", "id": "investigating_robustness_and_interpretability_of_link_prediction_via_adversarial_41"}
{"title": "Learning Supervised Topic Models for Classification and Regression from Crowds", "question": "what are the advantages of the proposed model?", "answer": "The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best.\n\nIn order to assess the computational advantages of the stochastic variational inference (svi) over the batch algorithm, the log marginal likelihood (or log evidence) was plotted against the number of iterations. Fig. FIGREF88 shows this comparison. Not surprisingly, the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm.", "id": "learning_supervised_topic_models_for_classification_and_regression_from_crowds_42"}
{"title": "Learning Supervised Topic Models for Classification and Regression from Crowds", "question": "what are the state of the art approaches?", "answer": "With the purpose of comparing the proposed model with a popular state-of-the-art approach for image classification, for the LabelMe dataset, the following baseline was introduced:\n\nBosch 2006 (mv): This baseline is similar to one in BIBREF33 . The authors propose the use of pLSA to extract the latent topics, and the use of k-nearest neighbor (kNN) classifier using the documents' topics distributions. For this baseline, unsupervised LDA is used instead of pLSA, and the labels from the different annotators for kNN (with INLINEFORM0 ) are aggregated using majority voting (mv).\n\nThe results obtained by the different approaches for the LabelMe data are shown in Fig. FIGREF94 , where the svi version is using mini-batches of 200 documents.\n\nAnalyzing the results for the Reuters-21578 and LabelMe data, we can observe that MA-sLDAc outperforms all the baselines, with slightly better accuracies for the batch version, especially in the Reuters data. Interestingly, the second best results are consistently obtained by the multi-annotator approaches, which highlights the need for accounting for the noise and biases of the answers of the different annotators.\n\nBoth the batch and the stochastic variational inference (svi) versions of the proposed model (MA-sLDAc) are compared with the following baselines:\n\n[itemsep=0.02cm]\n\nLDA + LogReg (mv): This baseline corresponds to applying unsupervised LDA to the data, and learning a logistic regression classifier on the inferred topics distributions of the documents. The labels from the different annotators were aggregated using majority voting (mv). Notice that, when there is a single annotator label per instance, majority voting is equivalent to using that label for training. This is the case of the 20-Newsgroups' simulated annotators, but the same does not apply for the experiments in Section UID89 .\n\nLDA + Raykar: For this baseline, the model of BIBREF21 was applied using the documents' topic distributions inferred by LDA as features.\n\nLDA + Rodrigues: This baseline is similar to the previous one, but uses the model of BIBREF9 instead.\n\nBlei 2003 (mv): The idea of this baseline is to replicate a popular state-of-the-art approach for document classification. Hence, the approach of BIBREF0 was used. It consists of applying LDA to extract the documents' topics distributions, which are then used to train a SVM. Similarly to the previous approach, the labels from the different annotators were aggregated using majority voting (mv).\n\nsLDA (mv): This corresponds to using the classification version of sLDA BIBREF2 with the labels obtained by performing majority voting (mv) on the annotators' answers.", "id": "learning_supervised_topic_models_for_classification_and_regression_from_crowds_43"}
{"title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset", "question": "How was the dataset collected?", "answer": "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database.\n\nGoal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context.\n\nDialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals.\n\nDialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories.\n\nThe data collection process is summarized as below:\n\nDatabase Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary.\n\nGoal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal.\n\nDialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.\n\nDialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances.", "id": "crosswoz_a_large-scale_chinese_cross-domain_task-oriented_dialogue_dataset_45"}
{"title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset", "question": "What are the benchmark models?", "answer": "We adapted BERTNLU from ConvLab-2.\n\nWe implemented a rule-based model (RuleDST) and adapted TRADE (Transferable Dialogue State Generator) BIBREF19 in this experiment.\n\nWe adapted a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy).", "id": "crosswoz_a_large-scale_chinese_cross-domain_task-oriented_dialogue_dataset_46"}
{"title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset", "question": "How was the corpus annotated?", "answer": "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.\n\nDialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances.", "id": "crosswoz_a_large-scale_chinese_cross-domain_task-oriented_dialogue_dataset_47"}
{"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "question": "What models other than standalone BERT is new model compared to?", "answer": "Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin.", "id": "bertram_improved_word_embeddings_have_big_impact_on_contextualized_model_perform_48"}
{"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "question": "How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?", "answer": "Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking.", "id": "bertram_improved_word_embeddings_have_big_impact_on_contextualized_model_perform_49"}
{"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "question": "What are three downstream task datasets?", "answer": "To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23.", "id": "bertram_improved_word_embeddings_have_big_impact_on_contextualized_model_perform_50"}
{"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "question": "What is dataset for word probing task?", "answer": "We evalute Bertram on the WNLaMPro dataset of BIBREF0.", "id": "bertram_improved_word_embeddings_have_big_impact_on_contextualized_model_perform_51"}
{"title": "Joint Entity Linking with Deep Reinforcement Learning", "question": "How big is the performance difference between this method and the baseline?", "answer": "FLOAT SELECTED: Table 3: Compare our model with other baseline methods on different types of datasets. The evaluation metric is micro F1.", "id": "joint_entity_linking_with_deep_reinforcement_learning_53"}
{"title": "Joint Entity Linking with Deep Reinforcement Learning", "question": "What datasets used for evaluation?", "answer": "In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets.\n\nAIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.\n\nWe conduct experiments on several different types of public datasets including news and encyclopedia corpus.\n\nAIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.\n\nOURSELF-WIKI is crawled by ourselves from Wikipedia pages.", "id": "joint_entity_linking_with_deep_reinforcement_learning_54"}
{"title": "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "question": "What approaches without reinforcement learning have been tried?", "answer": "The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively.\n\nThe bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC).\n\nBased on the findings of Section SECREF3, we apply minimal changes to the deep learning regression models of BIBREF2 to convert them to classification models.", "id": "classification_betters_regression_in_query-based_multi-document_summarisation_te_57"}
{"title": "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "question": "What classification approaches were experimented for this task?", "answer": "The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively.\n\nThe table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: “NNC top 5” uses classification labels as described in Section SECREF3, and “NNC SU4 F1” uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence.", "id": "classification_betters_regression_in_query-based_multi-document_summarisation_te_58"}
{"title": "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "question": "Did classification models perform better than previous regression one?", "answer": "We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels.", "id": "classification_betters_regression_in_query-based_multi-document_summarisation_te_59"}
{"title": "Marrying Universal Dependencies and Universal Morphology", "question": "What are the main sources of recall errors in the mapping?", "answer": "irremediable annotation discrepancies\n\nSome shortcomings of recall come from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase.\n\nWe were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources.", "id": "marrying_universal_dependencies_and_universal_morphology_60"}
{"title": "Marrying Universal Dependencies and Universal Morphology", "question": "Do they look for inconsistencies between different languages' annotations in UniMorph?", "answer": "Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources.", "id": "marrying_universal_dependencies_and_universal_morphology_61"}
{"title": "Marrying Universal Dependencies and Universal Morphology", "question": "Do they look for inconsistencies between different UD treebanks?", "answer": "The contributions of this work are:", "id": "marrying_universal_dependencies_and_universal_morphology_62"}
