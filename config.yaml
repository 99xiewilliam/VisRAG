# VisRAG 配置文件
# 支持本地模型和 OpenAI API 两种模式

# ==================== Generator 配置 ====================
generator:
  # 可选: "local" | "openai" | "deepseek_ocr2"
  # - deepseek_ocr2: 使用 DeepSeek-OCR-2 的 decoder，支持 text-only vs (vision_tokens+text) A/B
  backend: "decoder_only"
  
  # ------------------ 本地模型配置 ------------------
  local:
    # 模型路径或 HuggingFace 模型名称
    model_path: "/data/xwh/models/Qwen3-1.7B"
    # 最大生成 token 数
    max_new_tokens: 768
    # 是否使用 4-bit 量化（节省显存）
    load_in_4bit: false
    # 推理参数
    temperature: 0.0
    top_p: 1.0
    # 设备选择: "auto", "cuda", "cpu"
    device: "auto"
  
  # ------------------ OpenAI API 配置 ------------------
  openai:
    # （当前 pipeline 不使用 OpenAI；如需启用请改 backend=openai 并用环境变量配置密钥）
    api_key: null
    base_url: null
    # 模型名称
    model: "gpt-4o-mini"
    # 最大生成 token 数
    max_tokens: 768
    # 推理参数
    temperature: 0.0
    top_p: 1.0
    # 请求超时（秒）
    timeout: 60
    # 重试次数
    max_retries: 3

# ==================== 嵌入模型配置 ====================
embedding:
  # 可选: "hash" | "local" | "openai"
  # - hash: 使用 MD5 hash（轻量级，无额外依赖）
  # - local: 使用本地 HuggingFace embedding 模型
  # - openai: 使用 OpenAI embedding API
  backend: "openai"
  
  # 输出维度（hash 模式会忽略此配置，使用内部固定值）
  dim: 1024
  
  # ------------------ 本地模型配置 ------------------
  local:
    # 模型路径或 HuggingFace 模型名称
    model_path: "/data/xwh/models/Qwen3-Embedding-0.6B"
    # 设备选择: "auto", "cuda", "cpu"
    device: "auto"
    # 批处理大小
    batch_size: 32
    # 最大序列长度
    max_length: 512
    # 是否使用 fp16（节省显存）
    use_fp16: true
  
  # ------------------ OpenAI API 配置 ------------------
  openai:
    # （当前 pipeline 不使用 OpenAI；如需启用请改 embedding.backend=openai 并用环境变量配置密钥）
    api_key: "sk-zk29b3dee55ac5b9f4c62a542c15cd33408697c5911a1a69"
    # 也可用环境变量覆盖：OPENAI_BASE_URL
    base_url: "https://api.zhizengzeng.com/v1"
    # 模型名称: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
    model: "text-embedding-3-small"
    # 输出维度（仅对 text-embedding-3 系列有效，可选: 256, 512, 1024, 3072）
    dimensions: null  # null 表示使用模型默认维度
    # 请求超时（秒）
    timeout: 60
    # 重试次数
    max_retries: 3

# 视觉编码器配置
vision:
  model_path: "/home/xwh/models/DeepSeek-OCR-2"
  device: "auto"

# ==================== 日志配置 ====================
logging:
  # 是否开启日志（API 使用）
  enabled: true
  # 日志级别: DEBUG, INFO, WARNING, ERROR
  level: "INFO"
  
  # 是否输出到文件
  file_output: true
  
  # 日志文件目录
  log_dir: "./logs"
  
  # 日志文件名（null 表示按日期自动生成: visrag_YYYYMMDD.log）
  log_filename: null
  
  # 是否在日志中显示函数名和行号（调试用）
  show_location: false

# ==================== FastAPI 配置 ====================
api:
  host: "0.0.0.0"
  port: 8000
  cors_origins: ["*"]
  log_requests: true

# ==================== 检索配置 ====================
retrieval:
  image_top_k: 1
  text_top_k: 3

# ==================== Query 强化（可选）====================
# 用 OpenAI 兼容接口（如智增增）把 query 关键字抽取/重复，提升检索召回。
query_enhance:
  enabled: true
  max_tokens: 80
  temperature: 0.0
  openai:
    api_key: null              # 或用环境变量 OPENAI_API_KEY
    base_url: "https://api.zhizengzeng.com/v1"   # 或用环境变量 OPENAI_BASE_URL
    model: "gpt-4o-mini"
    timeout: 20

# ==================== 索引配置 ====================
indexing:
  default_image_collection: "vision_pages"
  default_text_collection: "default_texts"
  text_collection_prefix: "pdf"
  chunk_size: 400
  chunk_overlap: 50
  persist_dir: "./output/chroma_db"
  assets_dir: "./output/api_assets"

# ==================== Qwen VL Embedding 配置 ====================
vl_embedding:
  model_path: "/home/xwh/models/Qwen3-VL-Embedding-2B"
  device: "auto"
  dtype: "bfloat16"
  batch_size: 8
  max_length: 32768  # Qwen3-VL-Embedding-2B supports up to 32k context length
  embedding_dim: null  # null = use model default (up to 2048), or set 64-2048 for custom dimension
  # Optional: remote embedding service (e.g., vLLM FastAPI)
  service_url: "http://127.0.0.1:9002"  # e.g., "http://127.0.0.1:9001"
  service_timeout: 60

# ==================== OCR 配置 ====================
ocr:
  model_path: "/home/xwh/models/DeepSeek-OCR-2"
  vllm_code_dir: "/home/xwh/DeepSeek-OCR-2/DeepSeek-OCR2-master/DeepSeek-OCR2-vllm"  # 必须：deepseek_ocr2 / process 所在目录
  # HF infer() 参数（推荐）
  base_size: 1024
  image_size: 768
  crop_mode: true
  save_results: false
  output_dir: null  # null -> ./output/api_assets/ocr_infer
  # Optional: Custom prompt template. Use {question} and {context} as placeholders.
  # If not set, uses default English QA template.
  # prompt_template: "<image>\n<|grounding|>Convert the document to markdown.\n\nQuestion: {question}\nContext: {context}"
